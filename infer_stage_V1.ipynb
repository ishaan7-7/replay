{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "308d54c0-b28d-4c3f-b565-9de9c2a227dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Paths set. Using MLflow? False\n"
     ]
    }
   ],
   "source": [
    "import os, sys, json, time, uuid, glob, math, warnings\n",
    "from pathlib import Path\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "REPO_ROOT         = Path(r\"C:\\engine_module_pipeline\")\n",
    "INFER_STAGE_ROOT  = REPO_ROOT / \"infer_stage\"\n",
    "\n",
    "DELTA_BASE        = INFER_STAGE_ROOT / \"delta\"\n",
    "ARTIFACTS_DIR     = INFER_STAGE_ROOT / \"artifacts\"\n",
    "CHECKPOINTS_DIR   = INFER_STAGE_ROOT / \"checkpoints\"\n",
    "DLQ_DIR           = INFER_STAGE_ROOT / \"dlq\"\n",
    "LOGS_DIR          = INFER_STAGE_ROOT / \"logs\"\n",
    "\n",
    "for p in (DELTA_BASE, ARTIFACTS_DIR, CHECKPOINTS_DIR, DLQ_DIR, LOGS_DIR):\n",
    "    p.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "\n",
    "INFER_READY             = DELTA_BASE / \"engine_module_infer_ready\"        # reader\n",
    "INFER_RESULTS_DELTA     = DELTA_BASE / \"engine_module_inference_results\"  # writer \n",
    "ALERTS_DELTA            = DELTA_BASE / \"engine_module_alerts\"\n",
    "LSTM_WIN_DELTA          = DELTA_BASE / \"engine_module_lstm_windows\"\n",
    "MODEL_META_DELTA        = DELTA_BASE / \"engine_module_model_metadata\"\n",
    "VEH_HEALTH_DELTA        = DELTA_BASE / \"vehicle_health_summary\"\n",
    "\n",
    "# ---- Artifacts / contract files ----\n",
    "CONTRACT_JSON           = ARTIFACTS_DIR / \"model_input_contract.json\"\n",
    "SCALER_FNAME            = \"scaler_robust.joblib\"\n",
    "DENSE_TS_FNAME          = \"model_dense_best_torchscript.pt\"\n",
    "LSTM_SD_FNAME           = \"model_lstm_long_best.pt\"             \n",
    "LSTM_TS_FNAME           = \"model_lstm_long_best_torchscript.pt\" \n",
    "ISOF_FNAME              = \"isolation_forest_combiner_final.joblib\"\n",
    "KDE_PREFIX              = \"kde_\"\n",
    "GMM_PREFIX              = \"gmm_\"\n",
    "\n",
    "\n",
    "USE_MLFLOW = False  \n",
    "print(\"Paths set. Using MLflow?\", USE_MLFLOW)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "174e5a4f-8e9f-479b-8780-60edb71aa95d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "JAVA_HOME: C:\\jdk-11.0.28+6\n",
      "java exe exists: C:\\jdk-11.0.28+6\\bin\\java.exe True\n",
      "PATH[0..2]: ['C:\\\\jdk-11.0.28+6\\\\bin', 'C:\\\\jdk-11.0.28+6\\\\bin', 'C:\\\\engine_module_pipeline\\\\.venv-spark\\\\Scripts']\n",
      "java -version OK\n",
      "openjdk version \"11.0.28\" 2025-07-15\n",
      "OpenJDK Runtime Environment Temurin-11.0.28+6 (build 11.0.28+6)\n",
      "OpenJDK 64-Bit Server VM Temurin-11.0.28+6 (build 11.0.28+6, mixed mode)\n",
      "\n",
      "JARs: ['C:\\\\engine_module_pipeline\\\\jars\\\\delta-spark_2.12-3.2.0.jar', 'C:\\\\engine_module_pipeline\\\\jars\\\\delta-storage-3.2.0.jar']\n",
      "C:\\engine_module_pipeline\\jars\\delta-spark_2.12-3.2.0.jar exists: True\n",
      "C:\\engine_module_pipeline\\jars\\delta-storage-3.2.0.jar exists: True\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "from pathlib import Path\n",
    "\n",
    "print(\"JAVA_HOME:\", os.environ.get(\"JAVA_HOME\"))\n",
    "java_path = Path(os.environ.get(\"JAVA_HOME\",\"\"))/\"bin/java.exe\"\n",
    "print(\"java exe exists:\", java_path, java_path.exists())\n",
    "print(\"PATH[0..2]:\", os.environ.get(\"PATH\",\"\").split(os.pathsep)[:3])\n",
    "\n",
    "try:\n",
    "    out = subprocess.run([str(java_path), \"-version\"], capture_output=True, text=True)\n",
    "    print(\"java -version OK\")\n",
    "    print(out.stderr or out.stdout)\n",
    "except Exception as e:\n",
    "    print(\"java spawn FAILED:\", repr(e))\n",
    "\n",
    "delta_jar = REPO_ROOT / r\"jars\\delta-spark_2.12-3.2.0.jar\"\n",
    "storage_jar = REPO_ROOT / r\"jars\\delta-storage-3.2.0.jar\"\n",
    "print(\"JARs:\", [str(delta_jar), str(storage_jar)])\n",
    "print(delta_jar, \"exists:\", delta_jar.exists())\n",
    "print(storage_jar, \"exists:\", storage_jar.exists())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "03dbabe6-da91-426c-8896-95c2b4e2ba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Spark version: 3.5.1\n",
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n",
      "[OK] Delta IO verified.\n"
     ]
    }
   ],
   "source": [
    "import pyspark as _ps\n",
    "from pathlib import Path\n",
    "import os, shutil\n",
    "\n",
    "PYSPARK_DIR = Path(_ps.__file__).resolve().parent\n",
    "SPARK_HOME  = PYSPARK_DIR\n",
    "SPARK_SUBMIT = SPARK_HOME / \"bin\" / \"spark-submit.cmd\"\n",
    "assert SPARK_SUBMIT.exists(), f\"{SPARK_SUBMIT} missing. Reinstall pyspark==3.5.1\"\n",
    "\n",
    "JAVA_HOME = os.environ.get(\"JAVA_HOME\", r\"C:\\jdk-11.0.28+6\")\n",
    "java_exe  = Path(JAVA_HOME) / \"bin\" / \"java.exe\"\n",
    "assert java_exe.exists(), f\"java.exe not found under JAVA_HOME={JAVA_HOME}\"\n",
    "os.environ[\"JAVA_HOME\"] = JAVA_HOME\n",
    "os.environ[\"PATH\"] = str(java_exe.parent) + os.pathsep + os.environ.get(\"PATH\",\"\")\n",
    "\n",
    "os.environ[\"SPARK_HOME\"] = str(SPARK_HOME)\n",
    "DELTA_JAR   = REPO_ROOT / r\"jars\\delta-spark_2.12-3.2.0.jar\"\n",
    "STORAGE_JAR = REPO_ROOT / r\"jars\\delta-storage-3.2.0.jar\"\n",
    "for p in (DELTA_JAR, STORAGE_JAR):\n",
    "    assert p.exists(), f\"Missing JAR: {p}\"\n",
    "classpath = f\"{DELTA_JAR};{STORAGE_JAR}\"\n",
    "os.environ[\"CLASSPATH\"] = classpath\n",
    "\n",
    "from pyspark.sql import SparkSession\n",
    "builder = (\n",
    "    SparkSession.builder\n",
    "    .appName(\"engine_infer_stream_local_delta\")\n",
    "    .master(\"local[2]\")\n",
    "    .config(\"spark.sql.extensions\", \"io.delta.sql.DeltaSparkSessionExtension\")\n",
    "    .config(\"spark.sql.catalog.spark_catalog\", \"org.apache.spark.sql.delta.catalog.DeltaCatalog\")\n",
    "    .config(\"spark.jars\", f\"{DELTA_JAR},{STORAGE_JAR}\")\n",
    "    .config(\"spark.driver.extraClassPath\", classpath)\n",
    "    .config(\"spark.executor.extraClassPath\", classpath)\n",
    "    .config(\"spark.sql.shuffle.partitions\", \"8\")\n",
    "    .config(\"spark.driver.memory\", \"4g\")\n",
    ")\n",
    "spark = builder.getOrCreate()\n",
    "spark.sparkContext.setLogLevel(\"WARN\")\n",
    "print(\"Spark version:\", spark.version)\n",
    "\n",
    "# Probe Delta IO\n",
    "probe_path = (DELTA_BASE / \"__probe_delta\").as_posix()\n",
    "shutil.rmtree(Path(probe_path), ignore_errors=True)\n",
    "spark.range(1).write.format(\"delta\").mode(\"overwrite\").save(probe_path)\n",
    "spark.read.format(\"delta\").load(probe_path).show()\n",
    "print(\"[OK] Delta IO verified.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7e374308-5eb9-499c-8bcf-57028dae51d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded contract.\n",
      "First 5 features in-order: ['Air_Fuel_Ratio_Commanded_:1', 'Air_Fuel_Ratio_Measured_:1', 'Catalyst_Temperature__Bank_1_Sensor_1', 'Catalyst_Temperature__Bank_1_Sensor_2', 'Engine_kW__At_the_wheels_kW']\n",
      "SAN_COLON token: __COLON__\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "assert CONTRACT_JSON.exists(), f\"Missing model input contract: {CONTRACT_JSON}\"\n",
    "contract = json.loads(CONTRACT_JSON.read_text(encoding=\"utf-8\"))\n",
    "\n",
    "features = contract[\"features_canonical_order\"]\n",
    "assert isinstance(features, list) and len(features) == 25, f\"Expected 25 canonical features in contract, got {len(features)}\"\n",
    "timestamp_column = contract.get(\"timestamp_column\", \"timestamp\")\n",
    "\n",
    "SAN_COLON = contract.get(\"per_feature_filename_rule\", {}).get(\"colon\", \"__COLON__\")\n",
    "def sanitize_for_filename(feat: str) -> str:\n",
    "    return feat.replace(\":\", SAN_COLON)\n",
    "\n",
    "print(\"Loaded contract.\")\n",
    "print(\"First 5 features in-order:\", features[:5])\n",
    "print(\"SAN_COLON token:\", SAN_COLON)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "f7eb18ae-e3d1-49d7-9f87-0d70578c7bc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ARTS manifest: {'features': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\features.json', 'scaler': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\scaler_robust.joblib', 'dense_ts': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\model_dense_best_torchscript.pt', 'dense_sd': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\model_dense_best_state_dict.pt', 'lstm_ts': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\model_lstm_long_best_torchscript.pt', 'lstm_sd': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\model_lstm_long_best.pt', 'isof': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts\\\\isolation_forest_combiner_final.joblib', 'kde_dir': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts', 'gmm_dir': 'C:\\\\engine_module_pipeline\\\\infer_stage\\\\artifacts', 'kde_prefix': 'kde_', 'gmm_prefix': 'gmm_', 'sanitizer': {'colon': '__COLON__'}}\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os, json\n",
    "\n",
    "ARTIFACTS_DIR = REPO_ROOT / r\"infer_stage\\artifacts\"   \n",
    "assert ARTIFACTS_DIR.exists(), f\"Artifacts dir missing: {ARTIFACTS_DIR}\"\n",
    "\n",
    "FEATURES_FNAME = \"features.json\"\n",
    "SCALER_FNAME   = \"scaler_robust.joblib\"\n",
    "DENSE_TS       = \"model_dense_best_torchscript.pt\"\n",
    "DENSE_STATE    = \"model_dense_best_state_dict.pt\"\n",
    "LSTM_TS        = \"model_lstm_long_best_torchscript.pt\"  \n",
    "LSTM_STATE     = \"model_lstm_long_best.pt\"\n",
    "ISO_JOBLIB     = \"isolation_forest_combiner_final.joblib\"\n",
    "KDE_PREFIX     = \"kde_\"\n",
    "GMM_PREFIX     = \"gmm_\"\n",
    "\n",
    "def _safe_local(name: str|None):\n",
    "    if not name: return None\n",
    "    p = ARTIFACTS_DIR / name\n",
    "    return str(p) if p.exists() else None\n",
    "\n",
    "ARTS = {\n",
    "    \"features\": _safe_local(FEATURES_FNAME),\n",
    "    \"scaler\":   _safe_local(SCALER_FNAME),\n",
    "    \"dense_ts\": _safe_local(DENSE_TS),\n",
    "    \"dense_sd\": _safe_local(DENSE_STATE),\n",
    "    \"lstm_ts\":  _safe_local(LSTM_TS),\n",
    "    \"lstm_sd\":  _safe_local(LSTM_STATE),\n",
    "    \"isof\":     _safe_local(ISO_JOBLIB),\n",
    "    \"kde_dir\":  str(ARTIFACTS_DIR),\n",
    "    \"gmm_dir\":  str(ARTIFACTS_DIR),\n",
    "    \"kde_prefix\": KDE_PREFIX,\n",
    "    \"gmm_prefix\": GMM_PREFIX,\n",
    "    \"sanitizer\": {\"colon\": \"__COLON__\"},\n",
    "}\n",
    "\n",
    "missing_required = [k for k in (\"features\",\"scaler\",\"dense_ts\",\"lstm_sd\",\"isof\") if not ARTS[k]]\n",
    "if missing_required:\n",
    "    raise FileNotFoundError(f\"Required artifacts not found: {missing_required}\\nARTS={ARTS}\")\n",
    "\n",
    "SCALER_PATH   = ARTS[\"scaler\"]\n",
    "DENSE_TS_PATH = ARTS[\"dense_ts\"]\n",
    "DENSE_SD_PATH = ARTS[\"dense_sd\"]\n",
    "LSTM_TS_PATH  = ARTS[\"lstm_ts\"]\n",
    "LSTM_SD_PATH  = ARTS[\"lstm_sd\"]\n",
    "ISOF_PATH     = ARTS[\"isof\"]\n",
    "KDE_DIR       = Path(ARTS[\"kde_dir\"])\n",
    "GMM_DIR       = Path(ARTS[\"gmm_dir\"])\n",
    "KDE_PREFIX    = ARTS[\"kde_prefix\"]\n",
    "GMM_PREFIX    = ARTS[\"gmm_prefix\"]\n",
    "SAN_RULE      = ARTS[\"sanitizer\"]  \n",
    "\n",
    "\n",
    "for k in (\"features\",\"scaler\",\"dense_ts\",\"dense_sd\",\"lstm_ts\",\"lstm_sd\",\"isof\"):\n",
    "    p = ARTS.get(k)\n",
    "    if p and os.path.exists(p):\n",
    "        try:\n",
    "            spark.sparkContext.addFile(p)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] addFile failed for {p}: {e}\")\n",
    "\n",
    "print(\"ARTS manifest:\", ARTS)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "66c62574-4149-4385-957f-7911444169dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_alerts\n",
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_lstm_windows\n",
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_model_metadata\n",
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\vehicle_health_summary\n",
      "All 5 tables initialized; this notebook writes only inference_results.\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "\n",
    "feature_fields = [StructField(f, DoubleType(), True) for f in features]\n",
    "\n",
    "INFER_RESULTS_SCHEMA = StructType([\n",
    "    StructField(\"row_hash\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), True),\n",
    "    StructField(\"date\", StringType(), False),  # string in results (kept as your design)\n",
    "    StructField(\"source_id\", StringType(), True),\n",
    "    StructField(\"kafka_key\", StringType(), True),\n",
    "    StructField(\"offset\", LongType(), True),\n",
    "    StructField(\"source_file\", StringType(), True),\n",
    "    *feature_fields,\n",
    "    StructField(\"recon_error_dense\", DoubleType(), True),\n",
    "    StructField(\"dense_per_feature_error\", MapType(StringType(), DoubleType()), True),\n",
    "    StructField(\"recon_error_lstm\", DoubleType(), True),\n",
    "    StructField(\"lstm_window_id\", StringType(), True),\n",
    "    StructField(\"isolation_score\", DoubleType(), True),\n",
    "    StructField(\"kde_logp\", DoubleType(), True),\n",
    "    StructField(\"gmm_logp\", DoubleType(), True),\n",
    "    StructField(\"composite_score\", DoubleType(), True),\n",
    "    StructField(\"anomaly_label\", StringType(), True),\n",
    "    StructField(\"anomaly_severity\", IntegerType(), True),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"inference_run_id\", StringType(), True),\n",
    "    StructField(\"inference_ts\", TimestampType(), True),\n",
    "    StructField(\"processing_latency_ms\", LongType(), True),\n",
    "    StructField(\"explain_top_k\", ArrayType(StructType([\n",
    "        StructField(\"feature\", StringType(), False),\n",
    "        StructField(\"contribution\", DoubleType(), False),\n",
    "    ])), True),\n",
    "    StructField(\"raw_model_outputs\", MapType(StringType(), DoubleType()), True),\n",
    "])\n",
    "\n",
    "def ensure_delta_table(path: Path, schema: StructType, partition_cols=None):\n",
    "    partition_cols = partition_cols or []\n",
    "    if (path / \"_delta_log\").exists():\n",
    "        return\n",
    "    empty = spark.createDataFrame([], schema)\n",
    "    w = empty.write.format(\"delta\").mode(\"overwrite\")\n",
    "    if partition_cols:\n",
    "        w = w.partitionBy(*partition_cols)\n",
    "    w.save(str(path))\n",
    "    print(\"Initialized Delta table:\", path)\n",
    "\n",
    "ALERTS_SCHEMA = StructType([\n",
    "    StructField(\"alert_id\", StringType(), False),\n",
    "    StructField(\"alert_ts\", TimestampType(), False),\n",
    "    StructField(\"row_hash\", StringType(), True),\n",
    "    StructField(\"vehicle_id\", StringType(), True),\n",
    "    StructField(\"alert_type\", StringType(), True),\n",
    "    StructField(\"severity\", IntegerType(), True),\n",
    "    StructField(\"composite_score\", DoubleType(), True),\n",
    "    StructField(\"triggering_models\", ArrayType(StringType()), True),\n",
    "    StructField(\"reason\", StringType(), True),\n",
    "    StructField(\"top_features\", ArrayType(StringType()), True),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"inference_run_id\", StringType(), True),\n",
    "    StructField(\"acked\", BooleanType(), True),\n",
    "    StructField(\"acked_by\", StringType(), True),\n",
    "    StructField(\"acked_ts\", TimestampType(), True),\n",
    "    StructField(\"notified_channels\", ArrayType(StringType()), True),\n",
    "    StructField(\"linked_rows\", ArrayType(StringType()), True),\n",
    "    StructField(\"extra\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "])\n",
    "\n",
    "LSTM_WIN_SCHEMA = StructType([\n",
    "    StructField(\"lstm_window_id\", StringType(), False),\n",
    "    StructField(\"window_start_ts\", TimestampType(), False),\n",
    "    StructField(\"window_end_ts\", TimestampType(), False),\n",
    "    StructField(\"row_hashes\", ArrayType(StringType()), False),\n",
    "    StructField(\"reconstruction_error\", DoubleType(), True),\n",
    "    StructField(\"per_step_errors\", ArrayType(DoubleType()), True),\n",
    "    StructField(\"model_version\", StringType(), True),\n",
    "    StructField(\"inference_run_id\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "])\n",
    "\n",
    "MODEL_METADATA_SCHEMA = StructType([\n",
    "    StructField(\"inference_run_id\", StringType(), False),\n",
    "    StructField(\"timestamp\", TimestampType(), False),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), False),\n",
    "    StructField(\"params\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"baseline_stats\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"notes\", StringType(), True),\n",
    "    StructField(\"source_commit\", StringType(), True),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "])\n",
    "\n",
    "VEH_HEALTH_SCHEMA = StructType([\n",
    "    StructField(\"vehicle_id\", StringType(), False),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "    StructField(\"rows_count\", LongType(), True),\n",
    "    StructField(\"anomaly_count\", LongType(), True),\n",
    "    StructField(\"anomaly_rate\", DoubleType(), True),\n",
    "    StructField(\"median_composite_score\", DoubleType(), True),\n",
    "    StructField(\"p95_composite_score\", DoubleType(), True),\n",
    "    StructField(\"health_score\", DoubleType(), True),\n",
    "    StructField(\"days_since_last_alert\", IntegerType(), True),\n",
    "    StructField(\"top_failure_modes\", ArrayType(StringType()), True),\n",
    "    StructField(\"trend_flag\", StringType(), True),\n",
    "    StructField(\"estimated_rul\", DoubleType(), True),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"last_inference_ts\", TimestampType(), True),\n",
    "])\n",
    "\n",
    "# Initialize once (so downstream merges work later)\n",
    "ensure_delta_table(INFER_RESULTS_DELTA, INFER_RESULTS_SCHEMA, partition_cols=[\"date\"])\n",
    "ensure_delta_table(ALERTS_DELTA,        ALERTS_SCHEMA,        partition_cols=[\"date\"])\n",
    "ensure_delta_table(LSTM_WIN_DELTA,      LSTM_WIN_SCHEMA,      partition_cols=[\"date\"])\n",
    "ensure_delta_table(MODEL_META_DELTA,    MODEL_METADATA_SCHEMA,partition_cols=[\"date\"])\n",
    "ensure_delta_table(VEH_HEALTH_DELTA,    VEH_HEALTH_SCHEMA,    partition_cols=[\"date\"])\n",
    "print(\"All 5 tables initialized; this notebook writes only inference_results.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "bbf42487-2a17-4198-8f5f-b625d9aab51b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading, joblib, torch, numpy as np, pandas as pd, os, glob\n",
    "from pyspark import SparkFiles\n",
    "from pathlib import Path as _P\n",
    "\n",
    "req = [\"SCALER_PATH\",\"DENSE_TS_PATH\",\"DENSE_SD_PATH\",\"LSTM_TS_PATH\",\"LSTM_SD_PATH\",\"ISOF_PATH\",\n",
    "       \"KDE_DIR\",\"GMM_DIR\",\"KDE_PREFIX\",\"GMM_PREFIX\",\"SAN_RULE\"]\n",
    "missing = [r for r in req if r not in globals()]\n",
    "if missing:\n",
    "    raise RuntimeError(f\"Run Cell 5 first; missing globals: {missing}\")\n",
    "\n",
    "_executor_cache = {\"models\":{}, \"scaler\":None, \"kde\":{}, \"gmm\":{}, \"versions\":{}}\n",
    "_cache_lock = threading.Lock()\n",
    "\n",
    "def _first_existing(*candidates):\n",
    "    for p in candidates:\n",
    "        if p and os.path.exists(p): return p\n",
    "    return None\n",
    "\n",
    "def _sparkfile_by_basename(local_path: str|None):\n",
    "    if not local_path: return None\n",
    "    try:\n",
    "        base = os.path.basename(local_path)\n",
    "        p = SparkFiles.get(base)\n",
    "        return p if p and os.path.exists(p) else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "COLON_TOKEN = SAN_RULE.get(\"colon\", \"__COLON__\")\n",
    "def _sanitize_feature_name(s: str) -> str:\n",
    "    return (s or \"\").replace(\":\", COLON_TOKEN)\n",
    "def _desanitize_feature_name(s: str) -> str:\n",
    "    return (s or \"\").replace(COLON_TOKEN, \":\")\n",
    "\n",
    "def load_scaler():\n",
    "    with _cache_lock:\n",
    "        if _executor_cache[\"scaler\"] is not None:\n",
    "            return _executor_cache[\"scaler\"]\n",
    "        p = _first_existing(SCALER_PATH, _sparkfile_by_basename(SCALER_PATH))\n",
    "        if not p: raise RuntimeError(\"Scaler artifact not found.\")\n",
    "        sc = joblib.load(p); _executor_cache[\"scaler\"] = sc; return sc\n",
    "\n",
    "def load_dense():\n",
    "    with _cache_lock:\n",
    "        if \"dense\" in _executor_cache[\"models\"]:\n",
    "            return _executor_cache[\"models\"][\"dense\"]\n",
    "        p_ts = _first_existing(DENSE_TS_PATH, _sparkfile_by_basename(DENSE_TS_PATH))\n",
    "        p_sd = _first_existing(DENSE_SD_PATH, _sparkfile_by_basename(DENSE_SD_PATH))\n",
    "        if p_ts:\n",
    "            m = torch.jit.load(p_ts, map_location=\"cpu\").eval()\n",
    "            _executor_cache[\"models\"][\"dense\"] = (\"ts\", m)\n",
    "            _executor_cache[\"versions\"][\"dense\"] = \"ts\"; return _executor_cache[\"models\"][\"dense\"]\n",
    "        if p_sd:\n",
    "            m = torch.load(p_sd, map_location=\"cpu\"); \n",
    "            try: m.eval()\n",
    "            except: pass\n",
    "            _executor_cache[\"models\"][\"dense\"] = (\"sd\", m)\n",
    "            _executor_cache[\"versions\"][\"dense\"] = \"sd\"; return _executor_cache[\"models\"][\"dense\"]\n",
    "        raise RuntimeError(\"Dense AE artifact not found (TS/SD).\")\n",
    "\n",
    "def load_lstm():\n",
    "    with _cache_lock:\n",
    "        if \"lstm\" in _executor_cache[\"models\"]:\n",
    "            return _executor_cache[\"models\"][\"lstm\"]\n",
    "        p_ts = _first_existing(LSTM_TS_PATH, _sparkfile_by_basename(LSTM_TS_PATH))\n",
    "        p_sd = _first_existing(LSTM_SD_PATH, _sparkfile_by_basename(LSTM_SD_PATH))\n",
    "        if p_ts:\n",
    "            m = torch.jit.load(p_ts, map_location=\"cpu\").eval()\n",
    "            _executor_cache[\"models\"][\"lstm\"] = (\"ts\", m)\n",
    "            _executor_cache[\"versions\"][\"lstm\"] = \"ts\"; return _executor_cache[\"models\"][\"lstm\"]\n",
    "        if p_sd:\n",
    "            import torch.nn as nn\n",
    "            class ForgivingLSTMAE(nn.Module):\n",
    "                def __init__(self, input_dim=25, enc_hidden=64, dec_hidden=25, enc_layers=1, dec_layers=1):\n",
    "                    super().__init__()\n",
    "                    self.encoder = nn.LSTM(input_size=input_dim, hidden_size=enc_hidden, num_layers=enc_layers, batch_first=True)\n",
    "                    self.decoder = nn.LSTM(input_size=enc_hidden,  hidden_size=dec_hidden, num_layers=dec_layers, batch_first=True)\n",
    "                    self.out     = nn.Linear(dec_hidden, input_dim)\n",
    "                def forward(self, x):\n",
    "                    enc_out,_ = self.encoder(x); dec_out,_ = self.decoder(enc_out)\n",
    "                    return self.out(dec_out[:, -1:, :])\n",
    "            state = torch.load(p_sd, map_location=\"cpu\")\n",
    "            m = ForgivingLSTMAE(); m.load_state_dict(state, strict=False); m.eval()\n",
    "            _executor_cache[\"models\"][\"lstm\"] = (\"sd\", m)\n",
    "            _executor_cache[\"versions\"][\"lstm\"] = \"sd\"; return _executor_cache[\"models\"][\"lstm\"]\n",
    "        _executor_cache[\"versions\"][\"lstm\"] = \"none\"\n",
    "        return None\n",
    "\n",
    "def load_isof():\n",
    "    with _cache_lock:\n",
    "        if \"isof\" in _executor_cache[\"models\"]:\n",
    "            return _executor_cache[\"models\"][\"isof\"]\n",
    "        p = _first_existing(ISOF_PATH, _sparkfile_by_basename(ISOF_PATH))\n",
    "        if not p: raise RuntimeError(\"IsolationForest artifact not found.\")\n",
    "        m = joblib.load(p)\n",
    "        _executor_cache[\"models\"][\"isof\"] = m\n",
    "        _executor_cache[\"versions\"][\"isof\"] = \"joblib\"; return m\n",
    "\n",
    "def _scan_models(prefix: str, base_dir: _P):\n",
    "    \"\"\"Return mapping {canonical_feature_name: full_path} (de-sanitized).\"\"\"\n",
    "    pat = str(base_dir / f\"{prefix}*.joblib\")\n",
    "    out = {}\n",
    "    for p in glob.glob(pat):\n",
    "        base = os.path.basename(p)\n",
    "        key = base[len(prefix):-7]  # filename core\n",
    "        canon = _desanitize_feature_name(key)  \n",
    "        if canon:\n",
    "            out[canon] = p\n",
    "    return out\n",
    "\n",
    "_KDE_INDEX = None\n",
    "_GMM_INDEX = None\n",
    "\n",
    "def load_kde_for(feature):\n",
    "    global _KDE_INDEX\n",
    "    with _cache_lock:\n",
    "        if _KDE_INDEX is None:\n",
    "            _KDE_INDEX = _scan_models(KDE_PREFIX, _P(KDE_DIR))\n",
    "        p = _KDE_INDEX.get(feature) or _KDE_INDEX.get(_desanitize_feature_name(_sanitize_feature_name(feature)))\n",
    "        if not p or not os.path.exists(p): return None\n",
    "        if feature not in _executor_cache[\"kde\"]:\n",
    "            _executor_cache[\"kde\"][feature] = joblib.load(p)\n",
    "        return _executor_cache[\"kde\"][feature]\n",
    "\n",
    "def load_gmm_for(feature):\n",
    "    global _GMM_INDEX\n",
    "    with _cache_lock:\n",
    "        if _GMM_INDEX is None:\n",
    "            _GMM_INDEX = _scan_models(GMM_PREFIX, _P(GMM_DIR))\n",
    "        p = _GMM_INDEX.get(feature) or _GMM_INDEX.get(_desanitize_feature_name(_sanitize_feature_name(feature)))\n",
    "        if not p or not os.path.exists(p): return None\n",
    "        if feature not in _executor_cache[\"gmm\"]:\n",
    "            _executor_cache[\"gmm\"][feature] = joblib.load(p)\n",
    "        return _executor_cache[\"gmm\"][feature]\n",
    "\n",
    "def model_versions_map():\n",
    "    with _cache_lock:\n",
    "        return dict(_executor_cache[\"versions\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "5f0bf20f-85e1-4853-8dfd-1e738cf6f0d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "BASELINES = {\n",
    "    \"dense_med\": 0.05, \"dense_mad\": 0.05,\n",
    "    \"lstm_med\":  0.05, \"lstm_mad\":  0.05,\n",
    "    \"isof_med\":  0.0,  \"isof_mad\":  0.5,\n",
    "    \"kde_med\":  -10.0, \"kde_mad\":   3.0,\n",
    "    \"gmm_med\":  -10.0, \"gmm_mad\":   3.0\n",
    "}\n",
    "\n",
    "def robust_norm(x, med, mad, invert=False, k=1.4826):\n",
    "    if x is None: return 0.0\n",
    "    denom = (mad*k) if mad and mad>0 else 1.0\n",
    "    z = (x - med)/denom\n",
    "    if invert: z = -z\n",
    "    return 1/(1+math.exp(-z))\n",
    "\n",
    "WEIGHTS = {\"dense\":0.35, \"lstm\":0.25, \"isof\":0.20, \"kde\":0.10, \"gmm\":0.10}\n",
    "\n",
    "def composite_from_raw(dense_e, lstm_e, isof_s, kde_lp, gmm_lp):\n",
    "    s_dense = robust_norm(dense_e, BASELINES[\"dense_med\"], BASELINES[\"dense_mad\"])\n",
    "    s_lstm  = robust_norm(lstm_e,  BASELINES[\"lstm_med\"],  BASELINES[\"lstm_mad\"])\n",
    "    s_isof  = robust_norm(isof_s,  BASELINES[\"isof_med\"],  BASELINES[\"isof_mad\"], invert=True)\n",
    "    s_kde   = robust_norm(kde_lp,  BASELINES[\"kde_med\"],   BASELINES[\"kde_mad\"],  invert=True)\n",
    "    s_gmm   = robust_norm(gmm_lp,  BASELINES[\"gmm_med\"],   BASELINES[\"gmm_mad\"],  invert=True)\n",
    "    return (WEIGHTS[\"dense\"]*s_dense +\n",
    "            WEIGHTS[\"lstm\"] *s_lstm  +\n",
    "            WEIGHTS[\"isof\"] *s_isof  +\n",
    "            WEIGHTS[\"kde\"]  *s_kde   +\n",
    "            WEIGHTS[\"gmm\"]  *s_gmm)\n",
    "\n",
    "def label_from_composite(c):\n",
    "    if c is None: return \"unknown\", 0\n",
    "    if c < 0.20: return \"normal\", 0\n",
    "    if c < 0.50: return \"suspicious\", 1\n",
    "    if c < 0.75: return \"anomaly\", 2\n",
    "    return \"critical\", 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "145d970c-d0b0-414b-899e-1e9db5e4b92f",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from typing import Iterator\n",
    "import uuid, numpy as np, pandas as pd, torch\n",
    "\n",
    "LSTM_WINDOW = 10  \n",
    "\n",
    "\n",
    "def _feat_variants(feat: str):\n",
    "    if \":\" in feat:\n",
    "        return (feat, feat.replace(\":\", \"__COLON__\"))\n",
    "    return (feat,)\n",
    "\n",
    "def _get_kde(feat: str):\n",
    "    for key in _feat_variants(feat):\n",
    "        m = load_kde_for(key)\n",
    "        if m is not None:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def _get_gmm(feat: str):\n",
    "    for key in _feat_variants(feat):\n",
    "        m = load_gmm_for(key)\n",
    "        if m is not None:\n",
    "            return m\n",
    "    return None\n",
    "\n",
    "def infer_partition(pdf_iter: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "    scaler = load_scaler()\n",
    "    dense_mode, dense = load_dense()\n",
    "    lstm_entry = load_lstm()\n",
    "    isof = load_isof()\n",
    "\n",
    "    for pdf in pdf_iter:\n",
    "        if pdf.empty:\n",
    "            yield pd.DataFrame([], columns=[f.name for f in INFER_RESULTS_SCHEMA])\n",
    "            continue\n",
    "\n",
    "        for f in features:\n",
    "            if f not in pdf.columns:\n",
    "                pdf[f] = np.nan\n",
    "\n",
    "        feat_df = pdf[features].astype(float)\n",
    "        X = scaler.transform(np.nan_to_num(feat_df.values, copy=False))\n",
    "\n",
    "        with torch.no_grad():\n",
    "            xt = torch.from_numpy(X.astype(\"float32\"))\n",
    "            recon = dense(xt).numpy() if dense_mode == \"ts\" else dense(xt).detach().numpy()\n",
    "        dense_err = ((X - recon) ** 2).mean(axis=1)\n",
    "        dens_resid = np.abs(X - recon)  # [n, F]\n",
    "\n",
    "        lstm_err = [None] * len(X)\n",
    "        lstm_win_ids = [None] * len(X)\n",
    "        lstm_windows_rows = []\n",
    "        if lstm_entry is not None:\n",
    "            lstm_mode, lstm = lstm_entry\n",
    "            with torch.no_grad():\n",
    "                for i in range(len(X)):\n",
    "                    start = max(0, i - (LSTM_WINDOW - 1))\n",
    "                    window = X[start : i + 1]\n",
    "                    if window.shape[0] < 2:\n",
    "                        continue\n",
    "                    w_id = str(uuid.uuid4())\n",
    "                    wt = torch.from_numpy(window.astype(\"float32\")).unsqueeze(0)  \n",
    "                    out = lstm(wt)  \n",
    "                    out_np = out.squeeze().detach().cpu().numpy()\n",
    "                    last_rec = out_np[-1] if out_np.ndim == 2 else out_np  \n",
    "                    err = ((window[-1] - last_rec) ** 2).mean()\n",
    "                    lstm_err[i] = float(err)\n",
    "                    lstm_win_ids[i] = w_id\n",
    "\n",
    "                    rows_slice = pdf.iloc[start : i + 1]\n",
    "                    lstm_windows_rows.append({\n",
    "                        \"lstm_window_id\": w_id,\n",
    "                        \"window_start_ts\": pd.to_datetime(rows_slice[\"timestamp\"].iloc[0], utc=True, errors=\"coerce\"),\n",
    "                        \"window_end_ts\":   pd.to_datetime(rows_slice[\"timestamp\"].iloc[-1], utc=True, errors=\"coerce\"),\n",
    "                        \"row_hashes\": rows_slice[\"row_hash\"].astype(str).tolist(),\n",
    "                        \"reconstruction_error\": float(err),\n",
    "                        \"per_step_errors\": [],\n",
    "                        \"model_version\": \"ts\" if lstm_mode == \"ts\" else \"sd\",\n",
    "                        \"inference_run_id\": None,\n",
    "                        \"date\": str(pd.to_datetime(rows_slice[\"timestamp\"].iloc[-1]).date())\n",
    "                    })\n",
    "\n",
    "        try:\n",
    "            iso_scores = isof.decision_function(X).tolist()\n",
    "        except Exception:\n",
    "            iso_scores = [None] * len(X)\n",
    "\n",
    "        kde_lp, gmm_lp = [], []\n",
    "        Fw = len(features)\n",
    "        for r in range(len(X)):\n",
    "            s_kde = 0.0; cnt_k = 0\n",
    "            s_gmm = 0.0; cnt_g = 0\n",
    "            for j in range(Fw):\n",
    "                feat = features[j]\n",
    "                v = X[r, j]\n",
    "\n",
    "                mk = _get_kde(feat)\n",
    "                if mk is not None:\n",
    "                    try:\n",
    "                        s_kde += float(mk.score_samples(np.array([[v]]))[0]); cnt_k += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "                mg = _get_gmm(feat)\n",
    "                if mg is not None:\n",
    "                    try:\n",
    "                        s_gmm += float(mg.score(np.array([[v]]))[0]); cnt_g += 1\n",
    "                    except Exception:\n",
    "                        pass\n",
    "\n",
    "            kde_lp.append(s_kde if cnt_k > 0 else None)\n",
    "            gmm_lp.append(s_gmm if cnt_g > 0 else None)\n",
    "\n",
    "        n = len(pdf)\n",
    "\n",
    "        def _fix_len(seq, n, fill=None):\n",
    "            if seq is None:\n",
    "                return [fill] * n\n",
    "            if isinstance(seq, np.ndarray):\n",
    "                seq = seq.tolist()\n",
    "            if len(seq) < n:\n",
    "                return list(seq) + [fill] * (n - len(seq))\n",
    "            elif len(seq) > n:\n",
    "                return list(seq)[:n]\n",
    "            return list(seq)\n",
    "\n",
    "        dense_err     = _fix_len(dense_err, n, fill=None)\n",
    "        lstm_err      = _fix_len(lstm_err, n, fill=None)\n",
    "        lstm_win_ids  = _fix_len(lstm_win_ids, n, fill=None)\n",
    "        iso_scores    = _fix_len(iso_scores, n, fill=None)\n",
    "        kde_lp        = _fix_len(kde_lp, n, fill=None)\n",
    "        gmm_lp        = _fix_len(gmm_lp, n, fill=None)\n",
    "\n",
    "        # dens_resid -> [n, F] strictly\n",
    "        if not isinstance(dens_resid, np.ndarray) or dens_resid.ndim != 2:\n",
    "            dens_resid = np.zeros((n, Fw), dtype=np.float32)\n",
    "        else:\n",
    "            if dens_resid.shape[0] < n:\n",
    "                pad_rows = np.zeros((n - dens_resid.shape[0], dens_resid.shape[1]), dtype=dens_resid.dtype)\n",
    "                dens_resid = np.vstack([dens_resid, pad_rows])\n",
    "            elif dens_resid.shape[0] > n:\n",
    "                dens_resid = dens_resid[:n, :]\n",
    "            if dens_resid.shape[1] < Fw:\n",
    "                pad_cols = np.zeros((dens_resid.shape[0], Fw - dens_resid.shape[1]), dtype=dens_resid.dtype)\n",
    "                dens_resid = np.hstack([dens_resid, pad_cols])\n",
    "            elif dens_resid.shape[1] > Fw:\n",
    "                dens_resid = dens_resid[:, :Fw]\n",
    "\n",
    "        composites, labels, severities, explains = [], [], [], []\n",
    "        for i in range(n):\n",
    "            comp = composite_from_raw(\n",
    "                dense_err[i],\n",
    "                lstm_err[i],\n",
    "                iso_scores[i],\n",
    "                kde_lp[i],\n",
    "                gmm_lp[i]\n",
    "            )\n",
    "            composites.append(comp)\n",
    "            lab, sev = label_from_composite(comp)\n",
    "            labels.append(lab); severities.append(sev)\n",
    "\n",
    "            # top-3 by dense residuals (width-safe)\n",
    "            feats_abs = [(features[j], float(dens_resid[i, j])) for j in range(Fw)]\n",
    "            feats_abs.sort(key=lambda x: x[1], reverse=True)\n",
    "            explains.append([{\"feature\": a, \"contribution\": b} for a, b in feats_abs[:3]])\n",
    "\n",
    "        now = pd.Timestamp.utcnow()  \n",
    "        out_rows = []\n",
    "        for i, row in pdf.reset_index(drop=True).iterrows():\n",
    "            dct = dict(row)\n",
    "            dct[\"date\"] = str(pd.to_datetime(row[\"timestamp\"]).date())\n",
    "            dct[\"recon_error_dense\"] = None if dense_err[i] is None else float(dense_err[i])\n",
    "            dct[\"dense_per_feature_error\"] = {features[j]: float(dens_resid[i, j]) for j in range(Fw)}\n",
    "            dct[\"recon_error_lstm\"] = None if lstm_err[i] is None else float(lstm_err[i])\n",
    "            dct[\"lstm_window_id\"] = lstm_win_ids[i]\n",
    "            dct[\"isolation_score\"] = None if iso_scores[i] is None else float(iso_scores[i])\n",
    "            dct[\"kde_logp\"] = kde_lp[i]\n",
    "            dct[\"gmm_logp\"] = gmm_lp[i]   \n",
    "            dct[\"combiner_score\"] = None  \n",
    "            dct[\"composite_score\"] = composites[i]\n",
    "            dct[\"anomaly_label\"] = labels[i]\n",
    "            dct[\"anomaly_severity\"] = severities[i]\n",
    "            dct[\"model_versions\"] = model_versions_map()\n",
    "            dct[\"inference_run_id\"] = None\n",
    "            dct[\"inference_ts\"] = now\n",
    "            dct[\"processing_latency_ms\"] = None \n",
    "            dct[\"explain_top_k\"] = explains[i]\n",
    "            dct[\"raw_model_outputs\"] = {}\n",
    "            dct[\"notes\"] = None\n",
    "            out_rows.append(dct)\n",
    "\n",
    "        df_out = pd.DataFrame(out_rows)\n",
    "        df_lstm = pd.DataFrame(lstm_windows_rows) if lstm_windows_rows else pd.DataFrame(\n",
    "            [], columns=[f.name for f in LSTM_WIN_SCHEMA]\n",
    "        )\n",
    "        yield df_out.assign(__lstm_windows__=pd.Series([df_lstm] * len(df_out)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "61bfaf08-513a-4d64-a73d-224ffd85d1dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import DataFrame\n",
    "from pyspark.sql.types import (\n",
    "    StringType, TimestampType, LongType, IntegerType, DoubleType, DateType,\n",
    "    MapType, ArrayType, StructType, StructField\n",
    ")\n",
    "import pandas as pd, numpy as np, time\n",
    "\n",
    "def _to_int_or_none_series(s: pd.Series) -> pd.Series:\n",
    "    tmp = pd.to_numeric(s, errors=\"coerce\")\n",
    "    return tmp.apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "\n",
    "def _norm_map_float(m):\n",
    "    if not isinstance(m, dict): return {}\n",
    "    out = {}\n",
    "    for k, v in m.items():\n",
    "        try:\n",
    "            out[str(k)] = None if v is None else float(v)\n",
    "        except Exception:\n",
    "            out[str(k)] = None\n",
    "    return out\n",
    "\n",
    "def _norm_explain_top_k(v):\n",
    "    if v is None or (isinstance(v, float) and np.isnan(v)): return []\n",
    "    if isinstance(v, list):\n",
    "        out = []\n",
    "        for d in v:\n",
    "            if isinstance(d, dict):\n",
    "                f = \"\" if d.get(\"feature\") is None else str(d.get(\"feature\"))\n",
    "                try:\n",
    "                    c = 0.0 if d.get(\"contribution\") is None else float(d.get(\"contribution\"))\n",
    "                except Exception:\n",
    "                    c = 0.0\n",
    "                out.append({\"feature\": f, \"contribution\": c})\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "def _norm_versions(m):\n",
    "    if not isinstance(m, dict): return {}\n",
    "    return {str(k): (\"\" if v is None else str(v)) for k, v in m.items()}\n",
    "\n",
    "def _select_cast_by_name(df, target_schema: StructType):\n",
    "    def _is_simple(dt) -> bool:\n",
    "        return isinstance(dt, (StringType, TimestampType, LongType, IntegerType, DoubleType, DateType))\n",
    "    sel = []\n",
    "    for field in target_schema:\n",
    "        cname = field.name\n",
    "        col = F.col(f\"`{cname}`\")\n",
    "        if _is_simple(field.dataType):\n",
    "            col = col.cast(field.dataType)\n",
    "        sel.append(col.alias(cname))\n",
    "    return df.select(*sel)\n",
    "\n",
    "def foreach_batch(batch_df: DataFrame, batch_id: int):\n",
    "    t0 = time.time()\n",
    "\n",
    "    # Strict input projection by name (system + canonical features)\n",
    "    expected_in_cols = [\"row_hash\",\"timestamp\",\"source_id\",\"kafka_key\",\"offset\",\"source_file\",\"date\"] + features\n",
    "    src_cols = [c for c in expected_in_cols if c in batch_df.columns]\n",
    "    pdf = batch_df.select(*src_cols).toPandas()\n",
    "    if pdf.empty:\n",
    "        print(f\"[fb] ▶ batch_id={batch_id} empty; skip.\")\n",
    "        return\n",
    "\n",
    "    # Run inference\n",
    "    outs = list(infer_partition([pdf]))\n",
    "    df_out = outs[0]\n",
    "\n",
    "    # Fill lineage/timing\n",
    "    run_id = f\"run-{uuid.uuid4().hex}\"\n",
    "    now_ts = pd.Timestamp.utcnow()\n",
    "    df_out[\"inference_run_id\"] = run_id\n",
    "    if \"inference_ts\" not in df_out.columns or df_out[\"inference_ts\"].isna().all():\n",
    "        df_out[\"inference_ts\"] = now_ts\n",
    "    df_out[\"date\"] = pd.to_datetime(df_out[\"timestamp\"], errors=\"coerce\").dt.date.astype(str)\n",
    "\n",
    "    # Normalize fields\n",
    "    if \"offset\" in df_out.columns:\n",
    "        df_out[\"offset\"] = _to_int_or_none_series(df_out[\"offset\"])\n",
    "    if \"processing_latency_ms\" in df_out.columns:\n",
    "        # compute elapsed end-to-end per batch and stamp same number (simple, not per-row)\n",
    "        took_ms = int((time.time() - t0) * 1000)\n",
    "        df_out[\"processing_latency_ms\"] = took_ms\n",
    "\n",
    "    if \"dense_per_feature_error\" in df_out.columns:\n",
    "        df_out[\"dense_per_feature_error\"] = df_out[\"dense_per_feature_error\"].apply(_norm_map_float)\n",
    "    if \"raw_model_outputs\" in df_out.columns:\n",
    "        df_out[\"raw_model_outputs\"] = df_out[\"raw_model_outputs\"].apply(_norm_map_float)\n",
    "    if \"model_versions\" in df_out.columns:\n",
    "        df_out[\"model_versions\"] = df_out[\"model_versions\"].apply(_norm_versions)\n",
    "    if \"explain_top_k\" in df_out.columns:\n",
    "        df_out[\"explain_top_k\"] = df_out[\"explain_top_k\"].apply(_norm_explain_top_k)\n",
    "    if \"anomaly_severity\" in df_out.columns:\n",
    "        df_out[\"anomaly_severity\"] = pd.to_numeric(df_out[\"anomaly_severity\"], errors=\"coerce\").fillna(0).astype(int)\n",
    "\n",
    "    # Ensure column set & order\n",
    "    tgt_cols = [f.name for f in INFER_RESULTS_SCHEMA]\n",
    "    for c in tgt_cols:\n",
    "        if c not in df_out.columns:\n",
    "            df_out[c] = None\n",
    "    df_out = df_out[tgt_cols]\n",
    "\n",
    "    # String-stage everything, then parse/cast by schema\n",
    "    import json as _json\n",
    "    def _to_json_or_empty(v, empty):\n",
    "        try: return _json.dumps(v if v is not None else empty)\n",
    "        except Exception: return _json.dumps(empty)\n",
    "\n",
    "    def _to_string(v):\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)): return None\n",
    "        if isinstance(v, pd.Timestamp):\n",
    "            try: return v.tz_localize(\"UTC\").isoformat()\n",
    "            except Exception: return v.isoformat()\n",
    "        return str(v)\n",
    "\n",
    "    complex_schemas = {\n",
    "        \"dense_per_feature_error\": MapType(StringType(), DoubleType()),\n",
    "        \"raw_model_outputs\":       MapType(StringType(), DoubleType()),\n",
    "        \"model_versions\":          MapType(StringType(), StringType()),\n",
    "        \"explain_top_k\":           ArrayType(StructType([\n",
    "                                    StructField(\"feature\", StringType(), False),\n",
    "                                    StructField(\"contribution\", DoubleType(), False)\n",
    "                                  ])),\n",
    "    }\n",
    "\n",
    "    df_str = pd.DataFrame()\n",
    "    for c in tgt_cols:\n",
    "        if c in complex_schemas:\n",
    "            empty = {} if c != \"explain_top_k\" else []\n",
    "            df_str[c] = df_out[c].apply(lambda v: _to_json_or_empty(v, empty))\n",
    "        else:\n",
    "            df_str[c] = df_out[c].apply(_to_string)\n",
    "\n",
    "    string_schema = StructType([StructField(c, StringType(), True) for c in tgt_cols])\n",
    "    tmp_df = spark.createDataFrame(df_str, schema=string_schema)\n",
    "\n",
    "    # Parse/cast by name\n",
    "    select_exprs = []\n",
    "    def _is_simple(dt):\n",
    "        return isinstance(dt, (StringType, TimestampType, LongType, IntegerType, DoubleType, DateType))\n",
    "    for f in INFER_RESULTS_SCHEMA:\n",
    "        cname = f.name\n",
    "        col = F.col(f\"`{cname}`\")\n",
    "        if cname in complex_schemas:\n",
    "            col = F.from_json(col, complex_schemas[cname])\n",
    "        elif isinstance(f.dataType, (TimestampType, DateType)):\n",
    "            col = col.cast(f.dataType)\n",
    "        elif _is_simple(f.dataType):\n",
    "            col = col.cast(f.dataType)\n",
    "        select_exprs.append(col.alias(cname))\n",
    "    results_df = tmp_df.select(*select_exprs)\n",
    "\n",
    "    # MERGE by row_hash; newer inference_ts wins\n",
    "    tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "    cols = [f.name for f in INFER_RESULTS_SCHEMA]\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(results_df.alias(\"s\"), \"t.row_hash = s.row_hash\")\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"s.inference_ts > t.inference_ts\",\n",
    "            set={c: f\"s.`{c}`\" for c in cols}\n",
    "        )\n",
    "        .whenNotMatchedInsert(values={c: f\"s.`{c}`\" for c in cols})\n",
    "        .execute())\n",
    "\n",
    "    wrote = results_df.count()\n",
    "    took_ms = int((time.time() - t0) * 1000)\n",
    "    print(f\"[fb] ✓ batch_id={batch_id} rows_in={len(pdf)} rows_out={wrote} ms={took_ms}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "d91927a0-746e-4d2e-855a-fb8ba5a99393",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Streaming source prepared. (We won’t start it in this notebook.)\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\", \"false\")\n",
    "\n",
    "system_cols = [\n",
    "    StructField(\"row_hash\",    StringType(),    False),\n",
    "    StructField(\"timestamp\",   TimestampType(), True),\n",
    "    StructField(\"source_id\",   StringType(),    True),\n",
    "    StructField(\"kafka_key\",   StringType(),    True),\n",
    "    StructField(\"offset\",      LongType(),      True),\n",
    "    StructField(\"source_file\", StringType(),    True),\n",
    "    StructField(\"date\",        DateType(),      True),\n",
    "]\n",
    "feature_fields = [StructField(f, DoubleType(), True) for f in features]\n",
    "INFER_READY_SCHEMA = StructType(system_cols + feature_fields)\n",
    "\n",
    "infer_ready_glob = (INFER_READY / \"date=*\").as_posix()\n",
    "src = (\n",
    "    spark.readStream\n",
    "         .schema(INFER_READY_SCHEMA)\n",
    "         .option(\"maxFilesPerTrigger\", \"50\")\n",
    "         .parquet(infer_ready_glob)\n",
    ")\n",
    "print(\"Streaming source prepared. (We won’t start it in this notebook.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "6f1d269c-a774-4f7c-81ef-68b02ed12bef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prometheus metrics on :9109/metrics\n"
     ]
    }
   ],
   "source": [
    "import threading, http.server\n",
    "METRICS_PORT = 9109\n",
    "_metrics = {\"batches\":0, \"rows\":0}\n",
    "\n",
    "class Handler(http.server.BaseHTTPRequestHandler):\n",
    "    def do_GET(self):\n",
    "        if self.path != \"/metrics\":\n",
    "            self.send_response(404); self.end_headers(); return\n",
    "        self.send_response(200); self.end_headers()\n",
    "        out = \"\\n\".join([\n",
    "            f'engine_inference_batches_total {_metrics[\"batches\"]}',\n",
    "            f'engine_inference_rows_total {_metrics[\"rows\"]}',\n",
    "        ])\n",
    "        self.wfile.write(out.encode(\"utf-8\"))\n",
    "    def log_message(self, *args, **kwargs): pass\n",
    "\n",
    "def run_metrics():\n",
    "    with http.server.ThreadingHTTPServer((\"\", METRICS_PORT), Handler) as httpd:\n",
    "        print(f\"Prometheus metrics on :{METRICS_PORT}/metrics\")\n",
    "        httpd.serve_forever()\n",
    "\n",
    "t = threading.Thread(target=run_metrics, daemon=True); t.start()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "bd810074-f36e-49cd-8de1-59b8996c9cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Executors reachable. (Prewarm done)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "try:\n",
    "    _ = spark.sparkContext.parallelize([0], 1).map(lambda x: x).collect()\n",
    "    print(\"Executors reachable. (Prewarm done)\")\n",
    "except Exception as e:\n",
    "    print(\"Prewarm skip:\", e)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "9926c10b-c08d-44ca-b04d-8938321d4ecf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14A] Batch rows visible: 2000\n",
      "[14A] Prepared df_out_pdf. Sample cols: ['row_hash', 'timestamp', 'source_id', 'kafka_key', 'offset', 'source_file', 'date', 'Air_Fuel_Ratio_Commanded_:1']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "\n",
    "_system_cols = [\n",
    "    StructField(\"row_hash\",    StringType(),    False),\n",
    "    StructField(\"timestamp\",   TimestampType(), True),\n",
    "    StructField(\"source_id\",   StringType(),    True),\n",
    "    StructField(\"kafka_key\",   StringType(),    True),\n",
    "    StructField(\"offset\",      LongType(),      True),\n",
    "    StructField(\"source_file\", StringType(),    True),\n",
    "    StructField(\"date\",        DateType(),      True),\n",
    "]\n",
    "_feature_fields = [StructField(f, DoubleType(), True) for f in features]\n",
    "INFER_READY_SCHEMA = StructType(_system_cols + _feature_fields)\n",
    "\n",
    "infer_ready_glob = (INFER_READY / \"date=*\").as_posix()\n",
    "_sel_cols = [\"row_hash\",\"timestamp\",\"source_id\",\"kafka_key\",\"offset\",\"source_file\",\"date\"] + features\n",
    "\n",
    "df_batch = (\n",
    "    spark.read\n",
    "         .schema(INFER_READY_SCHEMA)\n",
    "         .parquet(infer_ready_glob)\n",
    "         .select(*_sel_cols)\n",
    ")\n",
    "total_rows = df_batch.count()\n",
    "print(f\"[14A] Batch rows visible: {total_rows}\")\n",
    "\n",
    "if total_rows == 0:\n",
    "    df_out_pdf = pd.DataFrame(columns=[f.name for f in INFER_RESULTS_SCHEMA])\n",
    "    print(\"[14A] Nothing to process.\")\n",
    "else:\n",
    "    pdf = df_batch.toPandas()\n",
    "    outs = list(infer_partition([pdf]))\n",
    "    df_out_pdf = outs[0].copy()\n",
    "\n",
    "    # Fill lineage/timing\n",
    "    run_id = f\"run-{uuid.uuid4().hex}\"\n",
    "    now_ts = pd.Timestamp.utcnow()\n",
    "    df_out_pdf[\"inference_run_id\"] = run_id\n",
    "    if \"inference_ts\" not in df_out_pdf.columns or df_out_pdf[\"inference_ts\"].isna().all():\n",
    "        df_out_pdf[\"inference_ts\"] = now_ts\n",
    "    df_out_pdf[\"date\"] = pd.to_datetime(df_out_pdf[\"timestamp\"], errors=\"coerce\").dt.date.astype(str)\n",
    "\n",
    "    # Normalize types that often bite\n",
    "    def _to_int_or_none_series(s: pd.Series) -> pd.Series:\n",
    "        tmp = pd.to_numeric(s, errors=\"coerce\")\n",
    "        return tmp.apply(lambda x: int(x) if pd.notna(x) else None)\n",
    "    if \"offset\" in df_out_pdf.columns:\n",
    "        df_out_pdf[\"offset\"] = _to_int_or_none_series(df_out_pdf[\"offset\"])\n",
    "    df_out_pdf[\"processing_latency_ms\"] = 0  # static run — set 0; IS-10 sets real ms per batch\n",
    "\n",
    "    print(\"[14A] Prepared df_out_pdf. Sample cols:\", df_out_pdf.columns[:8].tolist())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "49df324e-53cf-4a3a-ab4f-19a9d94a7526",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "gmm null frac: 1.0\n",
      "kde null frac: 0.0\n",
      "latency stats (ms): {'min': 0, 'p50': 0, 'max': 0}\n",
      "example row: [{'gmm_logp': None, 'kde_logp': -47.018585608970696, 'processing_latency_ms': 0}, {'gmm_logp': None, 'kde_logp': -47.018585608970696, 'processing_latency_ms': 0}, {'gmm_logp': None, 'kde_logp': -47.018585608970696, 'processing_latency_ms': 0}]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np, pandas as pd\n",
    "\n",
    "print(\"gmm null frac:\", float(pd.Series(df_out_pdf[\"gmm_logp\"]).isna().mean()))\n",
    "print(\"kde null frac:\", float(pd.Series(df_out_pdf[\"kde_logp\"]).isna().mean()))\n",
    "print(\"latency stats (ms):\", {\n",
    "    \"min\": int(pd.Series(df_out_pdf[\"processing_latency_ms\"]).min()),\n",
    "    \"p50\": int(pd.Series(df_out_pdf[\"processing_latency_ms\"]).median()),\n",
    "    \"max\": int(pd.Series(df_out_pdf[\"processing_latency_ms\"]).max())\n",
    "})\n",
    "print(\"example row:\", df_out_pdf[[\"gmm_logp\",\"kde_logp\",\"processing_latency_ms\"]].head(3).to_dict(orient=\"records\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "c4df038f-9d9f-4b50-ae2d-4d04f513f036",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[14B] inference_results MERGE complete. rows=2000\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *\n",
    "import pandas as pd, numpy as np, json as _json\n",
    "\n",
    "if len(df_out_pdf) == 0:\n",
    "    print(\"[14B] No results to write.\")\n",
    "else:\n",
    "    tgt_cols = [f.name for f in INFER_RESULTS_SCHEMA]\n",
    "    for c in tgt_cols:\n",
    "        if c not in df_out_pdf.columns:\n",
    "            df_out_pdf[c] = None\n",
    "    df_out_pdf = df_out_pdf[tgt_cols]\n",
    "\n",
    "    complex_schemas = {\n",
    "        \"dense_per_feature_error\": MapType(StringType(), DoubleType()),\n",
    "        \"raw_model_outputs\":       MapType(StringType(), DoubleType()),\n",
    "        \"model_versions\":          MapType(StringType(), StringType()),\n",
    "        \"explain_top_k\":           ArrayType(StructType([\n",
    "                                    StructField(\"feature\", StringType(), False),\n",
    "                                    StructField(\"contribution\", DoubleType(), False)\n",
    "                                  ])),\n",
    "    }\n",
    "\n",
    "    def _to_json_or_empty(v, empty):\n",
    "        try: return _json.dumps(v if v is not None else empty)\n",
    "        except Exception: return _json.dumps(empty)\n",
    "\n",
    "    def _to_string(v):\n",
    "        if v is None or (isinstance(v, float) and np.isnan(v)): return None\n",
    "        if isinstance(v, pd.Timestamp):\n",
    "            try: return v.tz_localize(\"UTC\").isoformat()\n",
    "            except Exception: return v.isoformat()\n",
    "        return str(v)\n",
    "\n",
    "    df_str = pd.DataFrame()\n",
    "    for c in tgt_cols:\n",
    "        if c in complex_schemas:\n",
    "            empty = {} if c != \"explain_top_k\" else []\n",
    "            df_str[c] = df_out_pdf[c].apply(lambda v: _to_json_or_empty(v, empty))\n",
    "        else:\n",
    "            df_str[c] = df_out_pdf[c].apply(_to_string)\n",
    "\n",
    "    string_schema = StructType([StructField(c, StringType(), True) for c in tgt_cols])\n",
    "    tmp_df = spark.createDataFrame(df_str, schema=string_schema)\n",
    "\n",
    "    select_exprs = []\n",
    "    def _is_simple(dt):\n",
    "        return isinstance(dt, (StringType, TimestampType, LongType, IntegerType, DoubleType, DateType))\n",
    "    for f in INFER_RESULTS_SCHEMA:\n",
    "        cname = f.name\n",
    "        col = F.col(f\"`{cname}`\")\n",
    "        if cname in complex_schemas:\n",
    "            col = F.from_json(col, complex_schemas[cname])\n",
    "        elif isinstance(f.dataType, (TimestampType, DateType)):\n",
    "            col = col.cast(f.dataType)\n",
    "        elif _is_simple(f.dataType):\n",
    "            col = col.cast(f.dataType)\n",
    "        select_exprs.append(col.alias(cname))\n",
    "    results_df = tmp_df.select(*select_exprs)\n",
    "\n",
    "    tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "    cols = [f.name for f in INFER_RESULTS_SCHEMA]\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(results_df.alias(\"s\"), \"t.row_hash = s.row_hash\")\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"s.inference_ts > t.inference_ts\",\n",
    "            set={c: f\"s.`{c}`\" for c in cols}\n",
    "        )\n",
    "        .whenNotMatchedInsert(values={c: f\"s.`{c}`\" for c in cols})\n",
    "        .execute())\n",
    "\n",
    "    wrote = results_df.count()\n",
    "    print(f\"[14B] inference_results MERGE complete. rows={wrote}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "066334bf-4840-4fb9-a063-fb9fcfbf9f51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "inference_results count: 2000\n",
      "schema:\n",
      "root\n",
      " |-- row_hash: string (nullable = true)\n",
      " |-- timestamp: timestamp (nullable = true)\n",
      " |-- date: string (nullable = true)\n",
      " |-- source_id: string (nullable = true)\n",
      " |-- kafka_key: string (nullable = true)\n",
      " |-- offset: long (nullable = true)\n",
      " |-- source_file: string (nullable = true)\n",
      " |-- Air_Fuel_Ratio_Commanded_:1: double (nullable = true)\n",
      " |-- Air_Fuel_Ratio_Measured_:1: double (nullable = true)\n",
      " |-- Catalyst_Temperature__Bank_1_Sensor_1: double (nullable = true)\n",
      " |-- Catalyst_Temperature__Bank_1_Sensor_2: double (nullable = true)\n",
      " |-- Engine_kW__At_the_wheels_kW: double (nullable = true)\n",
      " |-- Engine_Load_Absolute_pct: double (nullable = true)\n",
      " |-- Engine_Oil_Temperature: double (nullable = true)\n",
      " |-- Engine_RPM_rpm: double (nullable = true)\n",
      " |-- Fuel_flow_rate_hour_l_hr: double (nullable = true)\n",
      " |-- Fuel_Trim_Bank_1_Long_Term_pct: double (nullable = true)\n",
      " |-- Fuel_Trim_Bank_1_Short_Term_pct: double (nullable = true)\n",
      " |-- Mass_Air_Flow_Rate_g_s: double (nullable = true)\n",
      " |-- O2_Sensor1_Wide_Range_Current_mA: double (nullable = true)\n",
      " |-- O2_Bank_1_Sensor_2_Voltage_V: double (nullable = true)\n",
      " |-- Run_time_since_engine_start_s: double (nullable = true)\n",
      " |-- Timing_Advance: double (nullable = true)\n",
      " |-- Turbo_Boost_&_Vacuum_Gauge_psi: double (nullable = true)\n",
      " |-- Voltage__Control_Module_V: double (nullable = true)\n",
      " |-- Volumetric_Efficiency__Calculated_pct: double (nullable = true)\n",
      " |-- ECU_7EA:_Engine_Coolant_Temperature: double (nullable = true)\n",
      " |-- ECU_7EA:_Intake_Air_Temperature: double (nullable = true)\n",
      " |-- ECU_7EB:_Ambient_air_temp: double (nullable = true)\n",
      " |-- ECU_7EB:_Engine_Load_pct: double (nullable = true)\n",
      " |-- ECU_7EB:_Engine_RPM_rpm: double (nullable = true)\n",
      " |-- ECU_7EB:_Speed__OBD_km_h: double (nullable = true)\n",
      " |-- recon_error_dense: double (nullable = true)\n",
      " |-- dense_per_feature_error: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      " |-- recon_error_lstm: double (nullable = true)\n",
      " |-- lstm_window_id: string (nullable = true)\n",
      " |-- isolation_score: double (nullable = true)\n",
      " |-- kde_logp: double (nullable = true)\n",
      " |-- gmm_logp: double (nullable = true)\n",
      " |-- composite_score: double (nullable = true)\n",
      " |-- anomaly_label: string (nullable = true)\n",
      " |-- anomaly_severity: integer (nullable = true)\n",
      " |-- model_versions: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: string (valueContainsNull = true)\n",
      " |-- inference_run_id: string (nullable = true)\n",
      " |-- inference_ts: timestamp (nullable = true)\n",
      " |-- processing_latency_ms: long (nullable = true)\n",
      " |-- explain_top_k: array (nullable = true)\n",
      " |    |-- element: struct (containsNull = true)\n",
      " |    |    |-- feature: string (nullable = true)\n",
      " |    |    |-- contribution: double (nullable = true)\n",
      " |-- raw_model_outputs: map (nullable = true)\n",
      " |    |-- key: string\n",
      " |    |-- value: double (valueContainsNull = true)\n",
      "\n",
      "+-----------------------+---------------------+--------------+--------------+---------------------+-----------------------+\n",
      "|recon_error_dense_nulls|isolation_score_nulls|kde_logp_nulls|gmm_logp_nulls|composite_score_nulls|raw_model_outputs_nulls|\n",
      "+-----------------------+---------------------+--------------+--------------+---------------------+-----------------------+\n",
      "|0                      |0                    |0             |2000          |0                    |0                      |\n",
      "+-----------------------+---------------------+--------------+--------------+---------------------+-----------------------+\n",
      "\n",
      "sample rows:\n",
      "+----------------------------------------------------------------+-----------------------+------------------+-------------+-------------------+--------+-----------------+\n",
      "|row_hash                                                        |timestamp              |composite_score   |anomaly_label|kde_logp           |gmm_logp|raw_model_outputs|\n",
      "+----------------------------------------------------------------+-----------------------+------------------+-------------+-------------------+--------+-----------------+\n",
      "|d3d180f31455aa45ad65af08914f7e9038861eb75c800aeafa8747424a03a380|2025-09-16 18:00:15.797|0.3565672202523147|suspicious   |-47.018585608970696|NULL    |{}               |\n",
      "|d0b0fa020237ffd78f6120c946eabd80d2763cedc815ad267f5b9950cf9f504e|2025-09-16 18:00:15.797|0.6065672202523148|anomaly      |-47.018585608970696|NULL    |{}               |\n",
      "|5e2f4fb2d4a5b8b56f1caa1ee20a9427a6f078364ba8be5f7ebb42a0dde4ff9f|2025-09-16 18:00:15.797|0.6065672202523148|anomaly      |-47.018585608970696|NULL    |{}               |\n",
      "|4da988185369189b08a4902cedd6dfa4fc9cdae9f7d7c60b625fbd21abc39f68|2025-09-16 18:00:15.797|0.545708805926111 |anomaly      |-47.018585608970696|NULL    |{}               |\n",
      "|1e25ef1d3588f6a651a1307914cf91f9b529e4bf0afe9f2e1f99cbec61bd80e0|2025-09-16 18:00:15.8  |0.6172868646093548|anomaly      |-56.11623565703846 |NULL    |{}               |\n",
      "+----------------------------------------------------------------+-----------------------+------------------+-------------+-------------------+--------+-----------------+\n",
      "only showing top 5 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from delta.tables import DeltaTable\n",
    "import pandas as pd\n",
    "\n",
    "tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "df = tgt.toDF()\n",
    "\n",
    "print(\"inference_results count:\", df.count())\n",
    "print(\"schema:\")\n",
    "df.printSchema()\n",
    "\n",
    "# sanity: null counts for key cols\n",
    "key_cols = [\"recon_error_dense\",\"isolation_score\",\"kde_logp\",\"gmm_logp\",\"composite_score\",\"raw_model_outputs\"]\n",
    "agg = df.select([F.sum(F.col(c).isNull().cast(\"int\")).alias(c+\"_nulls\") for c in key_cols])\n",
    "agg.show(truncate=False)\n",
    "\n",
    "print(\"sample rows:\")\n",
    "df.select(\"row_hash\",\"timestamp\",\"composite_score\",\"anomaly_label\",\"kde_logp\",\"gmm_logp\",\"raw_model_outputs\").show(5, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "1e6b52be-2a35-447c-99bc-3bb20d00233e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[export] Wrote CSV: C:\\engine_module_pipeline\\infer_stage\\csv\\inference_results_20251003_044019.csv\n",
      "[export] JSON-encoded columns: ['dense_per_feature_error', 'explain_top_k', 'model_versions', 'raw_model_outputs']\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import os, shutil, glob, time\n",
    "from pathlib import Path\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import MapType, ArrayType, StructType\n",
    "\n",
    "assert 'spark' in globals(), \"SparkSession missing.\"\n",
    "assert 'INFER_RESULTS_DELTA' in globals(), \"INFER_RESULTS_DELTA path not set.\"\n",
    "assert 'INFER_RESULTS_SCHEMA' in globals(), \"INFER_RESULTS_SCHEMA not loaded.\"\n",
    "\n",
    "TARGET_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "TMP_DIR    = TARGET_DIR / \"_export_tmp\"\n",
    "TARGET_DIR.mkdir(parents=True, exist_ok=True)\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)\n",
    "\n",
    "# ---- Read Delta table ----\n",
    "tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "df  = tgt.toDF()\n",
    "\n",
    "# ---- Build select list: JSON-encode complex cols, keep others as-is ----\n",
    "complex_cols = set()\n",
    "sel_exprs = []\n",
    "schema_by_name = {f.name: f.dataType for f in INFER_RESULTS_SCHEMA}\n",
    "\n",
    "for f in INFER_RESULTS_SCHEMA:\n",
    "    c = f.name\n",
    "    dt = schema_by_name.get(c)\n",
    "    col = F.col(f\"`{c}`\")\n",
    "    if isinstance(dt, (MapType, ArrayType, StructType)):\n",
    "        complex_cols.add(c)\n",
    "        # Convert complex types to a compact JSON string column for CSV\n",
    "        col = F.to_json(col)\n",
    "    # else: leave simple numeric/string/timestamp/date columns as they are\n",
    "    sel_exprs.append(col.alias(c))\n",
    "\n",
    "df_flat = df.select(*sel_exprs)\n",
    "\n",
    "if df_flat.rdd.isEmpty():\n",
    "    raise FileNotFoundError(\"inference_results has no rows to export yet.\")\n",
    "\n",
    "# ---- Write single CSV part with header ----\n",
    "(df_flat\n",
    "    .coalesce(1)\n",
    "    .write\n",
    "    .mode(\"overwrite\")\n",
    "    .option(\"header\", \"true\")\n",
    "    .option(\"quote\", '\"')\n",
    "    .option(\"escape\", '\"')\n",
    "    .csv(str(TMP_DIR))\n",
    ")\n",
    "\n",
    "# ---- Move part file to final timestamped name ----\n",
    "part_files = glob.glob(str(TMP_DIR / \"part-*.csv\"))\n",
    "if not part_files:\n",
    "    raise FileNotFoundError(\"No CSV part file produced. (Check permissions/space.)\")\n",
    "part_path = Path(part_files[0])\n",
    "\n",
    "ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "final_path = TARGET_DIR / f\"inference_results_{ts}.csv\"\n",
    "\n",
    "if final_path.exists():\n",
    "    final_path.unlink()\n",
    "shutil.move(str(part_path), str(final_path))\n",
    "\n",
    "# cleanup Spark markers\n",
    "for p in (TMP_DIR / \"_SUCCESS\",):\n",
    "    try: p.unlink()\n",
    "    except Exception: pass\n",
    "shutil.rmtree(TMP_DIR, ignore_errors=True)\n",
    "\n",
    "print(f\"[export] Wrote CSV: {final_path}\")\n",
    "print(f\"[export] JSON-encoded columns: {sorted(complex_cols)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4060ebc-7c3f-4ec2-af57-d72ba1a7c229",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81805ad1-2c81-4ca6-ad37-13917b634fbf",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d8eddc3-4049-4568-b8c0-f2ea3336d44f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81304acf-918a-49d2-9e1b-96bc91c62656",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "00c2a44d-bf8e-4e83-9e44-1edf735fd6a3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GMM-1r] Indexed GMM files: 25/25\n",
      "[GMM-1r] Loadable GMMs: 25/25\n"
     ]
    }
   ],
   "source": [
    "import os, json, glob\n",
    "from pathlib import Path\n",
    "import joblib\n",
    "\n",
    "# --- where artifacts live (adjust only if yours differ) ---\n",
    "ARTIFACTS_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\artifacts\")\n",
    "assert ARTIFACTS_DIR.exists(), f\"Artifacts not found: {ARTIFACTS_DIR}\"\n",
    "\n",
    "# --- load feature contract (single source of truth) ---\n",
    "contract_path = ARTIFACTS_DIR / \"model_input_contract.json\"\n",
    "with open(contract_path, \"r\", encoding=\"utf-8\") as fh:\n",
    "    contract = json.load(fh)\n",
    "\n",
    "features = contract[\"features_canonical_order\"]\n",
    "colon_token = contract.get(\"per_feature_filename_rule\", {}).get(\"colon\", \"__COLON__\")\n",
    "\n",
    "def _desanitize_token(s: str) -> str:\n",
    "    # turn filename token back into canonical colon\n",
    "    return s.replace(colon_token, \":\")\n",
    "\n",
    "# Build index by scanning *.joblib and mapping back to canonical names\n",
    "_GMM_INDEX = {}            # canonical feature -> full path\n",
    "_GMM_CACHE = {}            # loaded models cache\n",
    "\n",
    "for p in glob.glob(str(ARTIFACTS_DIR / \"gmm_*.joblib\")):\n",
    "    base = os.path.basename(p)          # e.g., gmm_Air_Fuel_Ratio_Commanded__COLON__1.joblib\n",
    "    stem = base[len(\"gmm_\"):-len(\".joblib\")]\n",
    "    canonical = _desanitize_token(stem)\n",
    "    if canonical in features:\n",
    "        _GMM_INDEX[canonical] = p\n",
    "\n",
    "print(f\"[GMM-1r] Indexed GMM files: {len(_GMM_INDEX)}/{len(features)}\")\n",
    "missing = [f for f in features if f not in _GMM_INDEX]\n",
    "if missing:\n",
    "    print(\"[GMM-1r] Missing models for:\", missing[:10], \"...\" if len(missing)>10 else \"\")\n",
    "\n",
    "def get_gmm_for(feature: str):\n",
    "    \"\"\"Return a loaded sklearn.mixture.GaussianMixture for the canonical feature name, or None.\"\"\"\n",
    "    if feature in _GMM_CACHE:\n",
    "        return _GMM_CACHE[feature]\n",
    "    p = _GMM_INDEX.get(feature)\n",
    "    if not p or not os.path.exists(p):\n",
    "        return None\n",
    "    try:\n",
    "        m = joblib.load(p)\n",
    "        _GMM_CACHE[feature] = m\n",
    "        return m\n",
    "    except Exception as e:\n",
    "        print(f\"[GMM-1r] load failed for {feature}: {e}\")\n",
    "        return None\n",
    "\n",
    "# quick smoke test: how many GMMs can actually be loaded?\n",
    "loaded_ok = sum(1 for f in features if get_gmm_for(f) is not None)\n",
    "print(f\"[GMM-1r] Loadable GMMs: {loaded_ok}/{len(features)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "id": "9d998ca0-9746-4893-9eab-ddaba2edaf63",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[GMM-2rr] Rows needing gmm_logp backfill: 2000\n",
      "[GMM-2rr] GMMs available for features: 25/25\n",
      "[GMM-2rr] Pre-merge: rows=2000 nulls=0 mean=-4.731006930828524 min=-74.09407025208272 max=54.07666622201737\n",
      "+----+---------------+------------------+------------------+-----------------+\n",
      "|rows|gmm_nulls_total|gmm_mean          |gmm_min           |gmm_max          |\n",
      "+----+---------------+------------------+------------------+-----------------+\n",
      "|2000|0              |-4.731006930828524|-74.09407025208272|54.07666622201737|\n",
      "+----+---------------+------------------+------------------+-----------------+\n",
      "\n",
      "[GMM-2rr] Backfill complete.\n"
     ]
    }
   ],
   "source": [
    "# GMM-2rr — compute and MERGE only gmm_logp (fixed: use score_samples) + richer diagnostics\n",
    "\n",
    "from typing import Iterator\n",
    "import numpy as np, pandas as pd\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, DoubleType\n",
    "\n",
    "# Reuse: ARTIFACTS_DIR, features, scaler, get_gmm_for must already exist from GMM-1r & your notebook.\n",
    "# Quick asserts to avoid silent fallbacks:\n",
    "assert 'features' in globals() and len(features) == 25, \"features (25) not bound\"\n",
    "assert 'scaler' in globals(), \"scaler not loaded\"\n",
    "assert 'get_gmm_for' in globals(), \"get_gmm_for() missing (run GMM-1r first)\"\n",
    "assert 'INFER_RESULTS_DELTA' in globals(), \"target delta path missing\"\n",
    "\n",
    "# Read rows that need gmm_logp\n",
    "need_cols = [\"row_hash\", \"inference_ts\"] + features + [\"gmm_logp\"]\n",
    "df_in = (spark.read.format(\"delta\")\n",
    "         .load(str(INFER_RESULTS_DELTA))\n",
    "         .select(*need_cols)\n",
    "         .where(F.col(\"gmm_logp\").isNull()))\n",
    "\n",
    "todo = df_in.count()\n",
    "print(f\"[GMM-2rr] Rows needing gmm_logp backfill: {todo}\")\n",
    "if todo == 0:\n",
    "    print(\"[GMM-2rr] Nothing to do.\")\n",
    "else:\n",
    "    # Preflight: how many features have usable GMMs?\n",
    "    avail = sum(1 for f in features if get_gmm_for(f) is not None)\n",
    "    print(f\"[GMM-2rr] GMMs available for features: {avail}/{len(features)}\")\n",
    "\n",
    "    out_schema = StructType([\n",
    "        StructField(\"row_hash\",     StringType(),    False),\n",
    "        StructField(\"inference_ts\", TimestampType(), True),\n",
    "        StructField(\"gmm_logp\",     DoubleType(),    True),\n",
    "    ])\n",
    "\n",
    "    # Preload models once per task to avoid repeated disk IO\n",
    "    _GMM_LOCAL = {f: get_gmm_for(f) for f in features}\n",
    "\n",
    "    def _score_gmm_chunks(pdf_iter: Iterator[pd.DataFrame]) -> Iterator[pd.DataFrame]:\n",
    "        for pdf in pdf_iter:\n",
    "            if pdf is None or pdf.empty:\n",
    "                yield pd.DataFrame(columns=[\"row_hash\",\"inference_ts\",\"gmm_logp\"])\n",
    "                continue\n",
    "\n",
    "            # Ensure all features present\n",
    "            for f in features:\n",
    "                if f not in pdf.columns:\n",
    "                    pdf[f] = np.nan\n",
    "\n",
    "            # Scale\n",
    "            X = pdf[features].astype(float).values\n",
    "            Xs = scaler.transform(np.nan_to_num(X, copy=False))\n",
    "\n",
    "            # Row-wise aggregate log-prob across available feature models\n",
    "            gmm_vals = np.full((len(pdf),), np.nan, dtype=float)\n",
    "            # simple diagnostics for this partition\n",
    "            used_feat_counts = np.zeros((len(pdf),), dtype=int)\n",
    "\n",
    "            for j, feat in enumerate(features):\n",
    "                m = _GMM_LOCAL.get(feat)\n",
    "                if m is None:\n",
    "                    continue\n",
    "                try:\n",
    "                    # score_samples expects 2D; pass the column j as (n,1)\n",
    "                    col = Xs[:, [j]]\n",
    "                    lp = m.score_samples(col)  # shape: (n,)\n",
    "                    # accumulate; if any are nan/inf, treat safely\n",
    "                    mask = np.isfinite(lp)\n",
    "                    gmm_vals[mask] = np.where(np.isfinite(gmm_vals[mask]), gmm_vals[mask] + lp[mask], lp[mask])\n",
    "                    used_feat_counts[mask] += 1\n",
    "                except Exception as e:\n",
    "                    # continue; one bad feature shouldn't block others\n",
    "                    # optional: print(f\"[GMM-2rr] feature {feat} score_samples error: {e}\")\n",
    "                    pass\n",
    "\n",
    "            # Where no features contributed, leave as NaN\n",
    "            out = pd.DataFrame({\n",
    "                \"row_hash\": pdf[\"row_hash\"].astype(str).values,\n",
    "                \"inference_ts\": pd.to_datetime(pdf[\"inference_ts\"], utc=True),\n",
    "                \"gmm_logp\": gmm_vals,\n",
    "            })\n",
    "\n",
    "            # tiny debug for first few rows in this chunk\n",
    "            if len(out) > 0:\n",
    "                c0 = int(used_feat_counts[:10].mean())\n",
    "                print(f\"[GMM-2rr][chunk] first10 mean contributing feats: {c0}\")\n",
    "            yield out\n",
    "\n",
    "    df_updates = df_in.mapInPandas(_score_gmm_chunks, schema=out_schema)\n",
    "\n",
    "    # Pre-merge diagnostics\n",
    "    diag = df_updates.select(\n",
    "        F.count(\"*\").alias(\"rows\"),\n",
    "        F.count(F.when(F.col(\"gmm_logp\").isNull(), True)).alias(\"nulls\"),\n",
    "        F.mean(\"gmm_logp\").alias(\"mean_gmm_logp\"),\n",
    "        F.min(\"gmm_logp\").alias(\"min_gmm_logp\"),\n",
    "        F.max(\"gmm_logp\").alias(\"max_gmm_logp\")\n",
    "    ).collect()[0]\n",
    "    print(f\"[GMM-2rr] Pre-merge: rows={diag['rows']} nulls={diag['nulls']} \"\n",
    "          f\"mean={diag['mean_gmm_logp']} min={diag['min_gmm_logp']} max={diag['max_gmm_logp']}\")\n",
    "\n",
    "    # MERGE into inference_results by row_hash (keep newest inference_ts)\n",
    "    tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(df_updates.alias(\"s\"), \"t.row_hash = s.row_hash\")\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"s.inference_ts >= t.inference_ts\",\n",
    "            set={\"gmm_logp\": \"s.gmm_logp\"}\n",
    "        )\n",
    "        .execute())\n",
    "\n",
    "    # Post-merge verification\n",
    "    post = (spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA))\n",
    "            .select(\n",
    "                F.count(\"*\").alias(\"rows\"),\n",
    "                F.count(F.when(F.col(\"gmm_logp\").isNull(), True)).alias(\"gmm_nulls_total\"),\n",
    "                F.mean(\"gmm_logp\").alias(\"gmm_mean\"),\n",
    "                F.min(\"gmm_logp\").alias(\"gmm_min\"),\n",
    "                F.max(\"gmm_logp\").alias(\"gmm_max\")\n",
    "            ))\n",
    "    post.show(truncate=False)\n",
    "    print(\"[GMM-2rr] Backfill complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "c0791330-56ea-411e-871b-a4908ac86020",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----+------------------+------------------+------------------+\n",
      "|rows|comp_mean         |comp_min          |comp_max          |\n",
      "+----+------------------+------------------+------------------+\n",
      "|2000|0.5391502616027245|0.3604706184902124|0.8094386903476654|\n",
      "+----+------------------+------------------+------------------+\n",
      "\n",
      "[GMM-3] composite/labels refreshed from gmm-inclusive signals.\n"
     ]
    }
   ],
   "source": [
    "# GMM-3 — recompute composite_score, anomaly_label, anomaly_severity now that gmm_logp is filled\n",
    "\n",
    "from delta.tables import DeltaTable\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import (\n",
    "    StructType, StructField, StringType, TimestampType, DoubleType, IntegerType\n",
    ")\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import math\n",
    "\n",
    "# Reuse BASELINES/WEIGHTS & label function from your notebook; just mirror them here to keep cell self-contained\n",
    "BASELINES = {\n",
    "    \"dense_med\": 0.05, \"dense_mad\": 0.05,\n",
    "    \"lstm_med\":  0.05, \"lstm_mad\":  0.05,\n",
    "    \"isof_med\":  0.0,  \"isof_mad\":  0.5,\n",
    "    \"kde_med\":  -10.0, \"kde_mad\":   3.0,\n",
    "    \"gmm_med\":  -10.0, \"gmm_mad\":   3.0,\n",
    "}\n",
    "WEIGHTS = {\"dense\":0.35, \"lstm\":0.25, \"isof\":0.20, \"kde\":0.10, \"gmm\":0.10}\n",
    "\n",
    "def robust_norm(x, med, mad, invert=False, k=1.4826):\n",
    "    if x is None or (isinstance(x, float) and (np.isnan(x) or np.isinf(x))):\n",
    "        return 0.0\n",
    "    denom = (mad*k) if mad and mad>0 else 1.0\n",
    "    z = (x - med)/denom\n",
    "    if invert: z = -z\n",
    "    return 1/(1+math.exp(-z))\n",
    "\n",
    "def composite_from_raw(dense_e, lstm_e, isof_s, kde_lp, gmm_lp):\n",
    "    s_dense = robust_norm(dense_e, BASELINES[\"dense_med\"], BASELINES[\"dense_mad\"])\n",
    "    s_lstm  = robust_norm(lstm_e,  BASELINES[\"lstm_med\"],  BASELINES[\"lstm_mad\"])\n",
    "    s_isof  = robust_norm(isof_s,  BASELINES[\"isof_med\"],  BASELINES[\"isof_mad\"], invert=True)\n",
    "    s_kde   = robust_norm(kde_lp,  BASELINES[\"kde_med\"],   BASELINES[\"kde_mad\"],  invert=True)\n",
    "    s_gmm   = robust_norm(gmm_lp,  BASELINES[\"gmm_med\"],   BASELINES[\"gmm_mad\"],  invert=True)\n",
    "    return (WEIGHTS[\"dense\"]*s_dense +\n",
    "            WEIGHTS[\"lstm\"] *s_lstm  +\n",
    "            WEIGHTS[\"isof\"] *s_isof  +\n",
    "            WEIGHTS[\"kde\"]  *s_kde   +\n",
    "            WEIGHTS[\"gmm\"]  *s_gmm)\n",
    "\n",
    "def label_from_composite(c):\n",
    "    if c is None or (isinstance(c,float) and np.isnan(c)):\n",
    "        return \"unknown\", 0\n",
    "    if c < 0.20: return \"normal\", 0\n",
    "    if c < 0.50: return \"suspicious\", 1\n",
    "    if c < 0.75: return \"anomaly\", 2\n",
    "    return \"critical\", 3\n",
    "\n",
    "need_cols = [\"row_hash\",\"inference_ts\",\n",
    "             \"recon_error_dense\",\"recon_error_lstm\",\n",
    "             \"isolation_score\",\"kde_logp\",\"gmm_logp\",\n",
    "             \"composite_score\",\"anomaly_label\",\"anomaly_severity\"]\n",
    "\n",
    "df_base = (spark.read.format(\"delta\")\n",
    "           .load(str(INFER_RESULTS_DELTA))\n",
    "           .select(*need_cols))\n",
    "\n",
    "# Recompute for all rows, or restrict to rows where composite is null or changed\n",
    "df_needs = df_base  # keep simple & deterministic\n",
    "\n",
    "out_schema = StructType([\n",
    "    StructField(\"row_hash\",        StringType(),    False),\n",
    "    StructField(\"inference_ts\",    TimestampType(), True),\n",
    "    StructField(\"composite_score\", DoubleType(),    True),\n",
    "    StructField(\"anomaly_label\",   StringType(),    True),\n",
    "    StructField(\"anomaly_severity\",IntegerType(),   True),\n",
    "])\n",
    "\n",
    "def _recompute(pdf_iter):\n",
    "    for pdf in pdf_iter:\n",
    "        if pdf is None or pdf.empty:\n",
    "            yield pd.DataFrame(columns=[f.name for f in out_schema])\n",
    "            continue\n",
    "\n",
    "        # Compute composite row-wise\n",
    "        comp = []\n",
    "        lab  = []\n",
    "        sev  = []\n",
    "        for _, r in pdf.iterrows():\n",
    "            c = composite_from_raw(\n",
    "                r.get(\"recon_error_dense\"),\n",
    "                r.get(\"recon_error_lstm\"),\n",
    "                r.get(\"isolation_score\"),\n",
    "                r.get(\"kde_logp\"),\n",
    "                r.get(\"gmm_logp\")\n",
    "            )\n",
    "            l, s = label_from_composite(c)\n",
    "            comp.append(float(c))\n",
    "            lab.append(l)\n",
    "            sev.append(int(s))\n",
    "\n",
    "        out = pd.DataFrame({\n",
    "            \"row_hash\": pdf[\"row_hash\"].astype(str).values,\n",
    "            \"inference_ts\": pd.to_datetime(pdf[\"inference_ts\"], utc=True),\n",
    "            \"composite_score\": comp,\n",
    "            \"anomaly_label\": lab,\n",
    "            \"anomaly_severity\": sev,\n",
    "        })\n",
    "        yield out\n",
    "\n",
    "df_updates = df_needs.mapInPandas(_recompute, schema=out_schema)\n",
    "\n",
    "# quick diagnostics\n",
    "df_updates.select(\n",
    "    F.count(\"*\").alias(\"rows\"),\n",
    "    F.mean(\"composite_score\").alias(\"comp_mean\"),\n",
    "    F.min(\"composite_score\").alias(\"comp_min\"),\n",
    "    F.max(\"composite_score\").alias(\"comp_max\")\n",
    ").show(truncate=False)\n",
    "\n",
    "# MERGE by row_hash; keep newest inference_ts\n",
    "tgt = DeltaTable.forPath(spark, str(INFER_RESULTS_DELTA))\n",
    "(tgt.alias(\"t\")\n",
    "    .merge(df_updates.alias(\"s\"), \"t.row_hash = s.row_hash\")\n",
    "    .whenMatchedUpdate(\n",
    "        condition=\"s.inference_ts >= t.inference_ts\",\n",
    "        set={\n",
    "            \"composite_score\": \"s.composite_score\",\n",
    "            \"anomaly_label\":   \"s.anomaly_label\",\n",
    "            \"anomaly_severity\":\"s.anomaly_severity\",\n",
    "        }\n",
    "    )\n",
    "    .execute())\n",
    "\n",
    "print(\"[GMM-3] composite/labels refreshed from gmm-inclusive signals.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "20b2c678-eb8a-48d4-b397-82212db5234c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CSV-2] Wrote CSV to folder: C:/engine_module_pipeline/infer_stage/csv/inference_results.csv\n"
     ]
    }
   ],
   "source": [
    "# CSV-2 — export inference_results to CSV with complex columns flattened\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import StringType\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_all = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA))\n",
    "\n",
    "# Flatten complex columns to strings (JSON)\n",
    "to_json = F.to_json\n",
    "df_flat = (df_all\n",
    "    .withColumn(\"dense_per_feature_error_json\", to_json(\"dense_per_feature_error\"))\n",
    "    .withColumn(\"explain_top_k_json\",           to_json(\"explain_top_k\"))\n",
    "    .withColumn(\"model_versions_json\",          to_json(\"model_versions\"))\n",
    "    .withColumn(\"raw_model_outputs_json\",       to_json(\"raw_model_outputs\"))\n",
    ")\n",
    "\n",
    "# Select order: keep the important scalar cols + JSON strings at the end\n",
    "scalar_cols = [\n",
    "    \"row_hash\",\"timestamp\",\"date\",\"source_id\",\"kafka_key\",\"offset\",\"source_file\"\n",
    "] + features + [\n",
    "    \"recon_error_dense\",\"recon_error_lstm\",\"lstm_window_id\",\n",
    "    \"isolation_score\",\"kde_logp\",\"gmm_logp\",\n",
    "    \"composite_score\",\"anomaly_label\",\"anomaly_severity\",\n",
    "    \"inference_run_id\",\"inference_ts\",\"processing_latency_ms\"\n",
    "]\n",
    "\n",
    "export_cols = [c for c in scalar_cols if c in df_flat.columns] + [\n",
    "    \"dense_per_feature_error_json\",\"explain_top_k_json\",\"model_versions_json\",\"raw_model_outputs_json\"\n",
    "]\n",
    "\n",
    "out_path = (CSV_OUT_DIR / \"inference_results.csv\").as_posix()\n",
    "(df_flat.select(*export_cols)\n",
    "        .coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\",\"true\")\n",
    "        .csv(out_path))\n",
    "\n",
    "print(f\"[CSV-2] Wrote CSV to folder: {out_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b83a15-7fb6-432b-b702-90ea79c2ca4b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "157fe249-af8c-44d3-9731-95bfc6a867ed",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90031986-d16a-4cd9-aa52-b06cafa8a1f5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0c93ffb-8c60-4b85-948d-e6b1d967cc49",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "d989cf9d-14aa-43af-b1e9-a7434628969a",
   "metadata": {},
   "outputs": [
    {
     "ename": "PySparkValueError",
     "evalue": "[SLICE_WITH_STEP] Slice with step is not supported.",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mPySparkValueError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 38\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_top_feats\u001b[39m(col):\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# col is array<struct<feature:string, contribution:double>>\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.transform(\n\u001b[32m     17\u001b[39m         F.slice(\n\u001b[32m     18\u001b[39m             F.array_sort(\n\u001b[32m   (...)\u001b[39m\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: F.col(\u001b[33m\"\u001b[39m\u001b[33mx.f\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     )\n\u001b[32m     25\u001b[39m alerts_df = (\n\u001b[32m     26\u001b[39m     df_ir\n\u001b[32m     27\u001b[39m     .filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mcomposite_score\u001b[39m\u001b[33m\"\u001b[39m).isNotNull() & (F.col(\u001b[33m\"\u001b[39m\u001b[33mcomposite_score\u001b[39m\u001b[33m\"\u001b[39m) >= F.lit(ALERT_THRESHOLD)))\n\u001b[32m     28\u001b[39m     .select(\n\u001b[32m     29\u001b[39m         F.expr(\u001b[33m\"\u001b[39m\u001b[33muuid()\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33malert_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     30\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33minference_ts\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33malert_ts\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     31\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mrow_hash\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     32\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33msource_id\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mvehicle_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     33\u001b[39m         F.lit(\u001b[33m\"\u001b[39m\u001b[33mcomposite_high\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33malert_type\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     34\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33manomaly_severity\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mint\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mseverity\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     35\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mcomposite_score\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     36\u001b[39m         F.array(F.lit(\u001b[33m\"\u001b[39m\u001b[33mdense\u001b[39m\u001b[33m\"\u001b[39m),F.lit(\u001b[33m\"\u001b[39m\u001b[33mlstm\u001b[39m\u001b[33m\"\u001b[39m),F.lit(\u001b[33m\"\u001b[39m\u001b[33misof\u001b[39m\u001b[33m\"\u001b[39m),F.lit(\u001b[33m\"\u001b[39m\u001b[33mkde\u001b[39m\u001b[33m\"\u001b[39m),F.lit(\u001b[33m\"\u001b[39m\u001b[33mgmm\u001b[39m\u001b[33m\"\u001b[39m)).alias(\u001b[33m\"\u001b[39m\u001b[33mtriggering_models\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     37\u001b[39m         F.lit(\u001b[38;5;28;01mNone\u001b[39;00m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mreason\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m---> \u001b[39m\u001b[32m38\u001b[39m         \u001b[43m_top_feats\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mexplain_top_k\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m.alias(\u001b[33m\"\u001b[39m\u001b[33mtop_features\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     39\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mmodel_versions\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     40\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33minference_run_id\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     41\u001b[39m         F.lit(\u001b[38;5;28;01mFalse\u001b[39;00m).cast(BooleanType()).alias(\u001b[33m\"\u001b[39m\u001b[33macked\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     42\u001b[39m         F.lit(\u001b[38;5;28;01mNone\u001b[39;00m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33macked_by\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     43\u001b[39m         F.lit(\u001b[38;5;28;01mNone\u001b[39;00m).cast(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33macked_ts\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     44\u001b[39m         F.array().cast(ArrayType(StringType())).alias(\u001b[33m\"\u001b[39m\u001b[33mnotified_channels\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     45\u001b[39m         F.array(F.col(\u001b[33m\"\u001b[39m\u001b[33mrow_hash\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m)).alias(\u001b[33m\"\u001b[39m\u001b[33mlinked_rows\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     46\u001b[39m         F.map().alias(\u001b[33m\"\u001b[39m\u001b[33mextra\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     47\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     48\u001b[39m     )\n\u001b[32m     49\u001b[39m )\n\u001b[32m     51\u001b[39m \u001b[38;5;66;03m# append\u001b[39;00m\n\u001b[32m     52\u001b[39m alerts_df.write.format(\u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m).mode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m).save(\u001b[38;5;28mstr\u001b[39m(ALERTS_DELTA))\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[84]\u001b[39m\u001b[32m, line 18\u001b[39m, in \u001b[36m_top_feats\u001b[39m\u001b[34m(col)\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_top_feats\u001b[39m(col):\n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# col is array<struct<feature:string, contribution:double>>\u001b[39;00m\n\u001b[32m     16\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m F.transform(\n\u001b[32m     17\u001b[39m         F.slice(\n\u001b[32m---> \u001b[39m\u001b[32m18\u001b[39m             \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43marray_sort\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     19\u001b[39m \u001b[43m                \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mtransform\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43ms\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms.contribution\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mc\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcol\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43ms.feature\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43malias\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mf\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m \u001b[43m            \u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m:\u001b[49m\u001b[43m-\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m]\u001b[49m, \u001b[32m1\u001b[39m, \u001b[32m3\u001b[39m\n\u001b[32m     21\u001b[39m         ),\n\u001b[32m     22\u001b[39m         \u001b[38;5;28;01mlambda\u001b[39;00m x: F.col(\u001b[33m\"\u001b[39m\u001b[33mx.f\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     23\u001b[39m     )\n",
      "\u001b[36mFile \u001b[39m\u001b[32mC:\\engine_module_pipeline\\.venv-spark\\Lib\\site-packages\\pyspark\\sql\\column.py:709\u001b[39m, in \u001b[36mColumn.__getitem__\u001b[39m\u001b[34m(self, k)\u001b[39m\n\u001b[32m    707\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(k, \u001b[38;5;28mslice\u001b[39m):\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m k.step \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m709\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m PySparkValueError(\n\u001b[32m    710\u001b[39m             error_class=\u001b[33m\"\u001b[39m\u001b[33mSLICE_WITH_STEP\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m    711\u001b[39m             message_parameters={},\n\u001b[32m    712\u001b[39m         )\n\u001b[32m    713\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m.substr(k.start, k.stop)\n\u001b[32m    714\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n",
      "\u001b[31mPySparkValueError\u001b[39m: [SLICE_WITH_STEP] Slice with step is not supported."
     ]
    }
   ],
   "source": [
    "# DER-ALERTS — derive alerts Delta from inference_results and export CSV\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import ArrayType, StringType, BooleanType\n",
    "from delta.tables import DeltaTable\n",
    "from pathlib import Path\n",
    "\n",
    "ALERT_THRESHOLD = 0.75\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_ir = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA))\n",
    "\n",
    "# derive alerts\n",
    "def _top_feats(col):\n",
    "    # col is array<struct<feature:string, contribution:double>>\n",
    "    return F.transform(\n",
    "        F.slice(\n",
    "            F.array_sort(\n",
    "                F.transform(col, lambda s: F.struct(F.col(\"s.contribution\").alias(\"c\"), F.col(\"s.feature\").alias(\"f\")))\n",
    "            )[::-1], 1, 3\n",
    "        ),\n",
    "        lambda x: F.col(\"x.f\")\n",
    "    )\n",
    "\n",
    "alerts_df = (\n",
    "    df_ir\n",
    "    .filter(F.col(\"composite_score\").isNotNull() & (F.col(\"composite_score\") >= F.lit(ALERT_THRESHOLD)))\n",
    "    .select(\n",
    "        F.expr(\"uuid()\").alias(\"alert_id\"),\n",
    "        F.col(\"inference_ts\").alias(\"alert_ts\"),\n",
    "        F.col(\"row_hash\"),\n",
    "        F.col(\"source_id\").cast(\"string\").alias(\"vehicle_id\"),\n",
    "        F.lit(\"composite_high\").alias(\"alert_type\"),\n",
    "        F.col(\"anomaly_severity\").cast(\"int\").alias(\"severity\"),\n",
    "        F.col(\"composite_score\"),\n",
    "        F.array(F.lit(\"dense\"),F.lit(\"lstm\"),F.lit(\"isof\"),F.lit(\"kde\"),F.lit(\"gmm\")).alias(\"triggering_models\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"reason\"),\n",
    "        _top_feats(F.col(\"explain_top_k\")).alias(\"top_features\"),\n",
    "        F.col(\"model_versions\"),\n",
    "        F.col(\"inference_run_id\"),\n",
    "        F.lit(False).cast(BooleanType()).alias(\"acked\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"acked_by\"),\n",
    "        F.lit(None).cast(\"timestamp\").alias(\"acked_ts\"),\n",
    "        F.array().cast(ArrayType(StringType())).alias(\"notified_channels\"),\n",
    "        F.array(F.col(\"row_hash\").cast(\"string\")).alias(\"linked_rows\"),\n",
    "        F.map().alias(\"extra\"),\n",
    "        F.col(\"date\").cast(\"string\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# append\n",
    "alerts_df.write.format(\"delta\").mode(\"append\").save(str(ALERTS_DELTA))\n",
    "print(\"[DER-ALERTS] wrote:\", alerts_df.count())\n",
    "\n",
    "# CSV (flatten maps/arrays to JSON)\n",
    "alerts_csv = (\n",
    "    alerts_df\n",
    "    .withColumn(\"triggering_models_json\", F.to_json(\"triggering_models\"))\n",
    "    .withColumn(\"top_features_json\",      F.to_json(\"top_features\"))\n",
    "    .withColumn(\"model_versions_json\",    F.to_json(\"model_versions\"))\n",
    "    .withColumn(\"notified_channels_json\", F.to_json(\"notified_channels\"))\n",
    "    .withColumn(\"linked_rows_json\",       F.to_json(\"linked_rows\"))\n",
    "    .withColumn(\"extra_json\",             F.to_json(\"extra\"))\n",
    ")\n",
    "\n",
    "(alerts_csv\n",
    " .drop(\"triggering_models\",\"top_features\",\"model_versions\",\"notified_channels\",\"linked_rows\",\"extra\")\n",
    " .coalesce(1)\n",
    " .write.mode(\"overwrite\").option(\"header\",\"true\")\n",
    " .csv((CSV_OUT_DIR / \"alerts\").as_posix()))\n",
    "\n",
    "print(\"[DER-ALERTS] CSV ready:\", (CSV_OUT_DIR / \"alerts\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "1aadf949-36f1-421c-8fbe-8dba00534644",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DER-LSTM] wrote: 1999\n",
      "[DER-LSTM] CSV ready: C:/engine_module_pipeline/infer_stage/csv/lstm_windows\n"
     ]
    }
   ],
   "source": [
    "# DER-LSTM — reconstruct sliding windows (size=10) from inference_results and export CSV\n",
    "from pyspark.sql import Window, functions as F, types as T\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "WINDOW = 10\n",
    "\n",
    "df_ir = (spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA))\n",
    "         .select(\"source_id\",\"timestamp\",\"row_hash\",\"date\",\"recon_error_lstm\",\"inference_run_id\")\n",
    "         .withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "         .withColumn(\"source_id\", F.col(\"source_id\").cast(\"string\"))\n",
    "        )\n",
    "\n",
    "w = Window.partitionBy(\"source_id\").orderBy(F.col(\"timestamp\").cast(\"long\")).rowsBetween(-(WINDOW-1), 0)\n",
    "\n",
    "# Collect last N row_hashes and first/last timestamps in the frame\n",
    "lstm_df = (\n",
    "    df_ir\n",
    "    .withColumn(\"row_hashes\", F.collect_list(F.col(\"row_hash\").cast(\"string\")).over(w))\n",
    "    .withColumn(\"window_start_ts\", F.element_at(F.sort_array(F.collect_list(F.col(\"timestamp\")).over(w)), 1))\n",
    "    .withColumn(\"window_end_ts\",   F.col(\"timestamp\"))\n",
    "    .filter(F.size(\"row_hashes\") >= 2)  # need at least 2 points to be meaningful\n",
    "    .withColumn(\"lstm_window_id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"reconstruction_error\", F.col(\"recon_error_lstm\").cast(\"double\"))\n",
    "    .withColumn(\"per_step_errors\", F.array().cast(T.ArrayType(T.DoubleType())))\n",
    "    .withColumn(\"model_version\", F.lit(\"posthoc\"))\n",
    "    .withColumn(\"date\", F.col(\"date\").cast(\"string\"))\n",
    "    .select(\"lstm_window_id\",\"window_start_ts\",\"window_end_ts\",\"row_hashes\",\n",
    "            \"reconstruction_error\",\"per_step_errors\",\"model_version\",\"inference_run_id\",\"date\")\n",
    ")\n",
    "\n",
    "# append\n",
    "lstm_df.write.format(\"delta\").mode(\"append\").save(str(LSTM_WIN_DELTA))\n",
    "print(\"[DER-LSTM] wrote:\", lstm_df.count())\n",
    "\n",
    "# CSV (flatten arrays)\n",
    "lstm_csv = (lstm_df\n",
    "    .withColumn(\"row_hashes_json\", F.to_json(\"row_hashes\"))\n",
    "    .withColumn(\"per_step_errors_json\", F.to_json(\"per_step_errors\"))\n",
    "    .drop(\"row_hashes\",\"per_step_errors\")\n",
    ")\n",
    "(lstm_csv.coalesce(1)\n",
    "    .write.mode(\"overwrite\").option(\"header\",\"true\")\n",
    "    .csv((CSV_OUT_DIR / \"lstm_windows\").as_posix()))\n",
    "print(\"[DER-LSTM] CSV ready:\", (CSV_OUT_DIR / \"lstm_windows\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "f92679ba-d6b9-4424-a05b-b68d86690918",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DER-VEH] upserted.\n",
      "[DER-VEH] CSV ready: C:/engine_module_pipeline/infer_stage/csv/vehicle_health\n"
     ]
    }
   ],
   "source": [
    "# DER-VEH — derive vehicle health daily aggregates and export CSV\n",
    "from pyspark.sql import functions as F\n",
    "from delta.tables import DeltaTable\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_ir = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA)).select(\n",
    "    F.col(\"source_id\").cast(\"string\").alias(\"vehicle_id\"),\n",
    "    F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "    \"row_hash\",\"anomaly_severity\",\"composite_score\",\"inference_ts\",\"model_versions\"\n",
    ")\n",
    "\n",
    "veh_df = (\n",
    "    df_ir.groupBy(\"vehicle_id\",\"date\")\n",
    "         .agg(\n",
    "             F.count(\"row_hash\").alias(\"rows_count\"),\n",
    "             F.sum(F.when(F.col(\"anomaly_severity\")>=2, 1).otherwise(0)).cast(\"long\").alias(\"anomaly_count\"),\n",
    "             F.expr(\"percentile_approx(composite_score, 0.5)\").alias(\"median_composite_score\"),\n",
    "             F.expr(\"percentile_approx(composite_score, 0.95)\").alias(\"p95_composite_score\"),\n",
    "             F.max(\"inference_ts\").alias(\"last_inference_ts\"),\n",
    "             F.first(\"model_versions\", ignorenulls=True).alias(\"model_versions\")\n",
    "         )\n",
    "         .withColumn(\"anomaly_rate\",\n",
    "             F.when(F.col(\"rows_count\")>0, F.col(\"anomaly_count\")/F.col(\"rows_count\")).otherwise(F.lit(0.0))\n",
    "         )\n",
    "         .withColumn(\"health_score\",\n",
    "             F.expr(\"greatest(0.0, least(100.0, (1 - (0.6*median_composite_score + 0.3*anomaly_rate + 0.1*(1-1.0)))*100))\")\n",
    "         )\n",
    "         .withColumn(\"days_since_last_alert\", F.lit(None).cast(\"int\"))\n",
    "         .withColumn(\"top_failure_modes\", F.array().cast(\"array<string>\"))\n",
    "         .withColumn(\"trend_flag\", F.lit(\"steady\"))\n",
    "         .withColumn(\"estimated_rul\", F.lit(None).cast(\"double\"))\n",
    ")\n",
    "\n",
    "# MERGE on (vehicle_id, date) with recency\n",
    "tgt_h = DeltaTable.forPath(spark, str(VEH_HEALTH_DELTA))\n",
    "(tgt_h.alias(\"t\")\n",
    "     .merge(veh_df.alias(\"s\"),\n",
    "            \"t.vehicle_id = s.vehicle_id AND t.date = s.date\")\n",
    "     .whenMatchedUpdate(\n",
    "         condition=\"s.last_inference_ts > t.last_inference_ts\",\n",
    "         set={c: f\"s.`{c}`\" for c in veh_df.columns}\n",
    "     )\n",
    "     .whenNotMatchedInsert(values={c: f\"s.`{c}`\" for c in veh_df.columns})\n",
    "     .execute())\n",
    "\n",
    "print(\"[DER-VEH] upserted.\")\n",
    "\n",
    "# CSV (flatten maps/arrays)\n",
    "veh_csv = (veh_df\n",
    "    .withColumn(\"model_versions_json\", F.to_json(\"model_versions\"))\n",
    "    .withColumn(\"top_failure_modes_json\", F.to_json(\"top_failure_modes\"))\n",
    "    .drop(\"model_versions\",\"top_failure_modes\")\n",
    ")\n",
    "(veh_csv.coalesce(1)\n",
    "        .write.mode(\"overwrite\").option(\"header\",\"true\")\n",
    "        .csv((CSV_OUT_DIR / \"vehicle_health\").as_posix()))\n",
    "print(\"[DER-VEH] CSV ready:\", (CSV_OUT_DIR / \"vehicle_health\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "8ff140e6-861e-4a67-882e-4cd43898ebf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'pyspark.sql.functions' has no attribute 'map'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[87]\u001b[39m\u001b[32m, line 22\u001b[39m\n\u001b[32m     12\u001b[39m \u001b[38;5;66;03m# pick the latest row per run_id\u001b[39;00m\n\u001b[32m     13\u001b[39m w = Window.partitionBy(\u001b[33m\"\u001b[39m\u001b[33minference_run_id\u001b[39m\u001b[33m\"\u001b[39m).orderBy(F.col(\u001b[33m\"\u001b[39m\u001b[33minference_ts\u001b[39m\u001b[33m\"\u001b[39m).desc())\n\u001b[32m     14\u001b[39m meta_df = (\n\u001b[32m     15\u001b[39m     df_ir\n\u001b[32m     16\u001b[39m     .withColumn(\u001b[33m\"\u001b[39m\u001b[33mrn\u001b[39m\u001b[33m\"\u001b[39m, F.row_number().over(w))\n\u001b[32m     17\u001b[39m     .filter(F.col(\u001b[33m\"\u001b[39m\u001b[33mrn\u001b[39m\u001b[33m\"\u001b[39m)==\u001b[32m1\u001b[39m)\n\u001b[32m     18\u001b[39m     .select(\n\u001b[32m     19\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33minference_run_id\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m     20\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33minference_ts\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mtimestamp\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     21\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mmodel_versions\u001b[39m\u001b[33m\"\u001b[39m,\n\u001b[32m---> \u001b[39m\u001b[32m22\u001b[39m         \u001b[43mF\u001b[49m\u001b[43m.\u001b[49m\u001b[43mmap\u001b[49m().alias(\u001b[33m\"\u001b[39m\u001b[33mparams\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     23\u001b[39m         F.map().alias(\u001b[33m\"\u001b[39m\u001b[33mbaseline_stats\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     24\u001b[39m         F.lit(\u001b[38;5;28;01mNone\u001b[39;00m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mnotes\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     25\u001b[39m         F.lit(\u001b[38;5;28;01mNone\u001b[39;00m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33msource_commit\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     26\u001b[39m         F.col(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m).cast(\u001b[33m\"\u001b[39m\u001b[33mstring\u001b[39m\u001b[33m\"\u001b[39m).alias(\u001b[33m\"\u001b[39m\u001b[33mdate\u001b[39m\u001b[33m\"\u001b[39m),\n\u001b[32m     27\u001b[39m     )\n\u001b[32m     28\u001b[39m )\n\u001b[32m     30\u001b[39m meta_df.write.format(\u001b[33m\"\u001b[39m\u001b[33mdelta\u001b[39m\u001b[33m\"\u001b[39m).mode(\u001b[33m\"\u001b[39m\u001b[33mappend\u001b[39m\u001b[33m\"\u001b[39m).save(\u001b[38;5;28mstr\u001b[39m(MODEL_META_DELTA))\n\u001b[32m     31\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33m[DER-META] wrote:\u001b[39m\u001b[33m\"\u001b[39m, meta_df.count())\n",
      "\u001b[31mAttributeError\u001b[39m: module 'pyspark.sql.functions' has no attribute 'map'"
     ]
    }
   ],
   "source": [
    "# DER-META — derive one metadata row per inference_run_id and export CSV\n",
    "from pyspark.sql import functions as F\n",
    "from pathlib import Path\n",
    "\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "df_ir = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA)).select(\n",
    "    \"inference_run_id\",\"inference_ts\",\"model_versions\",\"date\"\n",
    ").where(F.col(\"inference_run_id\").isNotNull())\n",
    "\n",
    "# pick the latest row per run_id\n",
    "w = Window.partitionBy(\"inference_run_id\").orderBy(F.col(\"inference_ts\").desc())\n",
    "meta_df = (\n",
    "    df_ir\n",
    "    .withColumn(\"rn\", F.row_number().over(w))\n",
    "    .filter(F.col(\"rn\")==1)\n",
    "    .select(\n",
    "        \"inference_run_id\",\n",
    "        F.col(\"inference_ts\").alias(\"timestamp\"),\n",
    "        \"model_versions\",\n",
    "        F.map().alias(\"params\"),\n",
    "        F.map().alias(\"baseline_stats\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"notes\"),\n",
    "        F.lit(None).cast(\"string\").alias(\"source_commit\"),\n",
    "        F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "meta_df.write.format(\"delta\").mode(\"append\").save(str(MODEL_META_DELTA))\n",
    "print(\"[DER-META] wrote:\", meta_df.count())\n",
    "\n",
    "# CSV (flatten maps)\n",
    "meta_csv = meta_df.withColumn(\"model_versions_json\", F.to_json(\"model_versions\")) \\\n",
    "                  .withColumn(\"params_json\", F.to_json(\"params\")) \\\n",
    "                  .withColumn(\"baseline_stats_json\", F.to_json(\"baseline_stats\")) \\\n",
    "                  .drop(\"model_versions\",\"params\",\"baseline_stats\")\n",
    "(meta_csv.coalesce(1)\n",
    "        .write.mode(\"overwrite\").option(\"header\",\"true\")\n",
    "        .csv((CSV_OUT_DIR / \"model_metadata\").as_posix()))\n",
    "print(\"[DER-META] CSV ready:\", (CSV_OUT_DIR / \"model_metadata\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f526aae6-14a0-416f-8e83-e46fadc09503",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "612e28f1-c351-4b84-a238-a0828cb5bbeb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "id": "59a0f0f7-7af1-4193-97a0-61eb4796f5d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[DER-LSTM-FULL] LSTM windows appended. rows=1991\n",
      "[DER-LSTM-FULL] CSV ready: C:/engine_module_pipeline/infer_stage/csv/lstm_windows_full\n"
     ]
    }
   ],
   "source": [
    "# DER-LSTM-FULL — recompute complete LSTM windows with per-step errors and export CSV\n",
    "import hashlib, uuid, numpy as np, pandas as pd\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F, types as T\n",
    "from pyspark.sql import Row\n",
    "\n",
    "from pathlib import Path\n",
    "\n",
    "# Set this at the top of the notebook (or before the DER-LSTM-FULL cell)\n",
    "LSTM_WIN_DELTA = Path(r\"C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_lstm_windows\")\n",
    "\n",
    "\n",
    "# --------------------\n",
    "# Config\n",
    "# --------------------\n",
    "WINDOW = 10  # fixed window length for LSTM windows\n",
    "CSV_OUT_DIR = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\")\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --------------------\n",
    "# Preconditions\n",
    "# --------------------\n",
    "assert 'INFER_RESULTS_DELTA' in globals(), \"Missing INFER_RESULTS_DELTA\"\n",
    "assert 'LSTM_WIN_DELTA' in globals(), \"Missing LSTM_WIN_DELTA\"\n",
    "assert 'features' in globals() and isinstance(features, list) and len(features) == 25, \"Features (25) not loaded\"\n",
    "# Scaler is needed only for model path\n",
    "try:\n",
    "    _scaler = load_scaler()\n",
    "except Exception:\n",
    "    _scaler = None\n",
    "_lstm_entry = load_lstm()  # may be None (fallback path uses recon_error_lstm)\n",
    "\n",
    "# --------------------\n",
    "# Read inference_results minimal columns\n",
    "# --------------------\n",
    "cols_needed = [\"source_id\",\"timestamp\",\"row_hash\",\"date\",\"recon_error_lstm\",\"inference_run_id\"] + features\n",
    "ir_spark = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA)).select(*[c for c in cols_needed if c in spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA)).columns])\n",
    "\n",
    "# guard empties\n",
    "if ir_spark.rdd.isEmpty():\n",
    "    print(\"[DER-LSTM-FULL] inference_results is empty; nothing to do.\")\n",
    "else:\n",
    "    # maintain ordering per source_id\n",
    "    ir_pdf = (ir_spark\n",
    "              .withColumn(\"timestamp\", F.col(\"timestamp\").cast(\"timestamp\"))\n",
    "              .orderBy(F.col(\"source_id\").asc_nulls_last(), F.col(\"timestamp\").asc_nulls_last())\n",
    "              .toPandas())\n",
    "\n",
    "    # quick clean\n",
    "    ir_pdf = ir_pdf.dropna(subset=[\"timestamp\", \"row_hash\"])\n",
    "    if ir_pdf.empty:\n",
    "        print(\"[DER-LSTM-FULL] no valid rows after cleaning; nothing to write.\")\n",
    "    else:\n",
    "        # --------------------\n",
    "        # Helper: stable window id from row_hashes\n",
    "        # --------------------\n",
    "        def stable_id(row_hashes):\n",
    "            h = hashlib.sha1(\"|\".join(map(str, row_hashes)).encode(\"utf-8\")).hexdigest()\n",
    "            # uuid5-like string for readability\n",
    "            return str(uuid.UUID(h[:32]))\n",
    "\n",
    "        # --------------------\n",
    "        # Helper: model-based per-step error computation\n",
    "        # For each step t in the window, score prefix [0..t] and compute MSE on last step\n",
    "        # --------------------\n",
    "        def per_step_errors_model(Xw, lstm):\n",
    "            # Xw: np.array shape [W, F] (already scaled)\n",
    "            errs = []\n",
    "            with torch.no_grad():\n",
    "                for t in range(WINDOW):\n",
    "                    # Need at least 2 steps to compute a meaningful seq error\n",
    "                    if t < 1:\n",
    "                        errs.append(0.0)\n",
    "                        continue\n",
    "                    seq = Xw[:t+1]  # shape [t+1, F]\n",
    "                    wt = torch.from_numpy(seq.astype(\"float32\")).unsqueeze(0)  # [1, T, F]\n",
    "                    yhat = lstm(wt)  # [1,1,F] (per your loader)\n",
    "                    rec = yhat.squeeze(0).squeeze(0).cpu().numpy()\n",
    "                    errs.append(float(((seq[-1] - rec)**2).mean()))\n",
    "            return errs\n",
    "\n",
    "        # --------------------\n",
    "        # Helper: fallback per-step from existing recon_error_lstm (no model)\n",
    "        # --------------------\n",
    "        def per_step_errors_fallback(lstm_errors_slice):\n",
    "            # Use the known last-step recon errors for each row in the window.\n",
    "            arr = [float(x) if pd.notna(x) else 0.0 for x in lstm_errors_slice]\n",
    "            # if all zeros, keep zeros; last element becomes reconstruction_error\n",
    "            return arr\n",
    "\n",
    "        # --------------------\n",
    "        # Process per source_id in pandas\n",
    "        # --------------------\n",
    "        out_rows = []\n",
    "        grouped = ir_pdf.groupby(ir_pdf[\"source_id\"].astype(str), dropna=False)\n",
    "\n",
    "        # Prepare model/scaler if present\n",
    "        lstm_model = None\n",
    "        model_version = \"fallback\"\n",
    "        if _lstm_entry is not None and _scaler is not None:\n",
    "            lstm_mode, lstm_model = _lstm_entry\n",
    "            model_version = \"ts\" if lstm_mode == \"ts\" else \"sd\"\n",
    "\n",
    "        for sid, gdf in grouped:\n",
    "            gdf = gdf.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "            # build full windows only (exact WINDOW size)\n",
    "            if len(gdf) < WINDOW:\n",
    "                continue\n",
    "\n",
    "            # feature matrix (unscaled values -> scaled if we have a model)\n",
    "            feat_mat = gdf[features].astype(float).values\n",
    "            if lstm_model is not None and _scaler is not None:\n",
    "                Xs = _scaler.transform(np.nan_to_num(feat_mat, copy=False))\n",
    "            else:\n",
    "                Xs = np.nan_to_num(feat_mat, copy=False)  # not used for model path anyway\n",
    "\n",
    "            for end in range(WINDOW-1, len(gdf)):\n",
    "                start = end - (WINDOW-1)\n",
    "                win_slice = gdf.iloc[start:end+1]\n",
    "\n",
    "                row_hashes = win_slice[\"row_hash\"].astype(str).tolist()\n",
    "                win_id = stable_id(row_hashes)\n",
    "\n",
    "                # choose inference_run_id from the last row (most recent)\n",
    "                run_id = str(win_slice[\"inference_run_id\"].iloc[-1]) if \"inference_run_id\" in win_slice.columns else None\n",
    "\n",
    "                # per-step errors\n",
    "                if lstm_model is not None and _scaler is not None:\n",
    "                    Xw = Xs[start:end+1]  # [WINDOW, F]\n",
    "                    per_step = per_step_errors_model(Xw, lstm_model)\n",
    "                else:\n",
    "                    # fallback from recon_error_lstm column\n",
    "                    per_step = per_step_errors_fallback(win_slice[\"recon_error_lstm\"].tolist())\n",
    "\n",
    "                reconstruction_error = float(per_step[-1]) if len(per_step) > 0 else None\n",
    "\n",
    "                out_rows.append({\n",
    "                    \"lstm_window_id\": win_id,\n",
    "                    \"window_start_ts\": pd.to_datetime(win_slice[\"timestamp\"].iloc[0], utc=True).tz_convert(None),\n",
    "                    \"window_end_ts\":   pd.to_datetime(win_slice[\"timestamp\"].iloc[-1], utc=True).tz_convert(None),\n",
    "                    \"row_hashes\": row_hashes,\n",
    "                    \"reconstruction_error\": reconstruction_error,\n",
    "                    \"per_step_errors\": per_step,\n",
    "                    \"model_version\": model_version,\n",
    "                    \"inference_run_id\": run_id,\n",
    "                    \"date\": str(pd.to_datetime(win_slice[\"timestamp\"].iloc[-1]).date())\n",
    "                })\n",
    "\n",
    "        # --------------------\n",
    "        # Write to Delta (append; stable ids prevent dupes across re-runs)\n",
    "        # --------------------\n",
    "        if not out_rows:\n",
    "            print(\"[DER-LSTM-FULL] No windows created (not enough rows per vehicle).\")\n",
    "        else:\n",
    "            out_pdf = pd.DataFrame(out_rows)\n",
    "            # Create Spark DF and cast exactly to LSTM_WIN_SCHEMA\n",
    "            tmp = spark.createDataFrame(out_pdf)\n",
    "            lstm_df = tmp.select(\n",
    "                F.col(\"lstm_window_id\").cast(\"string\").alias(\"lstm_window_id\"),\n",
    "                F.col(\"window_start_ts\").cast(\"timestamp\").alias(\"window_start_ts\"),\n",
    "                F.col(\"window_end_ts\").cast(\"timestamp\").alias(\"window_end_ts\"),\n",
    "                F.col(\"row_hashes\").cast(T.ArrayType(T.StringType())).alias(\"row_hashes\"),\n",
    "                F.col(\"reconstruction_error\").cast(\"double\").alias(\"reconstruction_error\"),\n",
    "                F.col(\"per_step_errors\").cast(T.ArrayType(T.DoubleType())).alias(\"per_step_errors\"),\n",
    "                F.col(\"model_version\").cast(\"string\").alias(\"model_version\"),\n",
    "                F.col(\"inference_run_id\").cast(\"string\").alias(\"inference_run_id\"),\n",
    "                F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "            )\n",
    "\n",
    "            lstm_df.write.format(\"delta\").mode(\"append\").save(str(LSTM_WIN_DELTA))\n",
    "            wrote = lstm_df.count()\n",
    "            print(f\"[DER-LSTM-FULL] LSTM windows appended. rows={wrote}\")\n",
    "\n",
    "            # --------------------\n",
    "            # CSV export (arrays as JSON strings)\n",
    "            # --------------------\n",
    "            lstm_csv = (lstm_df\n",
    "                .withColumn(\"row_hashes_json\", F.to_json(\"row_hashes\"))\n",
    "                .withColumn(\"per_step_errors_json\", F.to_json(\"per_step_errors\"))\n",
    "                .drop(\"row_hashes\",\"per_step_errors\")\n",
    "            )\n",
    "            (lstm_csv.coalesce(1)\n",
    "                     .write.mode(\"overwrite\").option(\"header\",\"true\")\n",
    "                     .csv((CSV_OUT_DIR / \"lstm_windows_full\").as_posix()))\n",
    "            print(\"[DER-LSTM-FULL] CSV ready:\", (CSV_OUT_DIR / \"lstm_windows_full\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "id": "98bb4f4d-6c77-4a78-b11c-1dfcf039a84b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== First 2 rows from Delta table ===\n",
      "+------------------------------------+-----------------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------+----------+\n",
      "|lstm_window_id                      |window_start_ts        |window_end_ts          |row_hashes                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                                          |reconstruction_error|per_step_errors                                                                                                                                                                        |model_version|inference_run_id                    |date      |\n",
      "+------------------------------------+-----------------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------+----------+\n",
      "|71ba5445-28e3-3fc4-e6a1-ff8084eeced7|2025-09-16 18:00:16.104|2025-09-17 07:32:14.729|[c675b5e333b4c59ec6725ea5784494433ce38dd31a9f16d461ea26c16861e942, 829af404b0b6f10db35662b7c4aac077b65d01a95ed34201cc6767c9d4055bc2, 3cbdd7896a5be4604e0d04c9929f3a97c031c90619076981eaa9ad7846266835, 03c213560ad008c9cbc4bf89fa1e7a822c6ec419b088c9073568bbf0feabc0fd, b4f1cb0808c6dee6eb7bc90a7fed34124e51834dcb0f36b0355df65a556b8944, a3d520572cc276b296583867f2fbb12a2e47cf3fbd827df65367b98ca8211c75, 4ce941d287b11ad2df491bc3f730ede5090f17c6b66be118b9c0d164a347cbd1, c204141dc9b7339e1644f876128950811ab13f123d34b7d3b3d42a926e9f35be, 73962075afc311c0a6c157803812028841fb15e6d0d8589ea84845df607cdd19, c61ca661bc509a0d0ac73b3b73138f29e124d7ea4b4872c9110c2fec18eab5ff]|0.6847336242628971  |[0.0, 145.85532850115163, 16.43804848776724, 1.9363221180818142, 2.9509129227297266, 0.650611649094606, 0.44621327910885805, 0.725545246027952, 0.7927731843734501, 0.6847336242628971]|ts           |run-c9fd450635cc41388b962a452a3aea73|2025-09-17|\n",
      "|17a9cf75-bb24-895e-c635-a205d81f9e01|2025-09-16 18:00:16.104|2025-09-17 07:32:14.729|[829af404b0b6f10db35662b7c4aac077b65d01a95ed34201cc6767c9d4055bc2, 3cbdd7896a5be4604e0d04c9929f3a97c031c90619076981eaa9ad7846266835, 03c213560ad008c9cbc4bf89fa1e7a822c6ec419b088c9073568bbf0feabc0fd, b4f1cb0808c6dee6eb7bc90a7fed34124e51834dcb0f36b0355df65a556b8944, a3d520572cc276b296583867f2fbb12a2e47cf3fbd827df65367b98ca8211c75, 4ce941d287b11ad2df491bc3f730ede5090f17c6b66be118b9c0d164a347cbd1, c204141dc9b7339e1644f876128950811ab13f123d34b7d3b3d42a926e9f35be, 73962075afc311c0a6c157803812028841fb15e6d0d8589ea84845df607cdd19, c61ca661bc509a0d0ac73b3b73138f29e124d7ea4b4872c9110c2fec18eab5ff, 7c938cc88250331332c43ad1336a721bbfb2c94380fc41cdea72ee9fd7743576]|2.683521272584847   |[0.0, 132.31849598549448, 7.779808676466141, 2.252693269586332, 0.5414990201091208, 0.4299462318702467, 0.7382644340583178, 0.8092713825303879, 0.6946860605873111, 2.683521272584847] |ts           |run-c9fd450635cc41388b962a452a3aea73|2025-09-17|\n",
      "+------------------------------------+-----------------------+-----------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+--------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------+------------------------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "Pandas preview of first two rows:\n",
      "                          lstm_window_id         window_start_ts  \\\n",
      "0  71ba5445-28e3-3fc4-e6a1-ff8084eeced7 2025-09-16 18:00:16.104   \n",
      "1  17a9cf75-bb24-895e-c635-a205d81f9e01 2025-09-16 18:00:16.104   \n",
      "\n",
      "            window_end_ts                                         row_hashes  \\\n",
      "0 2025-09-17 07:32:14.729  [c675b5e333b4c59ec6725ea5784494433ce38dd31a9f1...   \n",
      "1 2025-09-17 07:32:14.729  [829af404b0b6f10db35662b7c4aac077b65d01a95ed34...   \n",
      "\n",
      "   reconstruction_error                                    per_step_errors  \\\n",
      "0              0.684734  [0.0, 145.85532850115163, 16.43804848776724, 1...   \n",
      "1              2.683521  [0.0, 132.31849598549448, 7.779808676466141, 2...   \n",
      "\n",
      "  model_version                      inference_run_id        date  \n",
      "0            ts  run-c9fd450635cc41388b962a452a3aea73  2025-09-17  \n",
      "1            ts  run-c9fd450635cc41388b962a452a3aea73  2025-09-17  \n",
      "\n",
      "✅ LSTM windows table re-exported to:\n",
      "  C:\\engine_module_pipeline\\infer_stage\\csv\\lstm_windows_full\\lstm_windows_export.csv\n",
      "Columns saved: ['lstm_window_id', 'window_start_ts', 'window_end_ts', 'row_hashes', 'reconstruction_error', 'per_step_errors', 'model_version', 'inference_run_id', 'date']\n",
      "Row count    : 1991\n"
     ]
    }
   ],
   "source": [
    "# CELL — Inspect & Re-export LSTM Windows Table\n",
    "from pyspark.sql import functions as F\n",
    "import pandas as pd\n",
    "import json\n",
    "from pathlib import Path\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 1) Read Delta table\n",
    "lstm_delta_path = r\"C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_lstm_windows\"\n",
    "df_lstm = spark.read.format(\"delta\").load(lstm_delta_path)\n",
    "\n",
    "print(\"=== First 2 rows from Delta table ===\")\n",
    "df_lstm.show(2, truncate=False)     # prints to console\n",
    "# Optionally collect a pandas slice for debugging\n",
    "pdf_preview = df_lstm.limit(2).toPandas()\n",
    "print(\"\\nPandas preview of first two rows:\\n\", pdf_preview)\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# 2) Re-export to CSV\n",
    "#    Serialize array/map columns as JSON strings for clean CSV output\n",
    "export_path = Path(r\"C:\\engine_module_pipeline\\infer_stage\\csv\\lstm_windows_full\")\n",
    "export_path.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Convert to pandas to handle JSON serialization\n",
    "pdf = df_lstm.toPandas()\n",
    "\n",
    "# Columns that are not plain scalars\n",
    "array_like_cols = [\"row_hashes\", \"per_step_errors\"]\n",
    "\n",
    "def to_json_if_needed(x):\n",
    "    if isinstance(x, (list, dict)):\n",
    "        return json.dumps(x)\n",
    "    return x\n",
    "\n",
    "for c in array_like_cols:\n",
    "    if c in pdf.columns:\n",
    "        pdf[c] = pdf[c].apply(to_json_if_needed)\n",
    "\n",
    "out_csv = export_path / \"lstm_windows_export.csv\"\n",
    "pdf.to_csv(out_csv, index=False, quoting=1)  # quoting=1 → quote all fields with commas/brackets\n",
    "\n",
    "print(f\"\\n✅ LSTM windows table re-exported to:\\n  {out_csv}\")\n",
    "print(\"Columns saved:\", pdf.columns.tolist())\n",
    "print(\"Row count    :\", len(pdf))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "id": "7aa73306-d953-4a83-b28b-b1de385f9202",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_alerts\n",
      "Initialized Delta table: C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_alerts_refined\n",
      "[ALERTS] FULL alerts written → C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_alerts\n",
      "[ALERTS] REFINED alerts written → C:\\engine_module_pipeline\\infer_stage\\delta\\engine_module_alerts_refined\n",
      "CSV full dir    : C:\\engine_module_pipeline\\infer_stage\\csv\\alerts_full\n",
      "CSV refined dir : C:\\engine_module_pipeline\\infer_stage\\csv\\alerts_refined\n"
     ]
    }
   ],
   "source": [
    "# === ALERTS — FULL & REFINED (trip-aware), fixed `extra` handling ===\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import (\n",
    "    StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType,\n",
    "    ArrayType, MapType, StructType, StructField\n",
    ")\n",
    "from delta.tables import DeltaTable\n",
    "from pathlib import Path\n",
    "\n",
    "# ---------- Paths ----------\n",
    "REPO_ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "DELTA_DIR = REPO_ROOT / r\"infer_stage\\delta\"\n",
    "CSV_DIR   = REPO_ROOT / r\"infer_stage\\csv\"\n",
    "CSV_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "INFER_RESULTS_DELTA  = DELTA_DIR / \"engine_module_inference_results\"\n",
    "ALERTS_DELTA         = DELTA_DIR / \"engine_module_alerts\"\n",
    "ALERTS_REFINED_DELTA = DELTA_DIR / \"engine_module_alerts_refined\"\n",
    "\n",
    "# ---------- Schemas ----------\n",
    "ALERTS_SCHEMA = StructType([\n",
    "    StructField(\"alert_id\", StringType(), False),\n",
    "    StructField(\"alert_ts\", TimestampType(), False),\n",
    "    StructField(\"row_hash\", StringType(), True),\n",
    "    StructField(\"vehicle_id\", StringType(), True),\n",
    "    StructField(\"alert_type\", StringType(), True),\n",
    "    StructField(\"severity\", IntegerType(), True),\n",
    "    StructField(\"composite_score\", DoubleType(), True),\n",
    "    StructField(\"triggering_models\", ArrayType(StringType()), True),\n",
    "    StructField(\"reason\", StringType(), True),\n",
    "    StructField(\"top_features\", ArrayType(StringType()), True),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"inference_run_id\", StringType(), True),\n",
    "    StructField(\"acked\", BooleanType(), True),\n",
    "    StructField(\"acked_by\", StringType(), True),\n",
    "    StructField(\"acked_ts\", TimestampType(), True),\n",
    "    StructField(\"notified_channels\", ArrayType(StringType()), True),\n",
    "    StructField(\"linked_rows\", ArrayType(StringType()), True),\n",
    "    StructField(\"extra\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"date\", StringType(), True),\n",
    "])\n",
    "\n",
    "ALERTS_REFINED_SCHEMA = StructType([\n",
    "    StructField(\"alert_group_id\", StringType(), False),\n",
    "    StructField(\"vehicle_id\", StringType(), False),\n",
    "    StructField(\"trip_id\", StringType(), False),\n",
    "    StructField(\"alert_type\", StringType(), False),\n",
    "    StructField(\"start_ts\", TimestampType(), False),\n",
    "    StructField(\"end_ts\", TimestampType(), False),\n",
    "    StructField(\"alerts_count\", IntegerType(), False),\n",
    "    StructField(\"max_severity\", IntegerType(), False),\n",
    "    StructField(\"max_composite_score\", DoubleType(), False),\n",
    "    StructField(\"representative_alert_id\", StringType(), True),\n",
    "    StructField(\"merged_alert_ids\", ArrayType(StringType()), True),\n",
    "    StructField(\"top_features_union\", ArrayType(StringType()), True),\n",
    "    StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "    StructField(\"date\", StringType(), False),\n",
    "])\n",
    "\n",
    "def ensure_delta_table(path: Path, schema: StructType, partition_cols=None):\n",
    "    partition_cols = partition_cols or []\n",
    "    if (path / \"_delta_log\").exists():\n",
    "        return\n",
    "    empty = spark.createDataFrame([], schema)\n",
    "    w = empty.write.format(\"delta\").mode(\"overwrite\")\n",
    "    if partition_cols:\n",
    "        w = w.partitionBy(*partition_cols)\n",
    "    w.save(str(path))\n",
    "    print(\"Initialized Delta table:\", path)\n",
    "\n",
    "ensure_delta_table(ALERTS_DELTA, ALERTS_SCHEMA, partition_cols=[\"date\"])\n",
    "ensure_delta_table(ALERTS_REFINED_DELTA, ALERTS_REFINED_SCHEMA, partition_cols=[\"date\"])\n",
    "\n",
    "# ---------- Typed empty literals for arrays / maps ----------\n",
    "EMPTY_STR_ARRAY = F.from_json(F.lit(\"[]\"), ArrayType(StringType()))\n",
    "EMPTY_STR_MAP   = F.from_json(F.lit(\"{}\"), MapType(StringType(), StringType()))\n",
    "\n",
    "# ---------- Parameters ----------\n",
    "ALERT_THRESHOLD        = 0.75\n",
    "TRIP_GAP_MINUTES       = 20   # gap > 20m => new trip\n",
    "MERGE_SAME_TYPE_GAP_MIN= 5    # within-trip same-type alerts within 5m collapse\n",
    "\n",
    "# ---------- 1) Load inference results above threshold ----------\n",
    "res_df = (\n",
    "    spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA))\n",
    "         .select(\n",
    "             \"row_hash\",\"timestamp\",\"date\",\"source_id\",\n",
    "             \"composite_score\",\"anomaly_severity\",\"explain_top_k\",\n",
    "             \"model_versions\",\"inference_run_id\"\n",
    "         )\n",
    ")\n",
    "\n",
    "cand = res_df.where(F.col(\"composite_score\") >= F.lit(ALERT_THRESHOLD))\n",
    "cand = cand.withColumn(\"alert_type\", F.lit(\"composite_high\"))\n",
    "\n",
    "# extract top_features = array<string>\n",
    "def top3_features(col):\n",
    "    return F.transform(F.slice(col, 1, 3), lambda s: F.coalesce(s[\"feature\"].cast(\"string\"), F.lit(\"\")))\n",
    "\n",
    "cand = cand.withColumn(\n",
    "    \"top_features\",\n",
    "    F.when(F.col(\"explain_top_k\").isNotNull(), top3_features(F.col(\"explain_top_k\")))\n",
    "     .otherwise(EMPTY_STR_ARRAY)\n",
    ")\n",
    "\n",
    "# ---------- 2) Build FULL alerts ----------\n",
    "full_alerts = (\n",
    "    cand\n",
    "    .withColumn(\"alert_id\", F.expr(\"uuid()\"))\n",
    "    .withColumn(\"alert_ts\", F.col(\"timestamp\").cast(TimestampType()))\n",
    "    .withColumn(\"vehicle_id\", F.col(\"source_id\").cast(StringType()))\n",
    "    .withColumn(\"severity\", F.col(\"anomaly_severity\").cast(IntegerType()))\n",
    "    .withColumn(\"triggering_models\", F.array(F.lit(\"dense\"), F.lit(\"lstm\"), F.lit(\"isof\"), F.lit(\"kde\"), F.lit(\"gmm\")))\n",
    "    .withColumn(\"reason\", F.lit(\"Composite score exceeded threshold\"))\n",
    "    .withColumn(\"notified_channels\", EMPTY_STR_ARRAY)\n",
    "    .withColumn(\"linked_rows\", F.array(F.col(\"row_hash\").cast(StringType())))\n",
    "    .withColumn(\"acked\", F.lit(False).cast(BooleanType()))\n",
    "    .withColumn(\"acked_by\", F.lit(None).cast(StringType()))\n",
    "    .withColumn(\"acked_ts\", F.lit(None).cast(TimestampType()))\n",
    "    .withColumn(\"extra\", EMPTY_STR_MAP)  # always a typed map; fixes NullType issues\n",
    "    .select(\n",
    "        \"alert_id\",\"alert_ts\",\"row_hash\",\"vehicle_id\",\"alert_type\",\"severity\",\n",
    "        \"composite_score\",\"triggering_models\",\"reason\",\"top_features\",\n",
    "        \"model_versions\",\"inference_run_id\",\n",
    "        \"acked\",\"acked_by\",\"acked_ts\",\"notified_channels\",\"linked_rows\",\n",
    "        \"extra\",\n",
    "        F.col(\"date\").cast(StringType()).alias(\"date\")\n",
    "    )\n",
    ")\n",
    "\n",
    "# name-cast by schema (no stray literals)\n",
    "full_alerts_s = full_alerts.select(\n",
    "    *[\n",
    "        (F.col(f.name).cast(f.dataType)\n",
    "         if isinstance(f.dataType, (StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType))\n",
    "         else F.col(f.name)\n",
    "        ).alias(f.name)\n",
    "        for f in ALERTS_SCHEMA\n",
    "    ]\n",
    ")\n",
    "\n",
    "full_alerts_s.write.format(\"delta\").mode(\"append\").save(str(ALERTS_DELTA))\n",
    "print(\"[ALERTS] FULL alerts written →\", ALERTS_DELTA)\n",
    "\n",
    "# ---------- 3) Trip detection ----------\n",
    "a = full_alerts_s.select(\"vehicle_id\",\"alert_id\",\"alert_ts\",\"alert_type\",\"severity\",\"composite_score\",\"top_features\",\"model_versions\",\"date\")\n",
    "wVehTs = Window.partitionBy(\"vehicle_id\").orderBy(\"alert_ts\")\n",
    "\n",
    "a_seg = (\n",
    "    a\n",
    "    .withColumn(\"prev_ts\", F.lag(\"alert_ts\").over(wVehTs))\n",
    "    .withColumn(\"gap_min\",\n",
    "        F.when(F.col(\"prev_ts\").isNull(), F.lit(999999.0))\n",
    "         .otherwise((F.col(\"alert_ts\").cast(\"long\") - F.col(\"prev_ts\").cast(\"long\")) / 60.0)\n",
    "    )\n",
    "    .withColumn(\"trip_break\", F.when(F.col(\"gap_min\") > F.lit(TRIP_GAP_MINUTES), F.lit(1)).otherwise(F.lit(0)))\n",
    "    .withColumn(\"trip_seq\", F.sum(\"trip_break\").over(wVehTs))\n",
    "    .withColumn(\"trip_id\", F.concat_ws(\"-\", F.col(\"vehicle_id\"), F.col(\"trip_seq\").cast(\"string\")))\n",
    "    .drop(\"prev_ts\",\"gap_min\",\"trip_break\",\"trip_seq\")\n",
    ")\n",
    "\n",
    "# ---------- 4) Within-trip merge of same-type alerts ----------\n",
    "wTripType = Window.partitionBy(\"vehicle_id\",\"trip_id\",\"alert_type\").orderBy(\"alert_ts\")\n",
    "with_gaps = (\n",
    "    a_seg\n",
    "    .withColumn(\"prev_ts2\", F.lag(\"alert_ts\").over(wTripType))\n",
    "    .withColumn(\"gap_min2\",\n",
    "        F.when(F.col(\"prev_ts2\").isNull(), F.lit(999999.0))\n",
    "         .otherwise((F.col(\"alert_ts\").cast(\"long\") - F.col(\"prev_ts2\").cast(\"long\")) / 60.0)\n",
    "    )\n",
    "    .withColumn(\"grp_bump\", F.when(F.col(\"gap_min2\") > F.lit(MERGE_SAME_TYPE_GAP_MIN), F.lit(1)).otherwise(F.lit(0)))\n",
    "    .withColumn(\"local_group\", F.sum(\"grp_bump\").over(wTripType))\n",
    "    .drop(\"prev_ts2\",\"gap_min2\",\"grp_bump\")\n",
    ")\n",
    "\n",
    "ref_groups = with_gaps.withColumn(\n",
    "    \"alert_group_id\",\n",
    "    F.concat_ws(\":\", F.lit(\"grp\"), F.col(\"vehicle_id\"), F.col(\"trip_id\"), F.col(\"alert_type\"), F.col(\"local_group\").cast(\"string\"))\n",
    ")\n",
    "\n",
    "agg = (\n",
    "    ref_groups\n",
    "    .groupBy(\"alert_group_id\",\"vehicle_id\",\"trip_id\",\"alert_type\")\n",
    "    .agg(\n",
    "        F.min(\"alert_ts\").alias(\"start_ts\"),\n",
    "        F.max(\"alert_ts\").alias(\"end_ts\"),\n",
    "        F.count(F.lit(1)).alias(\"alerts_count\"),\n",
    "        F.max(\"severity\").alias(\"max_severity\"),\n",
    "        F.max(\"composite_score\").alias(\"max_composite_score\"),\n",
    "        F.first(\n",
    "            F.sort_array(\n",
    "                F.transform(F.array(F.struct(\"composite_score\",\"alert_id\")),\n",
    "                            lambda x: x),\n",
    "                asc=False\n",
    "            )[0][\"alert_id\"]\n",
    "        ).alias(\"representative_alert_id\"),\n",
    "        F.collect_list(\"alert_id\").alias(\"merged_alert_ids\"),\n",
    "        F.flatten(F.collect_list(\"top_features\")).alias(\"top_features_union_flat\"),\n",
    "        F.last(\"model_versions\", ignorenulls=True).alias(\"model_versions\"),\n",
    "        F.max(\"date\").alias(\"date\")\n",
    "    )\n",
    "    .withColumn(\"top_features_union\", F.array_distinct(F.col(\"top_features_union_flat\")))\n",
    "    .drop(\"top_features_union_flat\")\n",
    ")\n",
    "\n",
    "refined_s = agg.select(\n",
    "    \"alert_group_id\",\"vehicle_id\",\"trip_id\",\"alert_type\",\n",
    "    \"start_ts\",\"end_ts\",\"alerts_count\",\"max_severity\",\"max_composite_score\",\n",
    "    \"representative_alert_id\",\"merged_alert_ids\",\"top_features_union\",\"model_versions\",\n",
    "    F.col(\"date\").cast(StringType()).alias(\"date\")\n",
    ")\n",
    "\n",
    "# coerce null arrays/maps to typed empties\n",
    "refined_s = (\n",
    "    refined_s\n",
    "    .withColumn(\"merged_alert_ids\",\n",
    "        F.when(F.col(\"merged_alert_ids\").isNull(), EMPTY_STR_ARRAY).otherwise(F.col(\"merged_alert_ids\")))\n",
    "    .withColumn(\"top_features_union\",\n",
    "        F.when(F.col(\"top_features_union\").isNull(), EMPTY_STR_ARRAY).otherwise(F.col(\"top_features_union\")))\n",
    "    .withColumn(\"model_versions\",\n",
    "        F.when(F.col(\"model_versions\").isNull(), EMPTY_STR_MAP).otherwise(F.col(\"model_versions\")))\n",
    ")\n",
    "\n",
    "refined_s = refined_s.select(\n",
    "    *[\n",
    "        (F.col(f.name).cast(f.dataType)\n",
    "         if isinstance(f.dataType, (StringType, IntegerType, DoubleType, BooleanType, TimestampType, DateType))\n",
    "         else F.col(f.name)\n",
    "        ).alias(f.name)\n",
    "        for f in ALERTS_REFINED_SCHEMA\n",
    "    ]\n",
    ")\n",
    "\n",
    "refined_s.write.format(\"delta\").mode(\"append\").save(str(ALERTS_REFINED_DELTA))\n",
    "print(\"[ALERTS] REFINED alerts written →\", ALERTS_REFINED_DELTA)\n",
    "\n",
    "# ---------- 5) CSV exports (complex columns → JSON strings) ----------\n",
    "def export_csv(df, out_dir: Path):\n",
    "    out = df\n",
    "    # Add JSON shadow columns for complex types if present\n",
    "    for c in [\"triggering_models\",\"top_features\",\"linked_rows\",\"notified_channels\",\"extra\",\"model_versions\",\n",
    "              \"merged_alert_ids\",\"top_features_union\"]:\n",
    "        if c in out.columns:\n",
    "            out = out.withColumn(c + \"_json\", F.to_json(F.col(c)))\n",
    "            out = out.drop(c)\n",
    "    (out.coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .csv(str(out_dir)))\n",
    "\n",
    "alerts_full_df    = spark.read.format(\"delta\").load(str(ALERTS_DELTA))\n",
    "alerts_refined_df = spark.read.format(\"delta\").load(str(ALERTS_REFINED_DELTA))\n",
    "\n",
    "export_csv(alerts_full_df,    CSV_DIR / \"alerts_full\")\n",
    "export_csv(alerts_refined_df, CSV_DIR / \"alerts_refined\")\n",
    "\n",
    "print(\"CSV full dir    :\", CSV_DIR / \"alerts_full\")\n",
    "print(\"CSV refined dir :\", CSV_DIR / \"alerts_refined\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "id": "e919d7b6-8bc5-43d2-ad83-dffd36918c20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote CSV:\n",
      "  Full   : C:\\engine_module_pipeline\\infer_stage\\csv\\alerts_full_clean\n",
      "  Refined: C:\\engine_module_pipeline\\infer_stage\\csv\\alerts_refined_clean\n",
      "\n",
      "=== Preview: alerts_full_clean (first 2 rows) ===\n",
      "+------------------------------------+-----------------------+----------------------------------------------------------------+----------+--------------+--------+------------------+----------------------------------+------------------------------------+-----+--------+--------+----------+-----------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+----------------------+--------------------------------------------------------------------+----------+\n",
      "|alert_id                            |alert_ts               |row_hash                                                        |vehicle_id|alert_type    |severity|composite_score   |reason                            |inference_run_id                    |acked|acked_by|acked_ts|date      |triggering_models_json             |top_features_json                                                                                   |model_versions_json                       |notified_channels_json|linked_rows_json                                                    |extra_json|\n",
      "+------------------------------------+-----------------------+----------------------------------------------------------------+----------+--------------+--------+------------------+----------------------------------+------------------------------------+-----+--------+--------+----------+-----------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+----------------------+--------------------------------------------------------------------+----------+\n",
      "|b8200993-19cf-4ff3-b2f1-d473301b927e|2025-09-17 07:32:14.772|8e207d62770c7aaf2bb0fea33eba8e16bab9d606447bc9dc9523fdf58d4c7af7|sim001    |composite_high|3       |0.8094386903476654|Composite score exceeded threshold|run-c9fd450635cc41388b962a452a3aea73|false|NULL    |NULL    |2025-09-17|[\"dense\",\"lstm\",\"isof\",\"kde\",\"gmm\"]|[\"Run_time_since_engine_start_s\",\"Catalyst_Temperature__Bank_1_Sensor_1\",\"Fuel_flow_rate_hour_l_hr\"]|{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|[]                    |[\"8e207d62770c7aaf2bb0fea33eba8e16bab9d606447bc9dc9523fdf58d4c7af7\"]|{}        |\n",
      "|cbbeb7ad-6a31-4bff-81ac-2267a6f6eaa6|2025-09-17 07:32:14.772|2a85307ab9a87c1d0b582ec0ac1bd2ceca5597848f45c83be4189ee9982c2198|sim001    |composite_high|3       |0.8094386903476654|Composite score exceeded threshold|run-c9fd450635cc41388b962a452a3aea73|false|NULL    |NULL    |2025-09-17|[\"dense\",\"lstm\",\"isof\",\"kde\",\"gmm\"]|[\"Run_time_since_engine_start_s\",\"Catalyst_Temperature__Bank_1_Sensor_1\",\"Fuel_flow_rate_hour_l_hr\"]|{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|[]                    |[\"2a85307ab9a87c1d0b582ec0ac1bd2ceca5597848f45c83be4189ee9982c2198\"]|{}        |\n",
      "+------------------------------------+-----------------------+----------------------------------------------------------------+----------+--------------+--------+------------------+----------------------------------+------------------------------------+-----+--------+--------+----------+-----------------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+----------------------+--------------------------------------------------------------------+----------+\n",
      "only showing top 2 rows\n",
      "\n",
      "\n",
      "=== Preview: alerts_refined_clean (first 2 rows) ===\n",
      "+------------------------------------+----------+--------+--------------+-----------------------+-----------------------+------------+------------+-------------------+------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "|alert_group_id                      |vehicle_id|trip_id |alert_type    |start_ts               |end_ts                 |alerts_count|max_severity|max_composite_score|representative_alert_id             |date      |merged_alert_ids_json                                                                                                                                                                                                                                                                                                    |top_features_union_json                                                                                                                                   |model_versions_json                       |\n",
      "+------------------------------------+----------+--------+--------------+-----------------------+-----------------------+------------+------------+-------------------+------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "|grp:sim001:sim001-3:composite_high:1|sim001    |sim001-3|composite_high|2025-09-17 07:32:14.772|2025-09-17 07:32:14.839|8           |3           |0.8094386903476654 |b8200993-19cf-4ff3-b2f1-d473301b927e|2025-09-17|[\"b8200993-19cf-4ff3-b2f1-d473301b927e\",\"cbbeb7ad-6a31-4bff-81ac-2267a6f6eaa6\",\"5e707b70-7227-4adb-93e2-2f9ab20bd594\",\"8329efff-feba-42e8-af8a-59210b6f9594\",\"72c22bdb-675b-471d-8cc7-8f25c7572dc3\",\"b2650b95-2459-41e8-8ad5-20c60b85f12e\",\"9359298f-af25-4ac3-b1ae-617ee5f5c18e\",\"84295168-2d06-4bc4-8da1-ea6766063cea\"]|[\"Run_time_since_engine_start_s\",\"Catalyst_Temperature__Bank_1_Sensor_1\",\"Fuel_flow_rate_hour_l_hr\",\"ECU_7EB:_Speed__OBD_km_h\",\"Engine_Load_Absolute_pct\"]|{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|\n",
      "|grp:sim001:sim001-1:composite_high:1|sim001    |sim001-1|composite_high|2025-09-16 10:28:03.244|2025-09-16 10:28:03.244|1           |3           |0.7508848994863806 |8e1fb32d-4d0e-4e3e-aa91-3f680a6b2025|2025-09-16|[\"8e1fb32d-4d0e-4e3e-aa91-3f680a6b2025\"]                                                                                                                                                                                                                                                                                 |[\"Catalyst_Temperature__Bank_1_Sensor_1\",\"Catalyst_Temperature__Bank_1_Sensor_2\",\"ECU_7EB:_Engine_RPM_rpm\"]                                               |{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|\n",
      "+------------------------------------+----------+--------+--------------+-----------------------+-----------------------+------------+------------+-------------------+------------------------------------+----------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+----------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "only showing top 2 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === ALERTS CSV — robust re-export + preview first 2 rows from each ===\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from pathlib import Path\n",
    "\n",
    "REPO_ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "DELTA_DIR = REPO_ROOT / r\"infer_stage\\delta\"\n",
    "CSV_DIR   = REPO_ROOT / r\"infer_stage\\csv\"\n",
    "\n",
    "ALERTS_DELTA         = DELTA_DIR / \"engine_module_alerts\"\n",
    "ALERTS_REFINED_DELTA = DELTA_DIR / \"engine_module_alerts_refined\"\n",
    "\n",
    "CSV_FULL_DIR    = CSV_DIR / \"alerts_full_clean\"\n",
    "CSV_REFINED_DIR = CSV_DIR / \"alerts_refined_clean\"\n",
    "\n",
    "# 1) Load delta tables\n",
    "alerts_full_df    = spark.read.format(\"delta\").load(str(ALERTS_DELTA))\n",
    "alerts_refined_df = spark.read.format(\"delta\").load(str(ALERTS_REFINED_DELTA))\n",
    "\n",
    "# 2) JSON-stringify complex fields to avoid CSV column shifts\n",
    "def make_csv_safe(df):\n",
    "    out = df\n",
    "    complex_cols = []\n",
    "    for c, dt in df.dtypes:\n",
    "        # detect complex types by Spark dtype string\n",
    "        if any(dt.startswith(prefix) for prefix in (\"array\", \"map\", \"struct\")):\n",
    "            complex_cols.append(c)\n",
    "    # add shadow columns with _json suffix and drop original complex\n",
    "    for c in complex_cols:\n",
    "        out = out.withColumn(c + \"_json\", F.to_json(F.col(c)))\n",
    "        out = out.drop(c)\n",
    "    # ensure all remaining are cast to string for consistent quoting\n",
    "    for c in out.columns:\n",
    "        out = out.withColumn(c, F.col(c).cast(T.StringType()))\n",
    "    return out\n",
    "\n",
    "alerts_full_csv    = make_csv_safe(alerts_full_df)\n",
    "alerts_refined_csv = make_csv_safe(alerts_refined_df)\n",
    "\n",
    "# 3) Write CSV with strict quoting/escaping\n",
    "def write_csv(df, target_dir: Path):\n",
    "    (df.coalesce(1)\n",
    "       .write.mode(\"overwrite\")\n",
    "       .option(\"header\", True)\n",
    "       .option(\"quote\", '\"')\n",
    "       .option(\"escape\", '\"')\n",
    "       .option(\"quoteAll\", True)\n",
    "       .option(\"multiLine\", False)\n",
    "       .csv(str(target_dir)))\n",
    "\n",
    "write_csv(alerts_full_csv,    CSV_FULL_DIR)\n",
    "write_csv(alerts_refined_csv, CSV_REFINED_DIR)\n",
    "\n",
    "print(\"Wrote CSV:\")\n",
    "print(\"  Full   :\", CSV_FULL_DIR)\n",
    "print(\"  Refined:\", CSV_REFINED_DIR)\n",
    "\n",
    "# 4) Read back and show first 2 rows from each (for sanity)\n",
    "def preview_csv(path: Path, name: str):\n",
    "    df = (spark.read\n",
    "                .option(\"header\", True)\n",
    "                .option(\"quote\", '\"')\n",
    "                .option(\"escape\", '\"')\n",
    "                .csv(str(path)))\n",
    "    print(f\"\\n=== Preview: {name} (first 2 rows) ===\")\n",
    "    df.show(2, truncate=False)\n",
    "\n",
    "preview_csv(CSV_FULL_DIR,    \"alerts_full_clean\")\n",
    "preview_csv(CSV_REFINED_DIR, \"alerts_refined_clean\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "9ab93171-55f5-4e05-a9bc-8e69164b3911",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== vehicle_health_summary (first row) ===\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+------------------------------------------------------------------------------------------------+----------+-------------+-----------------------------------------+--------------------------+\n",
      "|vehicle_id|date      |rows_count|anomaly_count|anomaly_rate       |median_composite_score|p95_composite_score|health_score      |days_since_last_alert|top_failure_modes                                                                               |trend_flag|estimated_rul|model_versions                           |last_inference_ts         |\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+------------------------------------------------------------------------------------------------+----------+-------------+-----------------------------------------+--------------------------+\n",
      "|sim001    |2025-09-17|450       |222          |0.49333333333333335|0.4981763664882844    |0.7116394177346642 |55.309418010702935|0                    |[Run_time_since_engine_start_s, Catalyst_Temperature__Bank_1_Sensor_1, ECU_7EB:_Speed__OBD_km_h]|steady    |33.2         |{dense -> ts, lstm -> ts, isof -> joblib}|2025-10-03 02:26:20.113502|\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+------------------------------------------------------------------------------------------------+----------+-------------+-----------------------------------------+--------------------------+\n",
      "only showing top 1 row\n",
      "\n",
      "CSV written to: C:\\engine_module_pipeline\\infer_stage\\csv\\vehicle_health_summary_clean\n",
      "=== vehicle_health_summary CSV (first row) ===\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+----------+-------------+--------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "|vehicle_id|date      |rows_count|anomaly_count|anomaly_rate       |median_composite_score|p95_composite_score|health_score      |days_since_last_alert|trend_flag|estimated_rul|last_inference_ts         |top_failure_modes_json                                                                              |model_versions_json                       |\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+----------+-------------+--------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "|sim001    |2025-09-17|450       |222          |0.49333333333333335|0.4981763664882844    |0.7116394177346642 |55.309418010702935|0                    |steady    |33.2         |2025-10-03 02:26:20.113502|[\"Run_time_since_engine_start_s\",\"Catalyst_Temperature__Bank_1_Sensor_1\",\"ECU_7EB:_Speed__OBD_km_h\"]|{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|\n",
      "+----------+----------+----------+-------------+-------------------+----------------------+-------------------+------------------+---------------------+----------+-------------+--------------------------+----------------------------------------------------------------------------------------------------+------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === VEHICLE HEALTH SUMMARY — build from results+alerts, write Delta+CSV, preview ===\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F, Window as W\n",
    "from pyspark.sql import types as T\n",
    "\n",
    "# ----- Paths -----\n",
    "REPO_ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "DELTA_DIR = REPO_ROOT / r\"infer_stage\\delta\"\n",
    "CSV_DIR   = REPO_ROOT / r\"infer_stage\\csv\"\n",
    "\n",
    "INFER_RESULTS_DELTA = DELTA_DIR / \"engine_module_inference_results\"\n",
    "ALERTS_DELTA        = DELTA_DIR / \"engine_module_alerts\"\n",
    "VEH_HEALTH_DELTA    = DELTA_DIR / \"vehicle_health_summary\"\n",
    "\n",
    "CSV_VEH_DIR = CSV_DIR / \"vehicle_health_summary_clean\"\n",
    "\n",
    "# ----- Load sources -----\n",
    "res = spark.read.format(\"delta\").load(str(INFER_RESULTS_DELTA)) \\\n",
    "        .select(\n",
    "            F.col(\"source_id\").alias(\"vehicle_id\"),\n",
    "            F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "            F.col(\"row_hash\"),\n",
    "            F.col(\"anomaly_severity\").cast(\"int\").alias(\"anomaly_severity\"),\n",
    "            F.col(\"composite_score\").cast(\"double\").alias(\"composite_score\"),\n",
    "            F.col(\"inference_ts\").cast(\"timestamp\").alias(\"inference_ts\"),\n",
    "            F.col(\"model_versions\").cast(\"map<string,string>\").alias(\"model_versions\")\n",
    "        )\n",
    "\n",
    "alerts = spark.read.format(\"delta\").load(str(ALERTS_DELTA)) \\\n",
    "        .select(\n",
    "            F.col(\"vehicle_id\").cast(\"string\").alias(\"vehicle_id\"),\n",
    "            F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "            F.col(\"alert_ts\").cast(\"timestamp\").alias(\"alert_ts\"),\n",
    "            F.col(\"top_features\").cast(\"array<string>\").alias(\"top_features\")\n",
    "        )\n",
    "\n",
    "# ----- Aggregate per vehicle_id + date from inference_results -----\n",
    "# rows_count, anomaly_count (severity >=2), median & p95 composite, last_inference_ts\n",
    "agg_base = (\n",
    "    res.groupBy(\"vehicle_id\", \"date\")\n",
    "       .agg(\n",
    "           F.count(\"*\").alias(\"rows_count\"),\n",
    "           F.sum(F.when(F.col(\"anomaly_severity\") >= 2, 1).otherwise(0)).alias(\"anomaly_count\"),\n",
    "           F.expr(\"percentile_approx(composite_score, 0.5)\").alias(\"median_composite_score\"),\n",
    "           F.expr(\"percentile_approx(composite_score, 0.95)\").alias(\"p95_composite_score\"),\n",
    "           F.max(\"inference_ts\").alias(\"last_inference_ts\")\n",
    "       )\n",
    ")\n",
    "\n",
    "# Bring forward a representative model_versions per vehicle/day (from the latest row that day)\n",
    "w_day = W.partitionBy(\"vehicle_id\", \"date\").orderBy(F.col(\"inference_ts\").desc())\n",
    "versions_latest = (\n",
    "    res.withColumn(\"rn\", F.row_number().over(w_day))\n",
    "       .where(F.col(\"rn\") == 1)\n",
    "       .select(\"vehicle_id\", \"date\", F.col(\"model_versions\").alias(\"model_versions_latest\"))\n",
    ")\n",
    "\n",
    "veh_day = (\n",
    "    agg_base.join(versions_latest, on=[\"vehicle_id\",\"date\"], how=\"left\")\n",
    "            .withColumn(\"model_versions\",\n",
    "                        F.when(F.col(\"model_versions_latest\").isNotNull(), F.col(\"model_versions_latest\"))\n",
    "                         .otherwise(F.create_map()))  # empty map if missing\n",
    "            .drop(\"model_versions_latest\")\n",
    ")\n",
    "\n",
    "# anomaly_rate\n",
    "veh_day = veh_day.withColumn(\n",
    "    \"anomaly_rate\",\n",
    "    F.when(F.col(\"rows_count\") > 0, F.col(\"anomaly_count\") / F.col(\"rows_count\")).otherwise(F.lit(0.0))\n",
    ")\n",
    "\n",
    "# ----- Last alert per vehicle (across all dates) & days_since_last_alert per day -----\n",
    "last_alert = (\n",
    "    alerts.groupBy(\"vehicle_id\")\n",
    "          .agg(F.max(F.to_date(\"alert_ts\")).alias(\"last_alert_date\"))\n",
    ")\n",
    "\n",
    "veh_day = (\n",
    "    veh_day.join(last_alert, on=\"vehicle_id\", how=\"left\")\n",
    "           .withColumn(\"day_date\", F.to_date(\"date\"))\n",
    "           .withColumn(\n",
    "               \"days_since_last_alert\",\n",
    "               F.when(F.col(\"last_alert_date\").isNotNull(),\n",
    "                      F.datediff(F.col(\"day_date\"), F.col(\"last_alert_date\")))\n",
    "                .otherwise(F.lit(None).cast(\"int\"))\n",
    "           )\n",
    "           .drop(\"day_date\")\n",
    ")\n",
    "\n",
    "# ----- Top failure modes (from alerts top_features) per vehicle_id+date -----\n",
    "# Explode top_features for that exact date and vehicle, count frequency, take top 3\n",
    "af = (alerts\n",
    "      .withColumn(\"feat\", F.explode(F.coalesce(\"top_features\", F.array().cast(\"array<string>\"))))\n",
    "      .groupBy(\"vehicle_id\", \"date\", \"feat\")\n",
    "      .agg(F.count(\"*\").alias(\"cnt\")))\n",
    "\n",
    "w_feat = W.partitionBy(\"vehicle_id\", \"date\").orderBy(F.col(\"cnt\").desc(), F.col(\"feat\").asc())\n",
    "top_feats_ranked = af.withColumn(\"rk\", F.row_number().over(w_feat)).where(F.col(\"rk\") <= 3)\n",
    "\n",
    "top_feats = (top_feats_ranked\n",
    "             .groupBy(\"vehicle_id\", \"date\")\n",
    "             .agg(F.collect_list(\"feat\").alias(\"top_failure_modes\")))\n",
    "\n",
    "veh_day = veh_day.join(top_feats, on=[\"vehicle_id\",\"date\"], how=\"left\") \\\n",
    "                 .withColumn(\"top_failure_modes\",\n",
    "                             F.coalesce(F.col(\"top_failure_modes\"), F.array().cast(\"array<string>\")))\n",
    "\n",
    "# ----- Trend flag (compare median against previous day for that vehicle) -----\n",
    "w_trend = W.partitionBy(\"vehicle_id\").orderBy(F.to_date(\"date\"))\n",
    "veh_day = veh_day.withColumn(\n",
    "    \"prev_median\", F.lag(F.col(\"median_composite_score\")).over(w_trend)\n",
    ")\n",
    "veh_day = veh_day.withColumn(\n",
    "    \"trend_flag\",\n",
    "    F.when(F.col(\"prev_median\").isNull(), F.lit(\"steady\"))\n",
    "     .when(F.col(\"median_composite_score\") - F.col(\"prev_median\") > F.lit(0.10), F.lit(\"worsening\"))\n",
    "     .when(F.col(\"median_composite_score\") - F.col(\"prev_median\") < F.lit(-0.10), F.lit(\"improving\"))\n",
    "     .otherwise(F.lit(\"steady\"))\n",
    ").drop(\"prev_median\")\n",
    "\n",
    "# ----- Health score (same logic you used earlier) -----\n",
    "# health_score = clip( (1 - (0.6*median + 0.3*anomaly_rate + 0.1*(1 - recency))) * 100 )\n",
    "veh_day = veh_day.withColumn(\"recency\", F.lit(1.0))\n",
    "veh_day = veh_day.withColumn(\n",
    "    \"health_score_raw\",\n",
    "    1.0 - (0.6*F.col(\"median_composite_score\") + 0.3*F.col(\"anomaly_rate\") + 0.1*(1.0 - F.col(\"recency\")))\n",
    ")\n",
    "veh_day = veh_day.withColumn(\"health_score\",\n",
    "                             F.when(F.col(\"health_score_raw\") < 0, 0.0)\n",
    "                              .when(F.col(\"health_score_raw\") > 1, 1.0)\n",
    "                              .otherwise(F.col(\"health_score_raw\")) * 100.0\n",
    "                            ).drop(\"health_score_raw\",\"recency\")\n",
    "\n",
    "# ----- Estimated RUL (simple, monotone with health) -----\n",
    "veh_day = veh_day.withColumn(\n",
    "    \"estimated_rul\",\n",
    "    F.round((F.col(\"health_score\")/100.0) * F.lit(60.0), 1).cast(\"double\")\n",
    ")\n",
    "\n",
    "# ----- Normalize columns to match VEH_HEALTH_SCHEMA (types + names) -----\n",
    "veh_day_out = (veh_day\n",
    "    .select(\n",
    "        F.col(\"vehicle_id\").cast(\"string\").alias(\"vehicle_id\"),\n",
    "        F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "        F.col(\"rows_count\").cast(\"long\").alias(\"rows_count\"),\n",
    "        F.col(\"anomaly_count\").cast(\"long\").alias(\"anomaly_count\"),\n",
    "        F.col(\"anomaly_rate\").cast(\"double\").alias(\"anomaly_rate\"),\n",
    "        F.col(\"median_composite_score\").cast(\"double\").alias(\"median_composite_score\"),\n",
    "        F.col(\"p95_composite_score\").cast(\"double\").alias(\"p95_composite_score\"),\n",
    "        F.col(\"health_score\").cast(\"double\").alias(\"health_score\"),\n",
    "        F.col(\"days_since_last_alert\").cast(\"int\").alias(\"days_since_last_alert\"),\n",
    "        F.col(\"top_failure_modes\").cast(\"array<string>\").alias(\"top_failure_modes\"),\n",
    "        F.col(\"trend_flag\").cast(\"string\").alias(\"trend_flag\"),\n",
    "        F.col(\"estimated_rul\").cast(\"double\").alias(\"estimated_rul\"),\n",
    "        F.col(\"model_versions\").cast(\"map<string,string>\").alias(\"model_versions\"),\n",
    "        F.col(\"last_inference_ts\").cast(\"timestamp\").alias(\"last_inference_ts\"),\n",
    "    )\n",
    ")\n",
    "\n",
    "# ----- Ensure Delta table exists with your preset schema (call your existing helper if available) -----\n",
    "# (If you already initialized the table earlier, this will just write.)\n",
    "from delta.tables import DeltaTable\n",
    "try:\n",
    "    DeltaTable.forPath(spark, str(VEH_HEALTH_DELTA))\n",
    "    table_exists = True\n",
    "except Exception:\n",
    "    table_exists = False\n",
    "\n",
    "if not table_exists:\n",
    "    # Minimal schema (matches your earlier VEH_HEALTH_SCHEMA)\n",
    "    from pyspark.sql.types import *\n",
    "    VEH_HEALTH_SCHEMA = StructType([\n",
    "        StructField(\"vehicle_id\", StringType(), False),\n",
    "        StructField(\"date\", StringType(), False),\n",
    "        StructField(\"rows_count\", LongType(), True),\n",
    "        StructField(\"anomaly_count\", LongType(), True),\n",
    "        StructField(\"anomaly_rate\", DoubleType(), True),\n",
    "        StructField(\"median_composite_score\", DoubleType(), True),\n",
    "        StructField(\"p95_composite_score\", DoubleType(), True),\n",
    "        StructField(\"health_score\", DoubleType(), True),\n",
    "        StructField(\"days_since_last_alert\", IntegerType(), True),\n",
    "        StructField(\"top_failure_modes\", ArrayType(StringType()), True),\n",
    "        StructField(\"trend_flag\", StringType(), True),\n",
    "        StructField(\"estimated_rul\", DoubleType(), True),\n",
    "        StructField(\"model_versions\", MapType(StringType(), StringType()), True),\n",
    "        StructField(\"last_inference_ts\", TimestampType(), True),\n",
    "    ])\n",
    "    spark.createDataFrame([], VEH_HEALTH_SCHEMA) \\\n",
    "         .write.format(\"delta\").mode(\"overwrite\").save(str(VEH_HEALTH_DELTA))\n",
    "\n",
    "# ----- MERGE into Delta (vehicle_id + date) -----\n",
    "tgt = DeltaTable.forPath(spark, str(VEH_HEALTH_DELTA))\n",
    "cols = veh_day_out.columns\n",
    "(\n",
    "  tgt.alias(\"t\")\n",
    "     .merge(\n",
    "        veh_day_out.alias(\"s\"),\n",
    "        \"t.vehicle_id = s.vehicle_id AND t.date = s.date\"\n",
    "     )\n",
    "     .whenMatchedUpdate(\n",
    "        condition=\"s.last_inference_ts > t.last_inference_ts\",\n",
    "        set={c: f\"s.`{c}`\" for c in cols}\n",
    "     )\n",
    "     .whenNotMatchedInsert(values={c: f\"s.`{c}`\" for c in cols})\n",
    "     .execute()\n",
    ")\n",
    "\n",
    "# ----- Preview first row from the Delta table -----\n",
    "print(\"=== vehicle_health_summary (first row) ===\")\n",
    "spark.read.format(\"delta\").load(str(VEH_HEALTH_DELTA)).show(1, truncate=False)\n",
    "\n",
    "# ----- CSV export (JSON-safe for complex columns) -----\n",
    "def make_csv_safe(df):\n",
    "    out = df\n",
    "    # JSONify complex columns\n",
    "    for c, dt in df.dtypes:\n",
    "        if dt.startswith(\"array\") or dt.startswith(\"map\") or dt.startswith(\"struct\"):\n",
    "            out = out.withColumn(c + \"_json\", F.to_json(F.col(c))).drop(c)\n",
    "    # Cast remaining to string & quote all\n",
    "    for c in out.columns:\n",
    "        out = out.withColumn(c, F.col(c).cast(\"string\"))\n",
    "    return out\n",
    "\n",
    "veh_csv = make_csv_safe(spark.read.format(\"delta\").load(str(VEH_HEALTH_DELTA)))\n",
    "\n",
    "(veh_csv.coalesce(1)\n",
    "        .write.mode(\"overwrite\")\n",
    "        .option(\"header\", True)\n",
    "        .option(\"quote\", '\"')\n",
    "        .option(\"escape\", '\"')\n",
    "        .option(\"quoteAll\", True)\n",
    "        .option(\"multiLine\", False)\n",
    "        .csv(str(CSV_VEH_DIR)))\n",
    "\n",
    "print(\"CSV written to:\", CSV_VEH_DIR)\n",
    "\n",
    "# Read back CSV and print first row to confirm header alignment\n",
    "csv_back = (spark.read\n",
    "                 .option(\"header\", True)\n",
    "                 .option(\"quote\", '\"')\n",
    "                 .option(\"escape\", '\"')\n",
    "                 .csv(str(CSV_VEH_DIR)))\n",
    "print(\"=== vehicle_health_summary CSV (first row) ===\")\n",
    "csv_back.show(1, truncate=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "bf676602-1da4-4928-a498-6d7a271fdd5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== engine_module_model_metadata (first row) ===\n",
      "+------------------------------------+--------------------------+-----------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------+----------+\n",
      "|inference_run_id                    |timestamp                 |model_versions                           |params                                                                                                                                                               |baseline_stats                                                                                                                                                                                                                                                                |notes|source_commit|date      |\n",
      "+------------------------------------+--------------------------+-----------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------+----------+\n",
      "|run-c9fd450635cc41388b962a452a3aea73|2025-10-03 02:26:20.113502|{dense -> ts, lstm -> ts, isof -> joblib}|{LSTM_WINDOW -> 10, ALERT_THRESHOLD -> 0.75, WEIGHTS -> {\"dense\":0.35,\"lstm\":0.25,\"isof\":0.20,\"kde\":0.10,\"gmm\":0.10}, ROBUST_SCALE -> RobustScaler(25-75) train-only}|{dense_p50 -> 0.011641, dense_p95 -> 0.046920, lstm_p50 -> 0.063774, lstm_p95 -> 0.637908, isof_p50 -> 0.006885, isof_p05 -> -0.113839, kde_p50 -> -38.020738, kde_p05 -> -66.899949, gmm_p50 -> -8.389138, gmm_p05 -> -56.270197, comp_p50 -> 0.523261, comp_p95 -> 0.717427}|NULL |NULL         |2025-09-16|\n",
      "+------------------------------------+--------------------------+-----------------------------------------+---------------------------------------------------------------------------------------------------------------------------------------------------------------------+------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-----+-------------+----------+\n",
      "only showing top 1 row\n",
      "\n",
      "CSV written to: C:\\engine_module_pipeline\\infer_stage\\csv\\engine_module_model_metadata_clean\n",
      "=== model_metadata CSV (first row) ===\n",
      "+------------------------------------+--------------------------+-----+-------------+----------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|inference_run_id                    |timestamp                 |notes|source_commit|date      |model_versions_json                       |params_json                                                                                                                                                                     |baseline_stats_json                                                                                                                                                                                                                                                            |\n",
      "+------------------------------------+--------------------------+-----+-------------+----------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "|run-c9fd450635cc41388b962a452a3aea73|2025-10-03 02:26:20.113502|NULL |NULL         |2025-09-16|{\"dense\":\"ts\",\"lstm\":\"ts\",\"isof\":\"joblib\"}|{\"LSTM_WINDOW\":\"10\",\"ALERT_THRESHOLD\":\"0.75\",\"WEIGHTS\":\"{\\\"dense\\\":0.35,\\\"lstm\\\":0.25,\\\"isof\\\":0.20,\\\"kde\\\":0.10,\\\"gmm\\\":0.10}\",\"ROBUST_SCALE\":\"RobustScaler(25-75) train-only\"}|{\"dense_p50\":\"0.011641\",\"dense_p95\":\"0.046920\",\"lstm_p50\":\"0.063774\",\"lstm_p95\":\"0.637908\",\"isof_p50\":\"0.006885\",\"isof_p05\":\"-0.113839\",\"kde_p50\":\"-38.020738\",\"kde_p05\":\"-66.899949\",\"gmm_p50\":\"-8.389138\",\"gmm_p05\":\"-56.270197\",\"comp_p50\":\"0.523261\",\"comp_p95\":\"0.717427\"}|\n",
      "+------------------------------------+--------------------------+-----+-------------+----------+------------------------------------------+--------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+-------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------+\n",
      "only showing top 1 row\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# === MODEL METADATA — build per inference_run_id, write Delta+CSV, preview ===\n",
    "from pathlib import Path\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql import types as T\n",
    "from delta.tables import DeltaTable\n",
    "\n",
    "# ----- Paths -----\n",
    "REPO_ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "RES_DELTA = REPO_ROOT / r\"infer_stage\\delta\\engine_module_inference_results\"   # source\n",
    "META_DELTA = REPO_ROOT / r\"delta\\engine_module_model_metadata\"                 # target (as requested)\n",
    "CSV_DIR   = REPO_ROOT / r\"infer_stage\\csv\\engine_module_model_metadata_clean\"\n",
    "\n",
    "# ----- Load inference results (source) -----\n",
    "res = (spark.read.format(\"delta\").load(str(RES_DELTA))\n",
    "         .select(\n",
    "            F.col(\"inference_run_id\").cast(\"string\").alias(\"inference_run_id\"),\n",
    "            F.col(\"inference_ts\").cast(\"timestamp\").alias(\"inference_ts\"),\n",
    "            F.col(\"date\").cast(\"string\").alias(\"date\"),\n",
    "            F.col(\"model_versions\").cast(\"map<string,string>\").alias(\"model_versions\"),\n",
    "            # Stats inputs (optional but nice to carry into baseline_stats)\n",
    "            F.col(\"recon_error_dense\").cast(\"double\").alias(\"recon_error_dense\"),\n",
    "            F.col(\"recon_error_lstm\").cast(\"double\").alias(\"recon_error_lstm\"),\n",
    "            F.col(\"isolation_score\").cast(\"double\").alias(\"isolation_score\"),\n",
    "            F.col(\"kde_logp\").cast(\"double\").alias(\"kde_logp\"),\n",
    "            F.col(\"gmm_logp\").cast(\"double\").alias(\"gmm_logp\"),\n",
    "            F.col(\"composite_score\").cast(\"double\").alias(\"composite_score\")\n",
    "         )\n",
    "      )\n",
    "\n",
    "# Guard for empty source\n",
    "if res.head(1):\n",
    "    # ----- Choose a representative row per run (latest inference_ts) -----\n",
    "    w = F.window(\"inference_ts\", \"36500 days\")  # big window not used; we’ll window by run_id instead\n",
    "    from pyspark.sql import Window\n",
    "    wrun = Window.partitionBy(\"inference_run_id\").orderBy(F.col(\"inference_ts\").desc())\n",
    "\n",
    "    res_ranked = res.withColumn(\"rn\", F.row_number().over(wrun))\n",
    "    reps = (res_ranked.where(F.col(\"rn\") == 1)\n",
    "                      .select(\"inference_run_id\", \"inference_ts\", \"date\", \"model_versions\"))\n",
    "\n",
    "    # ----- Aggregate baseline stats per run (string map) -----\n",
    "    # We compute robust-ish summaries and stringify them for baseline_stats.\n",
    "    stats = (res.groupBy(\"inference_run_id\")\n",
    "                .agg(\n",
    "                    F.expr(\"percentile_approx(recon_error_dense, 0.5)\").alias(\"dense_p50\"),\n",
    "                    F.expr(\"percentile_approx(recon_error_dense, 0.95)\").alias(\"dense_p95\"),\n",
    "                    F.expr(\"percentile_approx(recon_error_lstm, 0.5)\").alias(\"lstm_p50\"),\n",
    "                    F.expr(\"percentile_approx(recon_error_lstm, 0.95)\").alias(\"lstm_p95\"),\n",
    "                    F.expr(\"percentile_approx(isolation_score, 0.5)\").alias(\"isof_p50\"),\n",
    "                    F.expr(\"percentile_approx(isolation_score, 0.05)\").alias(\"isof_p05\"),\n",
    "                    F.expr(\"percentile_approx(kde_logp, 0.5)\").alias(\"kde_p50\"),\n",
    "                    F.expr(\"percentile_approx(kde_logp, 0.05)\").alias(\"kde_p05\"),\n",
    "                    F.expr(\"percentile_approx(gmm_logp, 0.5)\").alias(\"gmm_p50\"),\n",
    "                    F.expr(\"percentile_approx(gmm_logp, 0.05)\").alias(\"gmm_p05\"),\n",
    "                    F.expr(\"percentile_approx(composite_score, 0.5)\").alias(\"comp_p50\"),\n",
    "                    F.expr(\"percentile_approx(composite_score, 0.95)\").alias(\"comp_p95\")\n",
    "                ))\n",
    "\n",
    "    # stringify numerics into a map<string,string>\n",
    "    def str_map(cols):\n",
    "        pairs = []\n",
    "        for c in cols:\n",
    "            pairs.append(F.lit(c))\n",
    "            pairs.append(F.coalesce(F.format_number(F.col(c), 6), F.lit(\"null\")))\n",
    "        return F.create_map(*pairs)\n",
    "\n",
    "    baseline_cols = [\"dense_p50\",\"dense_p95\",\"lstm_p50\",\"lstm_p95\",\"isof_p50\",\"isof_p05\",\n",
    "                     \"kde_p50\",\"kde_p05\",\"gmm_p50\",\"gmm_p05\",\"comp_p50\",\"comp_p95\"]\n",
    "    stats_str = stats.select(\n",
    "        F.col(\"inference_run_id\"),\n",
    "        str_map(baseline_cols).alias(\"baseline_stats\")\n",
    "    )\n",
    "\n",
    "    # ----- Params map (minimal but complete; keep in sync with your pipeline knobs) -----\n",
    "    PARAMS = {\n",
    "        \"LSTM_WINDOW\": \"10\",\n",
    "        \"ALERT_THRESHOLD\": \"0.75\",\n",
    "        \"WEIGHTS\": '{\"dense\":0.35,\"lstm\":0.25,\"isof\":0.20,\"kde\":0.10,\"gmm\":0.10}',\n",
    "        \"ROBUST_SCALE\": \"RobustScaler(25-75) train-only\"\n",
    "    }\n",
    "    params_df = (reps\n",
    "                 .select(\"inference_run_id\")\n",
    "                 .withColumn(\"params\",\n",
    "                             F.create_map(\n",
    "                                 *[x for kv in PARAMS.items() for x in (F.lit(kv[0]), F.lit(kv[1]))]\n",
    "                             ).cast(\"map<string,string>\")\n",
    "                            )\n",
    "                )\n",
    "\n",
    "    # ----- Assemble final rows as per schema -----\n",
    "    meta = (reps.join(stats_str, \"inference_run_id\", \"left\")\n",
    "                 .join(params_df, \"inference_run_id\", \"left\")\n",
    "                 .withColumnRenamed(\"inference_ts\", \"timestamp\")\n",
    "                 .withColumn(\"notes\", F.lit(None).cast(\"string\"))\n",
    "                 .withColumn(\"source_commit\", F.lit(None).cast(\"string\"))\n",
    "            )\n",
    "\n",
    "    # Normalize types and order to preset schema\n",
    "    meta_out = (meta\n",
    "        .select(\n",
    "            F.col(\"inference_run_id\").cast(\"string\").alias(\"inference_run_id\"),\n",
    "            F.col(\"timestamp\").cast(\"timestamp\").alias(\"timestamp\"),\n",
    "            F.col(\"model_versions\").cast(\"map<string,string>\").alias(\"model_versions\"),\n",
    "            F.coalesce(F.col(\"params\").cast(\"map<string,string>\"), F.create_map()).alias(\"params\"),\n",
    "            F.coalesce(F.col(\"baseline_stats\").cast(\"map<string,string>\"), F.create_map()).alias(\"baseline_stats\"),\n",
    "            F.col(\"notes\").cast(\"string\").alias(\"notes\"),\n",
    "            F.col(\"source_commit\").cast(\"string\").alias(\"source_commit\"),\n",
    "            F.col(\"date\").cast(\"string\").alias(\"date\")\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # ----- Ensure target Delta table exists with preset schema -----\n",
    "    MODEL_METADATA_SCHEMA = T.StructType([\n",
    "        T.StructField(\"inference_run_id\", T.StringType(), False),\n",
    "        T.StructField(\"timestamp\", T.TimestampType(), False),\n",
    "        T.StructField(\"model_versions\", T.MapType(T.StringType(), T.StringType()), False),\n",
    "        T.StructField(\"params\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "        T.StructField(\"baseline_stats\", T.MapType(T.StringType(), T.StringType()), True),\n",
    "        T.StructField(\"notes\", T.StringType(), True),\n",
    "        T.StructField(\"source_commit\", T.StringType(), True),\n",
    "        T.StructField(\"date\", T.StringType(), False),\n",
    "    ])\n",
    "\n",
    "    try:\n",
    "        DeltaTable.forPath(spark, str(META_DELTA))\n",
    "        table_exists = True\n",
    "    except Exception:\n",
    "        table_exists = False\n",
    "\n",
    "    if not table_exists:\n",
    "        spark.createDataFrame([], MODEL_METADATA_SCHEMA) \\\n",
    "             .write.format(\"delta\").mode(\"overwrite\").save(str(META_DELTA))\n",
    "\n",
    "    # ----- MERGE by inference_run_id (keep the freshest timestamp) -----\n",
    "    tgt = DeltaTable.forPath(spark, str(META_DELTA))\n",
    "    cols = [f.name for f in MODEL_METADATA_SCHEMA]\n",
    "    (tgt.alias(\"t\")\n",
    "        .merge(meta_out.alias(\"s\"), \"t.inference_run_id = s.inference_run_id\")\n",
    "        .whenMatchedUpdate(\n",
    "            condition=\"s.timestamp > t.timestamp\",\n",
    "            set={c: f\"s.`{c}`\" for c in cols}\n",
    "        )\n",
    "        .whenNotMatchedInsert(values={c: f\"s.`{c}`\" for c in cols})\n",
    "        .execute())\n",
    "\n",
    "    # ----- Preview first row from Delta table -----\n",
    "    print(\"=== engine_module_model_metadata (first row) ===\")\n",
    "    spark.read.format(\"delta\").load(str(META_DELTA)).show(1, truncate=False)\n",
    "\n",
    "    # ----- CSV export mirroring the table (JSON-encode complex fields) -----\n",
    "    def csv_safe(df):\n",
    "        out = df\n",
    "        for c, dt in df.dtypes:\n",
    "            if dt.startswith(\"map\") or dt.startswith(\"array\") or dt.startswith(\"struct\"):\n",
    "                out = out.withColumn(c + \"_json\", F.to_json(F.col(c))).drop(c)\n",
    "        for c in out.columns:\n",
    "            out = out.withColumn(c, F.col(c).cast(\"string\"))\n",
    "        return out\n",
    "\n",
    "    csv_df = csv_safe(spark.read.format(\"delta\").load(str(META_DELTA)))\n",
    "    (csv_df.coalesce(1)\n",
    "          .write.mode(\"overwrite\")\n",
    "          .option(\"header\", True)\n",
    "          .option(\"quote\", '\"')\n",
    "          .option(\"escape\", '\"')\n",
    "          .option(\"quoteAll\", True)\n",
    "          .csv(str(CSV_DIR)))\n",
    "\n",
    "    print(\"CSV written to:\", CSV_DIR)\n",
    "\n",
    "    # Read back CSV to confirm header alignment\n",
    "    csv_back = (spark.read\n",
    "                    .option(\"header\", True)\n",
    "                    .option(\"quote\", '\"')\n",
    "                    .option(\"escape\", '\"')\n",
    "                    .csv(str(CSV_DIR)))\n",
    "    print(\"=== model_metadata CSV (first row) ===\")\n",
    "    csv_back.show(1, truncate=False)\n",
    "\n",
    "else:\n",
    "    print(\"[model_metadata] No inference_results found; nothing to build.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "585d8c4b-fd50-41ad-8f74-3906bcaaca12",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark (Py3.11 + Spark 3.5)",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
