{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0040324b-76f8-40d5-9684-3db967f7ea8d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x22b18968130>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 1: Imports, global paths, and small utilities\n",
    "\n",
    "from __future__ import annotations\n",
    "import os, json, math, gc, time, warnings\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score, brier_score_loss\n",
    "from sklearn.utils import shuffle as sk_shuffle\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# --- Paths (Windows-friendly) ---\n",
    "DATA_ROOT = Path(r\"C:\\engine_module_pipeline\\DTC_stage\\data\\synthetic\")\n",
    "ARTIFACTS_ROOT = Path(r\"C:\\engine_module_pipeline\\DTC_stage\\artifacts\")\n",
    "ARTIFACTS_ROOT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# --- Helper: load JSON ---\n",
    "def read_json(p: Path) -> dict:\n",
    "    with open(p, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "# --- Helper: find valid file even if typo existed ---\n",
    "def find_split_file(dtc_dir: Path, dtc_code: str, split: str) -> Path:\n",
    "    expected = dtc_dir / f\"{dtc_code}_synth_{split}.csv\"\n",
    "    if expected.exists():\n",
    "        return expected\n",
    "    # fallback for the common typo \"vaild\"\n",
    "    if split == \"valid\":\n",
    "        typo = dtc_dir / f\"{dtc_code}_synth_vaild.csv\"\n",
    "        if typo.exists():\n",
    "            return typo\n",
    "    raise FileNotFoundError(f\"Missing split file for {dtc_code}, split={split}. \"\n",
    "                            f\"Tried: {expected} (and typo fallback for valid)\")\n",
    "\n",
    "# --- Small metric helpers ---\n",
    "def safe_auc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    # AUROC can fail if only one class present; return 0.5 (chance)\n",
    "    try:\n",
    "        return float(roc_auc_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return 0.5\n",
    "\n",
    "def safe_auprc(y_true: np.ndarray, y_score: np.ndarray) -> float:\n",
    "    try:\n",
    "        return float(average_precision_score(y_true, y_score))\n",
    "    except Exception:\n",
    "        return np.mean(y_true)  # baseline prevalence\n",
    "\n",
    "def reliability_bins(y_true: np.ndarray, y_prob: np.ndarray, n_bins: int = 10):\n",
    "    bins = np.linspace(0.0, 1.0, n_bins + 1)\n",
    "    idx = np.digitize(y_prob, bins) - 1\n",
    "    # collect per bin\n",
    "    prob_mean, true_rate, count = [], [], []\n",
    "    for b in range(n_bins):\n",
    "        mask = idx == b\n",
    "        if mask.any():\n",
    "            prob_mean.append(y_prob[mask].mean())\n",
    "            true_rate.append(y_true[mask].mean())\n",
    "            count.append(mask.sum())\n",
    "        else:\n",
    "            prob_mean.append(np.nan)\n",
    "            true_rate.append(np.nan)\n",
    "            count.append(0)\n",
    "    return np.array(prob_mean), np.array(true_rate), np.array(count)\n",
    "\n",
    "# Repro\n",
    "GLOBAL_SEED = 1337\n",
    "np.random.seed(GLOBAL_SEED)\n",
    "torch.manual_seed(GLOBAL_SEED)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8ec81a43-593a-4f4f-ba4b-2a58f5c3b63e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Per-DTC default window sizes and the dataset class\n",
    "\n",
    "# Reasonable defaults per DTC family (seconds = samples when cadence=1s)\n",
    "DEFAULT_WINDOW_BY_FAMILY = {\n",
    "    \"BOOST\":   {\"window\": 48,  \"stride\": 1},   # P0234\n",
    "    \"MISFIRE\": {\"window\": 64,  \"stride\": 1},   # P0300\n",
    "    \"ELECT\":   {\"window\": 64,  \"stride\": 1},   # P0562\n",
    "    \"AIRFLOW\": {\"window\": 64,  \"stride\": 1},   # P0101\n",
    "    \"CAT\":     {\"window\": 128, \"stride\": 1},   # P0420\n",
    "    \"THERM\":   {\"window\": 96,  \"stride\": 1},   # P0217, P0125\n",
    "    \"O2\":      {\"window\": 64,  \"stride\": 1},   # P0133\n",
    "    \"SPEED\":   {\"window\": 64,  \"stride\": 1},   # P0501\n",
    "}\n",
    "\n",
    "def dtc_family(dtc: str) -> str:\n",
    "    if dtc == \"P0234\": return \"BOOST\"\n",
    "    if dtc == \"P0300\": return \"MISFIRE\"\n",
    "    if dtc == \"P0562\": return \"ELECT\"\n",
    "    if dtc == \"P0101\": return \"AIRFLOW\"\n",
    "    if dtc == \"P0420\": return \"CAT\"\n",
    "    if dtc in (\"P0217\", \"P0125\"): return \"THERM\"\n",
    "    if dtc == \"P0133\": return \"O2\"\n",
    "    if dtc == \"P0501\": return \"SPEED\"\n",
    "    # default\n",
    "    return \"AIRFLOW\"\n",
    "\n",
    "def get_default_window_stride(dtc: str) -> Tuple[int, int]:\n",
    "    fam = dtc_family(dtc)\n",
    "    spec = DEFAULT_WINDOW_BY_FAMILY.get(fam, {\"window\": 64, \"stride\": 1})\n",
    "    return spec[\"window\"], spec[\"stride\"]\n",
    "\n",
    "# --- Dataset class: builds sliding windows with per-timestep labels ---\n",
    "class DtcWindowDataset(Dataset):\n",
    "    def __init__(\n",
    "        self,\n",
    "        csv_path: Path,\n",
    "        features: List[str],\n",
    "        window: int,\n",
    "        stride: int,\n",
    "        fit_scaler: Optional[RobustScaler] = None,\n",
    "        apply_scaler: Optional[RobustScaler] = None,\n",
    "        cadence_seconds: float = 1.0,\n",
    "    ):\n",
    "        self.csv_path = csv_path\n",
    "        self.features = features\n",
    "        self.window = int(window)\n",
    "        self.stride = int(stride)\n",
    "        self.cadence_seconds = float(cadence_seconds)\n",
    "\n",
    "        df = pd.read_csv(csv_path)\n",
    "        # Ensure timestamp ordering\n",
    "        if \"timestamp\" in df.columns:\n",
    "            df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\")\n",
    "            df = df.sort_values(\"timestamp\").reset_index(drop=True)\n",
    "\n",
    "        # Impute features (forward fill then median)\n",
    "        assert all(f in df.columns for f in features), f\"Missing features in {csv_path}\"\n",
    "        X = df[features].copy()\n",
    "        X = X.replace([np.inf, -np.inf], np.nan)\n",
    "        X = X.fillna(method=\"ffill\")\n",
    "        X = X.fillna(X.median(numeric_only=True))\n",
    "\n",
    "        # Scaler\n",
    "        if fit_scaler is not None:\n",
    "            fit_scaler.fit(X.values.astype(np.float32))\n",
    "            self.scaler = fit_scaler\n",
    "        else:\n",
    "            self.scaler = apply_scaler\n",
    "        Xs = self.scaler.transform(X.values.astype(np.float32)) if self.scaler is not None else X.values.astype(np.float32)\n",
    "\n",
    "        # Labels per timestamp\n",
    "        y_prec = df[\"y_precursor\"].astype(np.float32).values\n",
    "        y_fault = df[\"y_fault_active\"].astype(np.float32).values\n",
    "\n",
    "        # Build windows\n",
    "        self.Xw, self.yw_prec, self.yw_fault = [], [], []\n",
    "        self.t_end_idx = []  # index of window end in original array\n",
    "\n",
    "        N = len(df)\n",
    "        W = self.window\n",
    "        for start in range(0, N - W + 1, self.stride):\n",
    "            end = start + W\n",
    "            self.Xw.append(Xs[start:end, :])\n",
    "            self.yw_prec.append(y_prec[start:end])\n",
    "            self.yw_fault.append(y_fault[start:end])\n",
    "            self.t_end_idx.append(end - 1)\n",
    "\n",
    "        self.Xw = np.stack(self.Xw).astype(np.float32)  # [B, W, C]\n",
    "        self.yw_prec = np.stack(self.yw_prec).astype(np.float32)  # [B, W]\n",
    "        self.yw_fault = np.stack(self.yw_fault).astype(np.float32)  # [B, W]\n",
    "        self.timestamps = df[\"timestamp\"] if \"timestamp\" in df.columns else None\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.Xw.shape[0]\n",
    "\n",
    "    def __getitem__(self, idx: int):\n",
    "        x = self.Xw[idx]         # [W, C]\n",
    "        yp = self.yw_prec[idx]   # [W]\n",
    "        yf = self.yw_fault[idx]  # [W]\n",
    "        return x, yp, yf, self.t_end_idx[idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b013243f-a976-458b-b34b-55fe1fce3293",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: Causal TCN (two heads), loss, training/eval, and plotting\n",
    "\n",
    "class Chomp1d(nn.Module):\n",
    "    \"\"\"Remove right padding to make conv effectively causal.\"\"\"\n",
    "    def __init__(self, chomp_size: int):\n",
    "        super().__init__()\n",
    "        self.chomp_size = chomp_size\n",
    "    def forward(self, x):\n",
    "        return x[:, :, :-self.chomp_size].contiguous() if self.chomp_size > 0 else x\n",
    "\n",
    "class TemporalBlock(nn.Module):\n",
    "    def __init__(self, in_ch, out_ch, kernel_size, dilation, dropout=0.1):\n",
    "        super().__init__()\n",
    "        pad = (kernel_size - 1) * dilation\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Conv1d(in_ch, out_ch, kernel_size, padding=pad, dilation=dilation),\n",
    "            Chomp1d(pad),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Conv1d(out_ch, out_ch, kernel_size, padding=pad, dilation=dilation),\n",
    "            Chomp1d(pad),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(dropout),\n",
    "        )\n",
    "        self.downsample = nn.Conv1d(in_ch, out_ch, kernel_size=1) if in_ch != out_ch else None\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.net(x)\n",
    "        res = x if self.downsample is None else self.downsample(x)\n",
    "        return self.relu(out + res)\n",
    "\n",
    "class TCNTwoHead(nn.Module):\n",
    "    \"\"\"\n",
    "    Input: x [B, T, C]\n",
    "    Output: dict with:\n",
    "        p_precursor: [B, T] probabilities\n",
    "        p_fault:     [B, T] probabilities\n",
    "    \"\"\"\n",
    "    def __init__(self, in_ch: int, hid: int = 64, n_blocks: int = 3, kernel_size: int = 3, dropout: float = 0.1):\n",
    "        super().__init__()\n",
    "        layers = []\n",
    "        ch_in = in_ch\n",
    "        for b in range(n_blocks):\n",
    "            dilation = 2 ** b\n",
    "            layers.append(TemporalBlock(ch_in, hid, kernel_size, dilation, dropout))\n",
    "            ch_in = hid\n",
    "        self.tcn = nn.Sequential(*layers)\n",
    "        self.head_prec = nn.Conv1d(hid, 1, kernel_size=1)\n",
    "        self.head_fault = nn.Conv1d(hid, 1, kernel_size=1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # to [B, C, T] for conv1d\n",
    "        x = x.transpose(1, 2)\n",
    "        z = self.tcn(x)\n",
    "        logit_prec = self.head_prec(z).squeeze(1)  # [B, T]\n",
    "        logit_fault = self.head_fault(z).squeeze(1)\n",
    "        p_prec = self.sigmoid(logit_prec)\n",
    "        p_fault = self.sigmoid(logit_fault)\n",
    "        return {\"p_precursor\": p_prec, \"p_fault\": p_fault, \"logits\": (logit_prec, logit_fault)}\n",
    "\n",
    "def compute_pos_weight(y_seq: np.ndarray, eps: float = 1e-6) -> float:\n",
    "    \"\"\"\n",
    "    y_seq: [N, T] 0/1\n",
    "    \"\"\"\n",
    "    pos = y_seq.sum()\n",
    "    neg = y_seq.size - pos\n",
    "    return float((neg + eps) / (pos + eps))\n",
    "\n",
    "def plot_loss_curves(train_hist: List[float], valid_hist: List[float], out_path: Path):\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.plot(train_hist, label=\"train\")\n",
    "    plt.plot(valid_hist, label=\"valid\")\n",
    "    plt.xlabel(\"epoch\")\n",
    "    plt.ylabel(\"loss\")\n",
    "    plt.title(\"Loss curves\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_path.as_posix(), dpi=150)\n",
    "    plt.close()\n",
    "\n",
    "def plot_reliability(y, p, title, out_path: Path):\n",
    "    pm, tr, cnt = reliability_bins(y, p, n_bins=10)\n",
    "    plt.figure(figsize=(4.5,4.5))\n",
    "    plt.plot([0,1],[0,1], \"--\", lw=1)\n",
    "    mask = ~np.isnan(pm)\n",
    "    plt.scatter(pm[mask], tr[mask], s=np.maximum(10, np.array(cnt[mask]) / np.max(cnt[mask]) * 80))\n",
    "    plt.xlabel(\"Predicted probability (bin mean)\")\n",
    "    plt.ylabel(\"Empirical frequency\")\n",
    "    plt.title(title)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    plt.savefig(out_path.as_posix(), dpi=150)\n",
    "    plt.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "81ffb5ca-9981-42b6-ade7-6b93ea9c21d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 4 (FIXED): Training loop + evaluation + calibration + threshold tuning\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Using device:\", DEVICE)\n",
    "\n",
    "def train_one_dtc(dtc_dir: Path) -> Dict:\n",
    "    meta = read_json(dtc_dir / \"meta.json\")\n",
    "    dtc_code: str = meta[\"dtc_code\"]\n",
    "    features: List[str] = meta[\"features\"]\n",
    "    cadence = float(meta.get(\"cadence_seconds\", 1.0))\n",
    "    win, stride = get_default_window_stride(dtc_code)\n",
    "\n",
    "    # --- Split files ---\n",
    "    f_train = find_split_file(dtc_dir, dtc_code, \"train\")\n",
    "    f_valid = find_split_file(dtc_dir, dtc_code, \"valid\")\n",
    "    f_test  = find_split_file(dtc_dir, dtc_code, \"test\")\n",
    "\n",
    "    # --- Fit scaler on TRAIN only (IQR 25–75) ---\n",
    "    scaler = RobustScaler(with_centering=True, with_scaling=True, quantile_range=(25.0, 75.0))\n",
    "    ds_train = DtcWindowDataset(f_train, features, window=win, stride=stride, fit_scaler=scaler, apply_scaler=None, cadence_seconds=cadence)\n",
    "    ds_valid = DtcWindowDataset(f_valid, features, window=win, stride=stride, fit_scaler=None, apply_scaler=scaler, cadence_seconds=cadence)\n",
    "    ds_test  = DtcWindowDataset(f_test,  features, window=win, stride=stride, fit_scaler=None, apply_scaler=scaler, cadence_seconds=cadence)\n",
    "\n",
    "    # --- DataLoaders ---\n",
    "    train_loader = DataLoader(ds_train, batch_size=64, shuffle=True, drop_last=True)\n",
    "    valid_loader = DataLoader(ds_valid, batch_size=64, shuffle=False)\n",
    "    test_loader  = DataLoader(ds_test,  batch_size=64, shuffle=False)\n",
    "\n",
    "    # --- Model ---\n",
    "    model = TCNTwoHead(in_ch=len(features), hid=64, n_blocks=3, kernel_size=3, dropout=0.10).to(DEVICE)\n",
    "    # class imbalance\n",
    "    pw_prec = compute_pos_weight(ds_train.yw_prec)\n",
    "    pw_fault = compute_pos_weight(ds_train.yw_fault)\n",
    "    loss_prec = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pw_prec, device=DEVICE))\n",
    "    loss_fault = nn.BCEWithLogitsLoss(pos_weight=torch.tensor(pw_fault, device=DEVICE))\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=2e-3, weight_decay=1e-4)\n",
    "    # FIX: use torch.optim.lr_scheduler\n",
    "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode=\"min\", factor=0.5, patience=3, verbose=False)\n",
    "\n",
    "    # --- Train ---\n",
    "    best_state = None\n",
    "    best_valid = 1e9\n",
    "    train_hist, valid_hist = [], []\n",
    "    EPOCHS = 20\n",
    "\n",
    "    for epoch in range(1, EPOCHS + 1):\n",
    "        model.train()\n",
    "        tr_loss = 0.0\n",
    "        for xb, ypb, yfb, _ in train_loader:\n",
    "            xb = xb.to(DEVICE)                         # [B, T, C]\n",
    "            ypb = ypb.to(DEVICE)                       # [B, T]\n",
    "            yfb = yfb.to(DEVICE)\n",
    "            out = model(xb)\n",
    "            logit_prec, logit_fault = out[\"logits\"]\n",
    "            # per-timestep losses\n",
    "            lp = loss_prec(logit_prec, ypb)\n",
    "            lf = loss_fault(logit_fault, yfb)\n",
    "            # TV smoothness penalty to reduce jitter (small)\n",
    "            tv = (out[\"p_fault\"][:, 1:] - out[\"p_fault\"][:, :-1]).abs().mean() + \\\n",
    "                 (out[\"p_precursor\"][:, 1:] - out[\"p_precursor\"][:, :-1]).abs().mean()\n",
    "            loss = lp + lf + 0.01 * tv\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            nn.utils.clip_grad_norm_(model.parameters(), max_norm=2.0)\n",
    "            optimizer.step()\n",
    "            tr_loss += loss.item()\n",
    "        tr_loss /= max(1, len(train_loader))\n",
    "        train_hist.append(tr_loss)\n",
    "\n",
    "        # validation loss\n",
    "        model.eval()\n",
    "        va_loss = 0.0\n",
    "        with torch.no_grad():\n",
    "            for xb, ypb, yfb, _ in valid_loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                ypb = ypb.to(DEVICE)\n",
    "                yfb = yfb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                logit_prec, logit_fault = out[\"logits\"]\n",
    "                lp = loss_prec(logit_prec, ypb)\n",
    "                lf = loss_fault(logit_fault, yfb)\n",
    "                tv = (out[\"p_fault\"][:, 1:] - out[\"p_fault\"][:, :-1]).abs().mean() + \\\n",
    "                     (out[\"p_precursor\"][:, 1:] - out[\"p_precursor\"][:, :-1]).abs().mean()\n",
    "                va_loss += (lp + lf + 0.01 * tv).item()\n",
    "        va_loss /= max(1, len(valid_loader))\n",
    "        valid_hist.append(va_loss)\n",
    "        scheduler.step(va_loss)\n",
    "\n",
    "        if va_loss < best_valid:\n",
    "            best_valid = va_loss\n",
    "            best_state = {k: v.detach().cpu().clone() for k, v in model.state_dict().items()}\n",
    "\n",
    "        print(f\"[{dtc_code}] epoch {epoch:02d} | train={tr_loss:.4f} valid={va_loss:.4f} | pw(fault)={pw_fault:.2f}, pw(prec)={pw_prec:.2f}\")\n",
    "\n",
    "    # Restore best\n",
    "    if best_state is not None:\n",
    "        model.load_state_dict(best_state)\n",
    "\n",
    "    # --- Evaluate on VALID (for calibration) + TEST (for final metrics) ---\n",
    "    def collect_probs_labels(loader):\n",
    "        model.eval()\n",
    "        all_pp, all_pf, all_lp, all_lf = [], [], [], []\n",
    "        with torch.no_grad():\n",
    "            for xb, ypb, yfb, _ in loader:\n",
    "                xb = xb.to(DEVICE)\n",
    "                out = model(xb)\n",
    "                all_pp.append(out[\"p_precursor\"].cpu().numpy())\n",
    "                all_pf.append(out[\"p_fault\"].cpu().numpy())\n",
    "                all_lp.append(ypb.numpy())\n",
    "                all_lf.append(yfb.numpy())\n",
    "        Pp = np.concatenate(all_pp, axis=0).reshape(-1)\n",
    "        Pf = np.concatenate(all_pf, axis=0).reshape(-1)\n",
    "        Lp = np.concatenate(all_lp, axis=0).reshape(-1)\n",
    "        Lf = np.concatenate(all_lf, axis=0).reshape(-1)\n",
    "        return Pp, Pf, Lp, Lf\n",
    "\n",
    "    Pp_val, Pf_val, Lp_val, Lf_val = collect_probs_labels(valid_loader)\n",
    "    Pp_tst, Pf_tst, Lp_tst, Lf_tst = collect_probs_labels(test_loader)\n",
    "\n",
    "    # --- Calibration (isotonic) on VALID ---\n",
    "    iso_prec = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    iso_fault = IsotonicRegression(out_of_bounds=\"clip\")\n",
    "    # Fit on raw probs from VALID\n",
    "    iso_prec.fit(Pp_val, Lp_val)\n",
    "    iso_fault.fit(Pf_val, Lf_val)\n",
    "    # Calibrated\n",
    "    Pp_val_c = iso_prec.transform(Pp_val)\n",
    "    Pf_val_c = iso_fault.transform(Pf_val)\n",
    "    Pp_tst_c = iso_prec.transform(Pp_tst)\n",
    "    Pf_tst_c = iso_fault.transform(Pf_tst)\n",
    "\n",
    "    # --- Metrics ---\n",
    "    metrics = {\n",
    "        \"valid\": {\n",
    "            \"AUROC_precursor\": safe_auc(Lp_val, Pp_val_c),\n",
    "            \"AUPRC_precursor\": safe_auprc(Lp_val, Pp_val_c),\n",
    "            \"Brier_precursor\": float(brier_score_loss(Lp_val, Pp_val_c)),\n",
    "            \"AUROC_fault\": safe_auc(Lf_val, Pf_val_c),\n",
    "            \"AUPRC_fault\": safe_auprc(Lf_val, Pf_val_c),\n",
    "            \"Brier_fault\": float(brier_score_loss(Lf_val, Pf_val_c)),\n",
    "        },\n",
    "        \"test\": {\n",
    "            \"AUROC_precursor\": safe_auc(Lp_tst, Pp_tst_c),\n",
    "            \"AUPRC_precursor\": safe_auprc(Lp_tst, Pp_tst_c),\n",
    "            \"Brier_precursor\": float(brier_score_loss(Lp_tst, Pp_tst_c)),\n",
    "            \"AUROC_fault\": safe_auc(Lf_tst, Pf_tst_c),\n",
    "            \"AUPRC_fault\": safe_auprc(Lf_tst, Pf_tst_c),\n",
    "            \"Brier_fault\": float(brier_score_loss(Lf_tst, Pf_tst_c)),\n",
    "        }\n",
    "    }\n",
    "\n",
    "    # --- Threshold tuning (per-head) on VALID calibrated probs ---\n",
    "    def tune_thresholds(y: np.ndarray, p: np.ndarray, cadence_s: float, head: str):\n",
    "        dp = np.diff(p, prepend=p[:1]) / max(cadence_s, 1e-6)\n",
    "        T_on_grid = np.linspace(0.5, 0.9, 9)\n",
    "        T_off_delta = 0.10\n",
    "        dPdt_grid = np.linspace(0.05, 0.20, 4)  # per second\n",
    "        min_consec_grid = [1, 3, 5]\n",
    "        best = None\n",
    "        best_score = -1e9\n",
    "\n",
    "        for T_on in T_on_grid:\n",
    "            T_off = max(0.0, T_on - T_off_delta)\n",
    "            for dthr in dPdt_grid:\n",
    "                for mcons in min_consec_grid:\n",
    "                    on = False\n",
    "                    preds = np.zeros_like(p, dtype=np.uint8)\n",
    "                    consec = 0\n",
    "                    for i in range(len(p)):\n",
    "                        trigger = (p[i] >= T_on) or (p[i] >= max(0.4, T_on - 0.1) and dp[i] >= dthr)\n",
    "                        if on:\n",
    "                            if p[i] <= T_off:\n",
    "                                consec = 0\n",
    "                                on = False\n",
    "                        else:\n",
    "                            if trigger:\n",
    "                                consec += 1\n",
    "                                if consec >= mcons:\n",
    "                                    on = True\n",
    "                                    consec = 0\n",
    "                        preds[i] = 1 if on else 0\n",
    "                    tp = int(((preds == 1) & (y == 1)).sum())\n",
    "                    fp = int(((preds == 1) & (y == 0)).sum())\n",
    "                    fn = int(((preds == 0) & (y == 1)).sum())\n",
    "                    precision = tp / (tp + fp + 1e-9)\n",
    "                    recall = tp / (tp + fn + 1e-9)\n",
    "                    f1 = 2 * precision * recall / (precision + recall + 1e-9)\n",
    "                    fpr = fp / max(1, int((y == 0).sum()))\n",
    "                    score = f1 - 0.10 * fpr\n",
    "                    if score > best_score:\n",
    "                        best_score = score\n",
    "                        best = {\n",
    "                            \"rate_threshold\": float(dthr),\n",
    "                            \"min_consec_on\": int(mcons),\n",
    "                            \"score\": float(score),\n",
    "                            \"T_on\": float(T_on),\n",
    "                            \"T_off\": float(T_off),\n",
    "                        }\n",
    "        return best\n",
    "\n",
    "    thr_prec = tune_thresholds(Lp_val, Pp_val_c, cadence, head=\"precursor\")\n",
    "    thr_fault = tune_thresholds(Lf_val, Pf_val_c, cadence, head=\"fault\")\n",
    "\n",
    "    thresholds = {\n",
    "        \"fault_on\":  thr_fault[\"T_on\"],\n",
    "        \"fault_off\": thr_fault[\"T_off\"],\n",
    "        \"prec_on\":   thr_prec[\"T_on\"],\n",
    "        \"prec_off\":  thr_prec[\"T_off\"],\n",
    "        \"dPdt_fault\": thr_fault[\"rate_threshold\"],\n",
    "        \"dPdt_prec\":  thr_prec[\"rate_threshold\"],\n",
    "        \"min_consec_on_fault\": thr_fault[\"min_consec_on\"],\n",
    "        \"min_consec_on_prec\":  thr_prec[\"min_consec_on\"],\n",
    "        \"cadence_seconds\": cadence,\n",
    "        \"window_length\": int(win),\n",
    "        \"stride\": int(stride),\n",
    "    }\n",
    "\n",
    "    # --- Artifacts directory per DTC ---\n",
    "    art_dir = ARTIFACTS_ROOT / dtc_code\n",
    "    art_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Save scaler\n",
    "    joblib.dump(scaler, (art_dir / f\"scaler_{dtc_code}.pkl\").as_posix())\n",
    "\n",
    "    # Save calibration models\n",
    "    joblib.dump(iso_prec, (art_dir / f\"calib_{dtc_code}_precursor.pkl\").as_posix())\n",
    "    joblib.dump(iso_fault, (art_dir / f\"calib_{dtc_code}_fault.pkl\").as_posix())\n",
    "\n",
    "    # Save thresholds & feature spec\n",
    "    feature_spec = {\n",
    "        \"dtc_code\": dtc_code,\n",
    "        \"features\": features,\n",
    "        \"window_length\": int(win),\n",
    "        \"stride\": int(stride),\n",
    "        \"cadence_seconds\": cadence,\n",
    "        \"expects_masks\": False,\n",
    "    }\n",
    "    (art_dir / \"feature_spec.json\").write_text(json.dumps(feature_spec, indent=2), encoding=\"utf-8\")\n",
    "    (art_dir / \"thresholds.json\").write_text(json.dumps(thresholds, indent=2), encoding=\"utf-8\")\n",
    "\n",
    "    # Save metrics + plots\n",
    "    (art_dir / \"metrics.json\").write_text(json.dumps(metrics, indent=2), encoding=\"utf-8\")\n",
    "    plot_loss_curves(train_hist, valid_hist, art_dir / \"loss_curves.png\")\n",
    "    plot_reliability(Lf_val, Pf_val_c, f\"{dtc_code} fault reliability (valid)\", art_dir / \"calibration_fault_valid.png\")\n",
    "    plot_reliability(Lp_val, Pp_val_c, f\"{dtc_code} precursor reliability (valid)\", art_dir / \"calibration_precursor_valid.png\")\n",
    "\n",
    "    # TorchScript export (model only; calibration & thresholds applied in runtime)\n",
    "    scripted = torch.jit.script(model.cpu())\n",
    "    scripted.save((art_dir / f\"model_{dtc_code}.ts\").as_posix())\n",
    "    model.to(DEVICE)  # optional: move back if continuing\n",
    "\n",
    "    # Return brief summary for registry\n",
    "    return {\n",
    "        \"dtc_code\": dtc_code,\n",
    "        \"artifacts_dir\": art_dir.as_posix(),\n",
    "        \"model\": f\"model_{dtc_code}.ts\",\n",
    "        \"scaler\": f\"scaler_{dtc_code}.pkl\",\n",
    "        \"calibrators\": {\n",
    "            \"precursor\": f\"calib_{dtc_code}_precursor.pkl\",\n",
    "            \"fault\": f\"calib_{dtc_code}_fault.pkl\",\n",
    "        },\n",
    "        \"thresholds\": \"thresholds.json\",\n",
    "        \"feature_spec\": \"feature_spec.json\",\n",
    "        \"metrics\": metrics,\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "da83a28b-cf6f-47af-bee3-3877f5e6e3c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found [WindowsPath('C:/engine_module_pipeline/DTC_stage/data/synthetic/dtc_P0234'), WindowsPath('C:/engine_module_pipeline/DTC_stage/data/synthetic/dtc_P0300'), WindowsPath('C:/engine_module_pipeline/DTC_stage/data/synthetic/dtc_P0420'), WindowsPath('C:/engine_module_pipeline/DTC_stage/data/synthetic/dtc_P0501'), WindowsPath('C:/engine_module_pipeline/DTC_stage/data/synthetic/dtc_P0562')] DTC folders.\n"
     ]
    }
   ],
   "source": [
    "def find_all_dtc_dirs(data_root: Path) -> List[Path]:\n",
    "    return sorted([p for p in data_root.glob(\"dtc_*\") if p.is_dir()])\n",
    "\n",
    "registry = {\"artifacts_root\": ARTIFACTS_ROOT.as_posix(), \"dtcs\": {}}\n",
    "dtc_dirs = find_all_dtc_dirs(DATA_ROOT)\n",
    "dtc_dirs = dtc_dirs[5:]\n",
    "print(f\"Found {dtc_dirs} DTC folders.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12fa473d-f951-40ab-8061-2765b2d277fa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Training P0234 ===\n",
      "[P0234] epoch 01 | train=1.2309 valid=0.9005 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 02 | train=0.6029 valid=1.2268 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 03 | train=0.4128 valid=1.7350 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 04 | train=0.3044 valid=1.9057 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 05 | train=0.2436 valid=1.8055 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 06 | train=0.1714 valid=1.9906 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 07 | train=0.1523 valid=2.0057 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 08 | train=0.1412 valid=2.0291 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 09 | train=0.1309 valid=2.1089 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 10 | train=0.1100 valid=2.4233 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 11 | train=0.1062 valid=2.4968 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 12 | train=0.1020 valid=2.4673 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 13 | train=0.1000 valid=2.4309 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 14 | train=0.0913 valid=2.6194 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 15 | train=0.0887 valid=2.7304 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 16 | train=0.0874 valid=2.5941 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 17 | train=0.0860 valid=2.6425 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 18 | train=0.0827 valid=2.8480 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 19 | train=0.0809 valid=2.6496 | pw(fault)=3.69, pw(prec)=6.30\n",
      "[P0234] epoch 20 | train=0.0798 valid=2.7686 | pw(fault)=3.69, pw(prec)=6.30\n",
      "\n",
      "=== Training P0300 ===\n",
      "[P0300] epoch 01 | train=1.2863 valid=1.2678 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 02 | train=0.6617 valid=1.5871 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 03 | train=0.4348 valid=1.5930 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 04 | train=0.3270 valid=1.5578 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 05 | train=0.2559 valid=0.9769 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 06 | train=0.2199 valid=1.1896 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 07 | train=0.1853 valid=1.1099 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 08 | train=0.1619 valid=0.9441 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 09 | train=0.1424 valid=0.7966 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 10 | train=0.1300 valid=0.8127 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 11 | train=0.1195 valid=0.9685 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 12 | train=0.1136 valid=0.8311 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 13 | train=0.1078 valid=0.9429 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 14 | train=0.0782 valid=0.9689 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 15 | train=0.0735 valid=0.8065 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 16 | train=0.0716 valid=0.7550 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 17 | train=0.0685 valid=0.8152 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 18 | train=0.0658 valid=0.8825 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 19 | train=0.0641 valid=0.9857 | pw(fault)=5.29, pw(prec)=4.12\n",
      "[P0300] epoch 20 | train=0.0637 valid=0.8359 | pw(fault)=5.29, pw(prec)=4.12\n",
      "\n",
      "=== Training P0420 ===\n",
      "[P0420] epoch 01 | train=1.4031 valid=1.2823 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 02 | train=0.9258 valid=1.4805 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 03 | train=0.6646 valid=1.8790 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 04 | train=0.4925 valid=2.3169 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 05 | train=0.3958 valid=2.6818 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 06 | train=0.2972 valid=2.8049 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 07 | train=0.2624 valid=2.9803 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 08 | train=0.2353 valid=3.1106 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 09 | train=0.2150 valid=3.2996 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 10 | train=0.1869 valid=3.3435 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 11 | train=0.1773 valid=3.4541 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 12 | train=0.1704 valid=3.4611 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 13 | train=0.1638 valid=3.5678 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 14 | train=0.1521 valid=3.6940 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 15 | train=0.1484 valid=3.6403 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 16 | train=0.1463 valid=3.7105 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 17 | train=0.1435 valid=3.8390 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 18 | train=0.1383 valid=3.8064 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 19 | train=0.1370 valid=3.8070 | pw(fault)=2.91, pw(prec)=2.93\n",
      "[P0420] epoch 20 | train=0.1351 valid=3.8274 | pw(fault)=2.91, pw(prec)=2.93\n",
      "\n",
      "=== Training P0501 ===\n",
      "[P0501] epoch 01 | train=1.3335 valid=1.2268 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 02 | train=0.8969 valid=1.4992 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 03 | train=0.6971 valid=1.6828 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 04 | train=0.5764 valid=2.1442 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 05 | train=0.4893 valid=2.4653 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 06 | train=0.4056 valid=2.5140 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 07 | train=0.3771 valid=2.7305 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 08 | train=0.3549 valid=2.8905 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 09 | train=0.3320 valid=2.7937 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 10 | train=0.3021 valid=3.2041 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 11 | train=0.2918 valid=3.1182 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 12 | train=0.2815 valid=3.1693 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 13 | train=0.2750 valid=3.2221 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 14 | train=0.2602 valid=3.3819 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 15 | train=0.2552 valid=3.2622 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 16 | train=0.2517 valid=3.3719 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 17 | train=0.2477 valid=3.4402 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 18 | train=0.2400 valid=3.4583 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 19 | train=0.2380 valid=3.5469 | pw(fault)=2.47, pw(prec)=2.90\n",
      "[P0501] epoch 20 | train=0.2357 valid=3.5439 | pw(fault)=2.47, pw(prec)=2.90\n",
      "\n",
      "=== Training P0562 ===\n",
      "[P0562] epoch 01 | train=0.9326 valid=0.6799 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 02 | train=0.5436 valid=0.7621 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 03 | train=0.4276 valid=0.9261 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 04 | train=0.3561 valid=1.0315 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 05 | train=0.3027 valid=0.9102 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 06 | train=0.2489 valid=1.3712 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 07 | train=0.2314 valid=1.5310 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 08 | train=0.2139 valid=1.4619 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 09 | train=0.2035 valid=1.5193 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 11 | train=0.1741 valid=1.6942 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 12 | train=0.1662 valid=1.5737 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 13 | train=0.1601 valid=1.6017 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 14 | train=0.1492 valid=1.7762 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 15 | train=0.1461 valid=1.7116 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 16 | train=0.1443 valid=1.7307 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 17 | train=0.1416 valid=1.7387 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 18 | train=0.1362 valid=1.7851 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 19 | train=0.1365 valid=1.7447 | pw(fault)=2.30, pw(prec)=3.49\n",
      "[P0562] epoch 20 | train=0.1333 valid=1.8260 | pw(fault)=2.30, pw(prec)=3.49\n",
      "\n",
      "✅ Training complete. Global registry at: C:/engine_module_pipeline/DTC_stage/artifacts/registry.json\n"
     ]
    }
   ],
   "source": [
    "for dtc_dir in dtc_dirs:\n",
    "    meta = read_json(dtc_dir / \"meta.json\")\n",
    "    dtc_code = meta[\"dtc_code\"]\n",
    "    print(f\"\\n=== Training {dtc_code} ===\")\n",
    "    summary = train_one_dtc(dtc_dir)\n",
    "    registry[\"dtcs\"][dtc_code] = {\n",
    "        \"path\": summary[\"artifacts_dir\"],\n",
    "        \"model\": summary[\"model\"],\n",
    "        \"scaler\": summary[\"scaler\"],\n",
    "        \"calibrators\": summary[\"calibrators\"],\n",
    "        \"thresholds\": summary[\"thresholds\"],\n",
    "        \"feature_spec\": summary[\"feature_spec\"],\n",
    "        \"metrics\": summary[\"metrics\"][\"test\"],  # stash test metrics for quick view\n",
    "    }\n",
    "    gc.collect()\n",
    "\n",
    "(ARTIFACTS_ROOT / \"registry.json\").write_text(json.dumps(registry, indent=2), encoding=\"utf-8\")\n",
    "print(\"\\n✅ Training complete. Global registry at:\", (ARTIFACTS_ROOT / \"registry.json\").as_posix())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9f838fb-08ae-4545-a324-deee8eca015c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark (Py3.11 + Spark 3.5)",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
