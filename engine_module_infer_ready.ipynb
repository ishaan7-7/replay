{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "6b85676d-a978-41bb-a788-332528744054",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python: 3.11.9\n",
      "deltalake: 1.1.4\n",
      "pyarrow: 16.1.0\n",
      "Paths:\n",
      "  SRC _delta_log: True C:\\engine_module_pipeline\\delta\\engine_module_delta\\_delta_log\n",
      "  TARGET (Delta): C:\\engine_module_pipeline\\delta\\engine_module_infer_ready\n",
      "  CHECKPOINT: C:\\engine_module_pipeline\\data\\checkpoints\\infer_ready_shared_checkpoint.json\n",
      "  LOG_STATE: C:\\engine_module_pipeline\\data\\checkpoints\\infer_ready_log_state.json\n",
      "  DLQ: C:\\engine_module_pipeline\\data\\dlq\\infer_ready\n"
     ]
    }
   ],
   "source": [
    "import sys, os, json, time\n",
    "from pathlib import Path\n",
    "\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "from deltalake import DeltaTable, write_deltalake\n",
    "\n",
    "ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "DELTA_SRC   = ROOT / r\"delta\\engine_module_delta\"          # source Delta (with _delta_log)\n",
    "INFER_READY = ROOT / r\"delta\\engine_module_infer_ready\"    # TARGET (Delta)\n",
    "CONFIG      = ROOT / \"config\"\n",
    "ARTIFACTS   = ROOT / \"engine_module_artifacts\"\n",
    "\n",
    "CHECKPOINT_DIR  = ROOT / r\"data\\checkpoints\"\n",
    "CHECKPOINT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "CHECKPOINT_PATH = CHECKPOINT_DIR / \"infer_ready_shared_checkpoint.json\"\n",
    "LOCK_PATH       = CHECKPOINT_DIR / \"infer_ready_shared.lock\"\n",
    "LOG_STATE_PATH  = CHECKPOINT_DIR / \"infer_ready_log_state.json\"   # incremental _delta_log tracker\n",
    "DLQ_DIR         = ROOT / r\"data\\dlq\\infer_ready\"\n",
    "DLQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "print(\"Python:\", sys.version.split()[0])\n",
    "print(\"deltalake:\", __import__(\"deltalake\").__version__)\n",
    "print(\"pyarrow:\", pa.__version__)\n",
    "print(\"Paths:\")\n",
    "print(\"  SRC _delta_log:\", (DELTA_SRC / \"_delta_log\").exists(), DELTA_SRC / \"_delta_log\")\n",
    "print(\"  TARGET (Delta):\", INFER_READY)\n",
    "print(\"  CHECKPOINT:\", CHECKPOINT_PATH)\n",
    "print(\"  LOG_STATE:\", LOG_STATE_PATH)\n",
    "print(\"  DLQ:\", DLQ_DIR)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ad1054bf-4e52-4589-bcbe-fce4c325cf40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created empty Delta table at: C:\\engine_module_pipeline\\delta\\engine_module_infer_ready\n",
      "Delta table ready at: C:\\engine_module_pipeline\\delta\\engine_module_infer_ready\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import json\n",
    "import pyarrow as pa\n",
    "from deltalake import write_deltalake\n",
    "\n",
    "FEATURES_JSON = ARTIFACTS / \"features.json\"\n",
    "MAP_MERGED    = CONFIG / \"mapping_merged.json\"\n",
    "\n",
    "if not FEATURES_JSON.exists():\n",
    "    raise FileNotFoundError(f\"Missing {FEATURES_JSON}\")\n",
    "if not MAP_MERGED.exists():\n",
    "    raise FileNotFoundError(f\"Missing {MAP_MERGED}\")\n",
    "\n",
    "\n",
    "features = json.loads(FEATURES_JSON.read_text(encoding=\"utf-8\"))\n",
    "if isinstance(features, dict) and \"features\" in features:\n",
    "    features = features[\"features\"]\n",
    "assert isinstance(features, list) and len(features) == 25, \"features.json must list exactly 25 canonical features\"\n",
    "\n",
    "\n",
    "mapping = json.loads(MAP_MERGED.read_text(encoding=\"utf-8\"))\n",
    "canonical_to_raw = {f: mapping[f][\"raw_key\"] for f in features}\n",
    "\n",
    "def infer_ready_arrow_schema():\n",
    "    \"\"\"Arrow schema for infer-ready: timestamp(us), date=date32, feature float64.\"\"\"\n",
    "    fields = [\n",
    "        pa.field(\"row_hash\",    pa.string(),           nullable=False),\n",
    "        pa.field(\"timestamp\",   pa.timestamp(\"us\"),    nullable=True),\n",
    "        pa.field(\"source_id\",   pa.string(),           nullable=True),\n",
    "        pa.field(\"kafka_key\",   pa.string(),           nullable=True),\n",
    "        pa.field(\"offset\",      pa.int64(),            nullable=True),\n",
    "        pa.field(\"source_file\", pa.string(),           nullable=True),\n",
    "        pa.field(\"date\",        pa.date32(),           nullable=False),  # true DATE (NOT NULL)\n",
    "    ]\n",
    "    for f in features:\n",
    "        fields.append(pa.field(f, pa.float64(), nullable=True))\n",
    "    return pa.schema(fields)\n",
    "\n",
    "SCHEMA = infer_ready_arrow_schema()\n",
    "\n",
    "def ensure_infer_ready_delta_exists():\n",
    "    if (INFER_READY / \"_delta_log\").exists():\n",
    "        return\n",
    "    empty_tbl = pa.Table.from_arrays([pa.array([], type=field.type) for field in SCHEMA], schema=SCHEMA)\n",
    "    write_deltalake(\n",
    "        str(INFER_READY),\n",
    "        empty_tbl,\n",
    "        mode=\"overwrite\",\n",
    "        partition_by=[\"date\"],\n",
    "    )\n",
    "    print(\"Created empty Delta table at:\", INFER_READY)\n",
    "\n",
    "ensure_infer_ready_delta_exists()\n",
    "print(\"Delta table ready at:\", INFER_READY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ec8067e-b763-41c0-96cf-da7caf3b284e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, json\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "def read_checkpoint():\n",
    "    if not CHECKPOINT_PATH.exists():\n",
    "        return {\"processed_files\": [], \"last_updated\": None}\n",
    "    try:\n",
    "        return json.loads(CHECKPOINT_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"processed_files\": [], \"last_updated\": None}\n",
    "\n",
    "def write_checkpoint_atomic(data: dict):\n",
    "    tmp = CHECKPOINT_PATH.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(data, indent=2), encoding=\"utf-8\")\n",
    "    os.replace(str(tmp), str(CHECKPOINT_PATH))\n",
    "\n",
    "def read_log_state():\n",
    "    if not LOG_STATE_PATH.exists():\n",
    "        return {\"last_log_file\": None}\n",
    "    try:\n",
    "        return json.loads(LOG_STATE_PATH.read_text(encoding=\"utf-8\"))\n",
    "    except Exception:\n",
    "        return {\"last_log_file\": None}\n",
    "\n",
    "def write_log_state(state: dict):\n",
    "    tmp = LOG_STATE_PATH.with_suffix(\".tmp\")\n",
    "    tmp.write_text(json.dumps(state, indent=2), encoding=\"utf-8\")\n",
    "    os.replace(str(tmp), str(LOG_STATE_PATH))\n",
    "\n",
    "def acquire_lock(timeout: int = 30) -> bool:\n",
    "    start = time.time()\n",
    "    while True:\n",
    "        try:\n",
    "            fd = os.open(str(LOCK_PATH), os.O_CREAT | os.O_EXCL | os.O_WRONLY)\n",
    "            os.write(fd, str(os.getpid()).encode(\"utf-8\"))\n",
    "            os.close(fd)\n",
    "            return True\n",
    "        except FileExistsError:\n",
    "            try:\n",
    "                # stale lock cleanup (older than 1h)\n",
    "                if time.time() - LOCK_PATH.stat().st_mtime > 3600:\n",
    "                    LOCK_PATH.unlink(); continue\n",
    "            except Exception:\n",
    "                pass\n",
    "            if time.time() - start > timeout:\n",
    "                return False\n",
    "            time.sleep(0.5)\n",
    "\n",
    "def refresh_lock_heartbeat():\n",
    "    try:\n",
    "        if LOCK_PATH.exists():\n",
    "            os.utime(LOCK_PATH, None)\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def release_lock():\n",
    "    try:\n",
    "        if LOCK_PATH.exists():\n",
    "            LOCK_PATH.unlink()\n",
    "    except Exception:\n",
    "        pass\n",
    "\n",
    "def dlq_write(rows: list, prefix: str):\n",
    "    if not rows:\n",
    "        return\n",
    "    DLQ_DIR.mkdir(parents=True, exist_ok=True)\n",
    "    ts = time.strftime(\"%Y%m%d_%H%M%S\")\n",
    "    fn = DLQ_DIR / f\"{prefix}_{ts}.jsonl\"\n",
    "    with fn.open(\"w\", encoding=\"utf-8\") as fh:\n",
    "        for r in rows:\n",
    "            fh.write(json.dumps(r) + \"\\n\")\n",
    "    print(\"DLQ written:\", fn.name, \"count:\", len(rows))\n",
    "\n",
    "def existing_row_hashes_from_delta(limit=None) -> set:\n",
    "    \"\"\"Read distinct row_hash from Delta table (columns=['row_hash']). For scale, consider a key-index later.\"\"\"\n",
    "    if not (INFER_READY / \"_delta_log\").exists():\n",
    "        return set()\n",
    "    dt = DeltaTable(str(INFER_READY))\n",
    "    tbl = dt.to_pyarrow_table(columns=[\"row_hash\"])\n",
    "    vals = tbl.column(\"row_hash\").to_pylist()\n",
    "    if limit is not None:\n",
    "        vals = vals[:limit]\n",
    "    return set(vals)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "44953cce-a69b-4fa0-9045-75f2ee3e65f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "LOG_DIR = DELTA_SRC / \"_delta_log\"\n",
    "if not LOG_DIR.exists():\n",
    "    raise FileNotFoundError(f\"Missing _delta_log at {LOG_DIR}\")\n",
    "\n",
    "def _list_json_logs():\n",
    "    return sorted([p for p in LOG_DIR.iterdir() if p.is_file() and p.suffix == \".json\" and not p.name.startswith(\".\")],\n",
    "                  key=lambda p: p.name)\n",
    "\n",
    "def discover_add_file_paths_incremental(last_log_file: str | None):\n",
    "    logs = _list_json_logs()\n",
    "    if last_log_file:\n",
    "        logs = [p for p in logs if p.name > last_log_file]\n",
    "    add_paths = []\n",
    "    latest = last_log_file\n",
    "    for jf in logs:\n",
    "        latest = jf.name\n",
    "        for line in jf.read_text(encoding=\"utf-8\").splitlines():\n",
    "            if not line.strip():\n",
    "                continue\n",
    "            try:\n",
    "                obj = json.loads(line)\n",
    "            except Exception:\n",
    "                continue\n",
    "            if isinstance(obj, dict) and \"add\" in obj and obj[\"add\"].get(\"path\"):\n",
    "                add_paths.append(DELTA_SRC / obj[\"add\"][\"path\"])\n",
    "    # de-dupe keep order\n",
    "    seen=set(); out=[]\n",
    "    for p in add_paths:\n",
    "        if p not in seen and p.exists():\n",
    "            seen.add(p); out.append(p)\n",
    "    return latest, out\n",
    "\n",
    "def discover_add_file_paths_full():\n",
    "    # Backfill convenience: scan the entire _delta_log\n",
    "    return discover_add_file_paths_incremental(last_log_file=None)[1]\n",
    "\n",
    "def _to_float_or_none(v):\n",
    "    if v is None or v == \"\":\n",
    "        return None\n",
    "    try:\n",
    "        return float(v)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return float(str(v).replace(\",\", \"\"))\n",
    "        except Exception:\n",
    "            return None\n",
    "\n",
    "def to_canonical_rows(parquet_file: Path) -> tuple[list, list]:\n",
    "    \"\"\"\n",
    "    Parse a source parquet into (ok_rows, dlq_rows).\n",
    "    Expect columns: row_hash (string), timestamp, payload (JSON/bytes), kafka_key, offset, ...\n",
    "    \"\"\"\n",
    "    ok_rows, dlq_rows = [], []\n",
    "    try:\n",
    "        pf = pq.ParquetFile(str(parquet_file))\n",
    "    except Exception as e:\n",
    "        dlq_rows.append({\"source_file\": str(parquet_file), \"reason\": f\"parquet_open_failed:{e!s}\"})\n",
    "        return ok_rows, dlq_rows\n",
    "\n",
    "    for rg in range(pf.num_row_groups):\n",
    "        table = pf.read_row_group(rg)\n",
    "        cols = {name: table.column(name).to_pylist() for name in table.column_names}\n",
    "        n = len(next(iter(cols.values()))) if cols else 0\n",
    "\n",
    "        for i in range(n):\n",
    "            row_hash = (cols.get(\"row_hash\") or [None])[i]\n",
    "            ts_raw   = (cols.get(\"timestamp\") or [None])[i]\n",
    "            kafka_key = (cols.get(\"kafka_key\") or [None])[i]\n",
    "            offset    = (cols.get(\"offset\") or [None])[i]\n",
    "            payload   = (cols.get(\"payload\") or [None])[i]\n",
    "\n",
    "            if row_hash is None:\n",
    "                dlq_rows.append({\"reason\":\"missing_row_hash\", \"source_file\": str(parquet_file)}); continue\n",
    "\n",
    "            # Parse payload to dict\n",
    "            parsed = {}\n",
    "            if payload is not None:\n",
    "                try:\n",
    "                    parsed = json.loads(payload) if isinstance(payload, str) else json.loads(payload.decode(\"utf-8\", errors=\"ignore\"))\n",
    "                except Exception:\n",
    "                    dlq_rows.append({\"row_hash\": str(row_hash), \"reason\":\"payload_parse_failed\", \"source_file\": str(parquet_file)})\n",
    "                    continue\n",
    "            data = parsed.get(\"data\", parsed) if isinstance(parsed, dict) else {}\n",
    "            meta = parsed.get(\"meta\", {}) if isinstance(parsed, dict) else {}\n",
    "\n",
    "            # Build canonical row (raw types; finalize later)\n",
    "            out = {\n",
    "                \"row_hash\":    str(row_hash),\n",
    "                \"timestamp\":   pd.to_datetime(ts_raw, errors=\"coerce\", utc=True),\n",
    "                \"source_id\":   meta.get(\"source_id\") or kafka_key,\n",
    "                \"kafka_key\":   kafka_key if kafka_key is not None else None,\n",
    "                \"offset\":      None,\n",
    "                \"source_file\": str(parquet_file),\n",
    "            }\n",
    "            # robust offset parse (allow negatives / strings)\n",
    "            try:\n",
    "                out[\"offset\"] = int(offset) if offset is not None else None\n",
    "            except Exception:\n",
    "                out[\"offset\"] = None\n",
    "\n",
    "            missing = 0\n",
    "            for f in features:\n",
    "                raw_key = canonical_to_raw.get(f)\n",
    "                v = data.get(raw_key) if raw_key is not None else None\n",
    "                fv = _to_float_or_none(v)\n",
    "                if fv is None: missing += 1\n",
    "                out[f] = fv\n",
    "\n",
    "            # Quality gate (feature completeness first)\n",
    "            if len(features) and (missing / len(features)) > 0.30:\n",
    "                dlq_rows.append({\"row_hash\": out[\"row_hash\"], \"reason\":\"too_many_missing\",\n",
    "                                 \"null_frac\": round(missing/len(features), 3),\n",
    "                                 \"source_file\": str(parquet_file)})\n",
    "            elif out[\"timestamp\"] is None:\n",
    "                dlq_rows.append({\"row_hash\": out[\"row_hash\"], \"reason\":\"null_or_bad_timestamp\",\n",
    "                                 \"source_file\": str(parquet_file)})\n",
    "            else:\n",
    "                ok_rows.append(out)\n",
    "\n",
    "    # finalize types for OK rows + intra-file dedupe by row_hash\n",
    "    if ok_rows:\n",
    "        df = pd.DataFrame(ok_rows)\n",
    "        before = len(df)\n",
    "        df = df.drop_duplicates(subset=[\"row_hash\"], keep=\"first\")\n",
    "        # timestamps -> microseconds (naive UTC)\n",
    "        df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True, errors=\"coerce\").dt.tz_convert(None).astype(\"datetime64[us]\")\n",
    "        # date partition from timestamp\n",
    "        df[\"date\"] = pd.to_datetime(df[\"timestamp\"], errors=\"coerce\").dt.date.astype(\"object\")  # Arrow will coerce to date32\n",
    "        # drop any rows whose date could not be derived (should be none after timestamp check)\n",
    "        df = df.dropna(subset=[\"date\"])\n",
    "        ok_rows = df.to_dict(orient=\"records\")\n",
    "\n",
    "    return ok_rows, dlq_rows\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "c81ba2fd-c806-4c00-9fe2-3e07f1890a2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Backfill candidates: 4\n",
      "Existing infer-ready row_hash keys: 0\n",
      "Backfill done. Appended 2000 rows to Delta. Checkpoint updated with 4 files.\n"
     ]
    }
   ],
   "source": [
    "def delta_append_rows(rows: list) -> int:\n",
    "    \"\"\"Append rows to Delta using the Arrow schema (enforces date32/timestamp(us)/float64).\"\"\"\n",
    "    if not rows:\n",
    "        return 0\n",
    "    df = pd.DataFrame(rows)\n",
    "\n",
    "    # Guarantee all feature columns exist\n",
    "    for f in features:\n",
    "        if f not in df.columns:\n",
    "            df[f] = None\n",
    "\n",
    "    # Hard guard: timestamp / date must be non-null\n",
    "    df = df.dropna(subset=[\"timestamp\"])\n",
    "    df = df.dropna(subset=[\"date\"])\n",
    "    if df.empty:\n",
    "        return 0\n",
    "\n",
    "    # Column order strictly follows SCHEMA\n",
    "    df = df[[f.name for f in SCHEMA]]\n",
    "\n",
    "    # Convert to Arrow using our schema (forces date32 & timestamp(us))\n",
    "    table = pa.Table.from_pandas(df, schema=SCHEMA, preserve_index=False)\n",
    "    write_deltalake(\n",
    "        str(INFER_READY),\n",
    "        table,\n",
    "        mode=\"append\",\n",
    "        partition_by=[\"date\"]\n",
    "    )\n",
    "    return len(df)\n",
    "\n",
    "# ===== RUN BACKFILL (idempotent, lock-protected) =====\n",
    "all_add_files = discover_add_file_paths_full()\n",
    "ck = read_checkpoint()\n",
    "already = set(ck.get(\"processed_files\", []))\n",
    "to_process = [p for p in all_add_files if p.name not in already]\n",
    "print(\"Backfill candidates:\", len(to_process))\n",
    "\n",
    "if to_process:\n",
    "    if not acquire_lock(timeout=30):\n",
    "        raise RuntimeError(\"Could not acquire lock; another writer is active.\")\n",
    "    try:\n",
    "        existing_keys = existing_row_hashes_from_delta()\n",
    "        print(\"Existing infer-ready row_hash keys:\", len(existing_keys))\n",
    "        newly = []\n",
    "        total_appended = 0\n",
    "        for idx, pf in enumerate(to_process, 1):\n",
    "            ok, dlq = to_canonical_rows(pf)\n",
    "            # de-dupe vs Delta table (set membership)\n",
    "            ok = [r for r in ok if r[\"row_hash\"] not in existing_keys]\n",
    "            appended = delta_append_rows(ok)\n",
    "            total_appended += appended\n",
    "            if appended:\n",
    "                existing_keys.update([r[\"row_hash\"] for r in ok])\n",
    "            if dlq:\n",
    "                dlq_write(dlq, f\"dlq_backfill_{pf.stem}\")\n",
    "            newly.append(pf.name)\n",
    "            if idx % 20 == 0:\n",
    "                refresh_lock_heartbeat()\n",
    "        # checkpoint\n",
    "        new_ck = {\"processed_files\": sorted(already.union(newly)), \"last_updated\": int(time.time())}\n",
    "        write_checkpoint_atomic(new_ck)\n",
    "        print(f\"Backfill done. Appended {total_appended} rows to Delta. Checkpoint updated with {len(newly)} files.\")\n",
    "    finally:\n",
    "        release_lock()\n",
    "else:\n",
    "    print(\"Nothing to backfill. Skipping.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4feedeac-024d-4992-bbc2-b2819ed36540",
   "metadata": {},
   "outputs": [],
   "source": [
    "POLL_SECS = 8\n",
    "REFRESH_KEYS_EVERY = 60\n",
    "\n",
    "print(\"Starting watcher… CTRL+C to stop.\")\n",
    "if not acquire_lock(timeout=30):\n",
    "    raise RuntimeError(\"Could not acquire lock; another writer is active.\")\n",
    "try:\n",
    "    ck = read_checkpoint()\n",
    "    processed_files = set(ck.get(\"processed_files\", []))\n",
    "    existing_keys = existing_row_hashes_from_delta()\n",
    "    log_state = read_log_state()\n",
    "    last_log = log_state.get(\"last_log_file\")\n",
    "\n",
    "    print(\"Watcher bootstrap:\", len(processed_files), \"processed files;\", len(existing_keys), \"existing keys.\")\n",
    "    last_refresh = time.time()\n",
    "\n",
    "    while True:\n",
    "        last_log, candidates = discover_add_file_paths_incremental(last_log)\n",
    "        candidates = [p for p in candidates if p.name not in processed_files and p.exists()]\n",
    "        if candidates:\n",
    "            total_app = 0\n",
    "            newly = []\n",
    "            for pf in candidates:\n",
    "                ok, dlq = to_canonical_rows(pf)\n",
    "                ok = [r for r in ok if r[\"row_hash\"] not in existing_keys]\n",
    "                appended = delta_append_rows(ok)\n",
    "                total_app += appended\n",
    "                if appended:\n",
    "                    existing_keys.update([r[\"row_hash\"] for r in ok])\n",
    "                if dlq:\n",
    "                    dlq_write(dlq, f\"dlq_watch_{pf.stem}\")\n",
    "                newly.append(pf.name)\n",
    "            processed_files.update(newly)\n",
    "            write_checkpoint_atomic({\"processed_files\": sorted(processed_files), \"last_updated\": int(time.time())})\n",
    "            write_log_state({\"last_log_file\": last_log})\n",
    "            print(f\"Watcher: processed {len(newly)} files; appended {total_app} rows.\")\n",
    "        if time.time() - last_refresh > REFRESH_KEYS_EVERY:\n",
    "            existing_keys = existing_row_hashes_from_delta()\n",
    "            last_refresh = time.time()\n",
    "        refresh_lock_heartbeat()\n",
    "        time.sleep(POLL_SECS)\n",
    "except KeyboardInterrupt:\n",
    "    print(\"Watcher interrupted by user.\")\n",
    "finally:\n",
    "    release_lock()\n",
    "    print(\"Watcher stopped; lock released.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "46aa96d4-0ad8-4f4d-b24b-15e670f2e226",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Infer-ready row count: 2,000\n",
      "date field: date32[day]\n",
      "timestamp field: timestamp[us]\n"
     ]
    }
   ],
   "source": [
    "from deltalake import DeltaTable\n",
    "import pyarrow as pa\n",
    "\n",
    "if not (INFER_READY / \"_delta_log\").exists():\n",
    "    print(\"Infer-ready Delta not initialized.\")\n",
    "else:\n",
    "    dt = DeltaTable(str(INFER_READY))\n",
    "\n",
    "    # Row count via light projection (prefer a narrow column)\n",
    "    try:\n",
    "        t_min = dt.to_pyarrow_table(columns=[\"row_hash\"])\n",
    "        total_rows = t_min.num_rows\n",
    "    except Exception:\n",
    "        t_all = dt.to_pyarrow_table()\n",
    "        total_rows = t_all.num_rows\n",
    "\n",
    "    # Get a real pyarrow.Schema\n",
    "    try:\n",
    "        pa_schema = dt.schema().to_pyarrow()   # delta-rs Schema -> pyarrow.Schema\n",
    "    except Exception:\n",
    "        pa_schema = dt.to_pyarrow_table().schema  # fallback\n",
    "\n",
    "    # Safe field lookups\n",
    "    names = set(pa_schema.names)\n",
    "    date_field = pa_schema.field(\"date\") if \"date\" in names else None\n",
    "    ts_field   = pa_schema.field(\"timestamp\") if \"timestamp\" in names else None\n",
    "\n",
    "    print(\"Infer-ready row count:\", f\"{total_rows:,}\")\n",
    "    print(\"date field:\", date_field.type if date_field else \"MISSING\")\n",
    "    print(\"timestamp field:\", ts_field.type if ts_field else \"MISSING\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7f9ab51-dd13-49c8-a844-2bb73ae122bc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cce36eb5-aaa2-430c-b5d5-71b5a35b24e3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ccab94ef-8497-4b38-a102-9c22635de891",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a01c979-62c7-44d8-ba51-0018e5b5cb37",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9da59938-e907-4a31-9b01-64078c961e72",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a254f902-bc65-4d3f-83b9-710ba13039e5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "c6c5fcc9-c556-43e2-bcfd-e45d63d8e74b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Loaded features (25). First 5: ['Air_Fuel_Ratio_Commanded_:1', 'Air_Fuel_Ratio_Measured_:1', 'Catalyst_Temperature__Bank_1_Sensor_1', 'Catalyst_Temperature__Bank_1_Sensor_2', 'Engine_kW__At_the_wheels_kW']\n",
      "✓ INFER_READY: C:\\engine_module_pipeline\\delta\\engine_module_infer_ready\n"
     ]
    }
   ],
   "source": [
    "import os, json, math, time\n",
    "from pathlib import Path\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "from deltalake import DeltaTable\n",
    "\n",
    "ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "INFER_READY = ROOT / r\"delta\\engine_module_infer_ready\"\n",
    "ARTIFACTS   = ROOT / \"engine_module_artifacts\"\n",
    "FEATURES_JSON = ARTIFACTS / \"features.json\"\n",
    "\n",
    "assert (INFER_READY / \"_delta_log\").exists(), f\"Infer-ready Delta missing at {INFER_READY}\"\n",
    "assert FEATURES_JSON.exists(), f\"Missing {FEATURES_JSON}\"\n",
    "\n",
    "features = json.loads(FEATURES_JSON.read_text(encoding=\"utf-8\"))\n",
    "if isinstance(features, dict) and \"features\" in features:\n",
    "    features = features[\"features\"]\n",
    "assert isinstance(features, list) and len(features) == 25, \"features.json must list exactly 25 canonical features\"\n",
    "\n",
    "SYSTEM_COLS = [\"row_hash\",\"timestamp\",\"source_id\",\"kafka_key\",\"offset\",\"source_file\",\"date\"]\n",
    "EXPECTED_COLS = SYSTEM_COLS + features\n",
    "\n",
    "print(\"✓ Loaded features (25). First 5:\", features[:5])\n",
    "print(\"✓ INFER_READY:\", INFER_READY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "8a13b49c-f2f1-40a0-b3cd-479b19376cf8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SHAPE ===\n",
      "rows: 2,000\n",
      "cols: 32\n",
      "\n",
      "=== SCHEMA (pyarrow) ===\n",
      "- row_hash: string not null\n",
      "- timestamp: timestamp[us]\n",
      "- source_id: string\n",
      "- kafka_key: string\n",
      "- offset: int64\n",
      "- source_file: string\n",
      "- date: date32[day] not null\n",
      "- Air_Fuel_Ratio_Commanded_:1: double\n",
      "- Air_Fuel_Ratio_Measured_:1: double\n",
      "- Catalyst_Temperature__Bank_1_Sensor_1: double\n",
      "- Catalyst_Temperature__Bank_1_Sensor_2: double\n",
      "- Engine_kW__At_the_wheels_kW: double\n",
      "- Engine_Load_Absolute_pct: double\n",
      "- Engine_Oil_Temperature: double\n",
      "- Engine_RPM_rpm: double\n",
      "- Fuel_flow_rate_hour_l_hr: double\n",
      "- Fuel_Trim_Bank_1_Long_Term_pct: double\n",
      "- Fuel_Trim_Bank_1_Short_Term_pct: double\n",
      "- Mass_Air_Flow_Rate_g_s: double\n",
      "- O2_Sensor1_Wide_Range_Current_mA: double\n",
      "- O2_Bank_1_Sensor_2_Voltage_V: double\n",
      "- Run_time_since_engine_start_s: double\n",
      "- Timing_Advance: double\n",
      "- Turbo_Boost_&_Vacuum_Gauge_psi: double\n",
      "- Voltage__Control_Module_V: double\n",
      "- Volumetric_Efficiency__Calculated_pct: double\n",
      "- ECU_7EA:_Engine_Coolant_Temperature: double\n",
      "- ECU_7EA:_Intake_Air_Temperature: double\n",
      "- ECU_7EB:_Ambient_air_temp: double\n",
      "- ECU_7EB:_Engine_Load_pct: double\n",
      "- ECU_7EB:_Engine_RPM_rpm: double\n",
      "- ECU_7EB:_Speed__OBD_km_h: double\n",
      "\n",
      "=== PARTITIONS (first 20) ===\n",
      "['date=2025-09-15', 'date=2025-09-16']\n",
      "\n",
      "=== TIME COVERAGE ===\n",
      "timestamp min: 2025-09-15 23:28:03.053000\n",
      "timestamp max: 2025-09-16 20:33:36.403000\n",
      "date      min: 2025-09-15\n",
      "date      max: 2025-09-16\n"
     ]
    }
   ],
   "source": [
    "dt = DeltaTable(str(INFER_READY))\n",
    "\n",
    "# Row count using a narrow projection\n",
    "try:\n",
    "    t_min = dt.to_pyarrow_table(columns=[\"row_hash\"])\n",
    "    total_rows = t_min.num_rows\n",
    "except Exception:\n",
    "    total_rows = dt.to_pyarrow_table().num_rows\n",
    "\n",
    "# Schema -> pyarrow.Schema\n",
    "try:\n",
    "    pa_schema = dt.schema().to_pyarrow()\n",
    "except Exception:\n",
    "    pa_schema = dt.to_pyarrow_table().schema\n",
    "\n",
    "names = pa_schema.names\n",
    "print(\"\\n=== SHAPE ===\")\n",
    "print(\"rows:\", f\"{total_rows:,}\")\n",
    "print(\"cols:\", len(names))\n",
    "\n",
    "print(\"\\n=== SCHEMA (pyarrow) ===\")\n",
    "for f in pa_schema:\n",
    "    print(f\"- {f.name}: {f.type}{'' if f.nullable else ' not null'}\")\n",
    "\n",
    "# Partitions on disk\n",
    "parts = sorted([p.name for p in INFER_READY.glob(\"date=*\") if p.is_dir()])\n",
    "print(\"\\n=== PARTITIONS (first 20) ===\")\n",
    "print(parts[:20])\n",
    "\n",
    "# Time coverage\n",
    "ts_tbl = dt.to_pyarrow_table(columns=[\"timestamp\",\"date\"])\n",
    "ts_df = ts_tbl.to_pandas()\n",
    "if ts_df.empty:\n",
    "    print(\"\\n=== TIME COVERAGE ===\\n(table empty)\")\n",
    "else:\n",
    "    tmin = pd.to_datetime(ts_df[\"timestamp\"]).min()\n",
    "    tmax = pd.to_datetime(ts_df[\"timestamp\"]).max()\n",
    "    dmin = pd.to_datetime(ts_df[\"date\"]).min()\n",
    "    dmax = pd.to_datetime(ts_df[\"date\"]).max()\n",
    "    print(\"\\n=== TIME COVERAGE ===\")\n",
    "    print(\"timestamp min:\", tmin)\n",
    "    print(\"timestamp max:\", tmax)\n",
    "    print(\"date      min:\", None if pd.isna(dmin) else dmin.date())\n",
    "    print(\"date      max:\", None if pd.isna(dmax) else dmax.date())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a8057d7c-ce19-420f-9520-7afbd84f052d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COLUMN SET CHECK ===\n",
      "Missing columns: []\n",
      "Unexpected columns: []\n",
      "\n",
      "=== COLUMN ORDER CHECK ===\n",
      "Matches ideal order from contract?: True\n",
      "\n",
      "First 10 columns: ['row_hash', 'timestamp', 'source_id', 'kafka_key', 'offset', 'source_file', 'date', 'Air_Fuel_Ratio_Commanded_:1', 'Air_Fuel_Ratio_Measured_:1', 'Catalyst_Temperature__Bank_1_Sensor_1']\n"
     ]
    }
   ],
   "source": [
    "actual_cols = set(names)\n",
    "expected_cols = set(EXPECTED_COLS)\n",
    "\n",
    "missing = sorted(list(expected_cols - actual_cols))\n",
    "extras  = sorted(list(actual_cols - expected_cols))\n",
    "\n",
    "print(\"\\n=== COLUMN SET CHECK ===\")\n",
    "print(\"Missing columns:\", missing)\n",
    "print(\"Unexpected columns:\", extras)\n",
    "\n",
    "# Order check (not required by Delta itself, but good for humans)\n",
    "ideal_order = EXPECTED_COLS\n",
    "order_ok = list(names) == ideal_order\n",
    "print(\"\\n=== COLUMN ORDER CHECK ===\")\n",
    "print(\"Matches ideal order from contract?:\", order_ok)\n",
    "\n",
    "# Spot-print first 10 column names\n",
    "print(\"\\nFirst 10 columns:\", list(names)[:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "fb177ec9-11b1-4cc4-9017-e6aa92ed21c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== COMPLETENESS (top 15 by best completeness) ===\n",
      "row_hash                         total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "timestamp                        total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "source_id                        total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "kafka_key                        total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "offset                           total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "source_file                      total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "date                             total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Air_Fuel_Ratio_Commanded_:1      total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Air_Fuel_Ratio_Measured_:1       total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Catalyst_Temperature__Bank_1_Sensor_1 total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Catalyst_Temperature__Bank_1_Sensor_2 total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Engine_kW__At_the_wheels_kW      total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Engine_Load_Absolute_pct         total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Engine_Oil_Temperature           total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Engine_RPM_rpm                   total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "\n",
      "=== COMPLETENESS (worst 10 by null_frac) ===\n",
      "Timing_Advance                   total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Turbo_Boost_&_Vacuum_Gauge_psi   total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Voltage__Control_Module_V        total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "Volumetric_Efficiency__Calculated_pct total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EA:_Engine_Coolant_Temperature total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EA:_Intake_Air_Temperature  total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EB:_Ambient_air_temp        total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EB:_Engine_Load_pct         total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EB:_Engine_RPM_rpm          total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n",
      "ECU_7EB:_Speed__OBD_km_h         total=   2000 non_null=   2000 nulls=      0 null_frac=0.000\n"
     ]
    }
   ],
   "source": [
    "tbl_all = dt.to_pyarrow_table()\n",
    "pdf = tbl_all.to_pandas()  # fine at 2k rows; revisit if table grows huge\n",
    "\n",
    "n = len(pdf)\n",
    "stats = []\n",
    "for c in pdf.columns:\n",
    "    nulls = int(pd.isna(pdf[c]).sum())\n",
    "    stats.append((c, n, n - nulls, nulls, (nulls / n if n > 0 else 0.0)))\n",
    "\n",
    "stats.sort(key=lambda x: x[4])  # by null_frac ASC (most complete first)\n",
    "\n",
    "print(\"\\n=== COMPLETENESS (top 15 by best completeness) ===\")\n",
    "for name, tot, non_null, nulls, frac in stats[:15]:\n",
    "    print(f\"{name:32s} total={tot:7d} non_null={non_null:7d} nulls={nulls:7d} null_frac={frac:5.3f}\")\n",
    "\n",
    "print(\"\\n=== COMPLETENESS (worst 10 by null_frac) ===\")\n",
    "for name, tot, non_null, nulls, frac in stats[-10:]:\n",
    "    print(f\"{name:32s} total={tot:7d} non_null={non_null:7d} nulls={nulls:7d} null_frac={frac:5.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "6c053a95-5de9-4810-895d-05fd2d74b47d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== ROW HASH UNIQUENESS ===\n",
      "All row_hash values are unique.\n",
      "\n",
      "=== DATE VS TIMESTAMP FLOOR ===\n",
      "Mismatched rows: 0\n",
      "\n",
      "=== SYSTEM COL TYPES (pandas dtypes) ===\n",
      "row_hash               object\n",
      "timestamp      datetime64[us]\n",
      "source_id              object\n",
      "kafka_key              object\n",
      "offset                  int64\n",
      "source_file            object\n",
      "date                   object\n",
      "dtype: object\n",
      "\n",
      "=== OFFSET TYPE CHECK ===\n",
      "Is int-like distribution: {True: 2000}\n",
      "\n",
      "=== PROBLEMS SUMMARY ===\n",
      "Problems: None\n"
     ]
    }
   ],
   "source": [
    "problems = []\n",
    "\n",
    "dups = pdf[\"row_hash\"].value_counts()\n",
    "dups = dups[dups > 1]\n",
    "print(\"\\n=== ROW HASH UNIQUENESS ===\")\n",
    "if dups.empty:\n",
    "    print(\"All row_hash values are unique.\")\n",
    "else:\n",
    "    print(\"Duplicate row_hash count:\", len(dups))\n",
    "    print(\"Sample duplicates:\", list(dups.head(5).items()))\n",
    "    problems.append(\"DUP_ROW_HASH\")\n",
    "\n",
    "ts = pd.to_datetime(pdf[\"timestamp\"], errors=\"coerce\", utc=True)\n",
    "date_from_ts = ts.dt.tz_convert(None).dt.date.astype(\"object\")\n",
    "mismatch = (date_from_ts != pd.to_datetime(pdf[\"date\"], errors=\"coerce\").dt.date)\n",
    "mismatch_cnt = int(mismatch.sum())\n",
    "print(\"\\n=== DATE VS TIMESTAMP FLOOR ===\")\n",
    "print(\"Mismatched rows:\", mismatch_cnt)\n",
    "if mismatch_cnt > 0:\n",
    "    print(pdf.loc[mismatch, [\"row_hash\",\"timestamp\",\"date\"]].head(5))\n",
    "    problems.append(\"DATE_TS_MISMATCH\")\n",
    "\n",
    "print(\"\\n=== SYSTEM COL TYPES (pandas dtypes) ===\")\n",
    "print(pdf[SYSTEM_COLS].dtypes)\n",
    "\n",
    "non_int_offsets = pdf[\"offset\"].dropna().map(lambda x: isinstance(x, (int, np.integer))).value_counts()\n",
    "print(\"\\n=== OFFSET TYPE CHECK ===\")\n",
    "print(\"Is int-like distribution:\", dict(non_int_offsets))\n",
    "if not non_int_offsets.get(True, 0) == len(pdf[\"offset\"].dropna()):\n",
    "    problems.append(\"OFFSET_NON_INT\")\n",
    "\n",
    "print(\"\\n=== PROBLEMS SUMMARY ===\")\n",
    "print(\"Problems:\", problems if problems else \"None\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b8094413-71ca-488b-b5bb-33ecc11c2c3f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== FEATURES: WORST 10 BY NULL COUNT ===\n",
      "Air_Fuel_Ratio_Commanded_:1         nn= 2000 nulls=    0 null_frac=0.000 min=9.64949 p05=13.316 mean=13.7007 p95=14.0882 max=14.8418 std=0.755145 nonfinite=False\n",
      "Air_Fuel_Ratio_Measured_:1          nn= 2000 nulls=    0 null_frac=0.000 min=11.5725 p05=13.1274 mean=15.0592 p95=17.3593 max=17.3593 std=1.44844 nonfinite=False\n",
      "Catalyst_Temperature__Bank_1_Sensor_1 nn= 2000 nulls=    0 null_frac=0.000 min=461.8 p05=474.9 mean=552.685 p95=607.6 max=633.9 std=39.0235 nonfinite=False\n",
      "Catalyst_Temperature__Bank_1_Sensor_2 nn= 2000 nulls=    0 null_frac=0.000 min=282.3 p05=301.1 mean=424.801 p95=503.7 max=531 std=61.7923 nonfinite=False\n",
      "Engine_kW__At_the_wheels_kW         nn= 2000 nulls=    0 null_frac=0.000 min=0.255514 p05=0.668211 mean=0.667337 p95=0.668211 max=1.32983 std=0.0631608 nonfinite=False\n",
      "Engine_Load_Absolute_pct            nn= 2000 nulls=    0 null_frac=0.000 min=9.41177 p05=16.0784 mean=51.6355 p95=78.0392 max=101.176 std=16.5171 nonfinite=False\n",
      "Engine_Oil_Temperature              nn= 2000 nulls=    0 null_frac=0.000 min=64 p05=70 mean=80.1205 p95=85 max=86 std=4.64659 nonfinite=False\n",
      "Engine_RPM_rpm                      nn= 2000 nulls=    0 null_frac=0.000 min=114 p05=415.425 mean=1371.99 p95=2271 max=3060.75 std=531.807 nonfinite=False\n",
      "Fuel_flow_rate_hour_l_hr            nn= 2000 nulls=    0 null_frac=0.000 min=0.0031838 p05=0.00367102 mean=3.27303 p95=7.79269 max=11.5207 std=2.55692 nonfinite=False\n",
      "Fuel_Trim_Bank_1_Long_Term_pct      nn= 2000 nulls=    0 null_frac=0.000 min=-3.125 p05=-2.34375 mean=-0.823828 p95=-0.78125 max=0.78125 std=0.542263 nonfinite=False\n",
      "\n",
      "=== FEATURES: BEST 10 BY COMPLETENESS ===\n",
      "Air_Fuel_Ratio_Commanded_:1         nn= 2000 nulls=    0 null_frac=0.000 min=9.64949 p05=13.316 mean=13.7007 p95=14.0882 max=14.8418 std=0.755145 nonfinite=False\n",
      "Air_Fuel_Ratio_Measured_:1          nn= 2000 nulls=    0 null_frac=0.000 min=11.5725 p05=13.1274 mean=15.0592 p95=17.3593 max=17.3593 std=1.44844 nonfinite=False\n",
      "Catalyst_Temperature__Bank_1_Sensor_1 nn= 2000 nulls=    0 null_frac=0.000 min=461.8 p05=474.9 mean=552.685 p95=607.6 max=633.9 std=39.0235 nonfinite=False\n",
      "Catalyst_Temperature__Bank_1_Sensor_2 nn= 2000 nulls=    0 null_frac=0.000 min=282.3 p05=301.1 mean=424.801 p95=503.7 max=531 std=61.7923 nonfinite=False\n",
      "Engine_kW__At_the_wheels_kW         nn= 2000 nulls=    0 null_frac=0.000 min=0.255514 p05=0.668211 mean=0.667337 p95=0.668211 max=1.32983 std=0.0631608 nonfinite=False\n",
      "Engine_Load_Absolute_pct            nn= 2000 nulls=    0 null_frac=0.000 min=9.41177 p05=16.0784 mean=51.6355 p95=78.0392 max=101.176 std=16.5171 nonfinite=False\n",
      "Engine_Oil_Temperature              nn= 2000 nulls=    0 null_frac=0.000 min=64 p05=70 mean=80.1205 p95=85 max=86 std=4.64659 nonfinite=False\n",
      "Engine_RPM_rpm                      nn= 2000 nulls=    0 null_frac=0.000 min=114 p05=415.425 mean=1371.99 p95=2271 max=3060.75 std=531.807 nonfinite=False\n",
      "Fuel_flow_rate_hour_l_hr            nn= 2000 nulls=    0 null_frac=0.000 min=0.0031838 p05=0.00367102 mean=3.27303 p95=7.79269 max=11.5207 std=2.55692 nonfinite=False\n",
      "Fuel_Trim_Bank_1_Long_Term_pct      nn= 2000 nulls=    0 null_frac=0.000 min=-3.125 p05=-2.34375 mean=-0.823828 p95=-0.78125 max=0.78125 std=0.542263 nonfinite=False\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "WindowsPath('C:/engine_module_pipeline/delta/engine_module_infer_ready/date=2025-09-16')"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feat_stats = []\n",
    "nonfinite_any = False\n",
    "for f in features:\n",
    "    col = pd.to_numeric(pdf[f], errors=\"coerce\")\n",
    "    nn = int(col.notna().sum())\n",
    "    nz = int((col == 0).sum(skipna=True)) if nn else 0\n",
    "    vals = col.dropna()\n",
    "    if nn:\n",
    "        mn = float(vals.min())\n",
    "        mx = float(vals.max())\n",
    "        mean = float(vals.mean())\n",
    "        std = float(vals.std(ddof=1)) if len(vals) > 1 else 0.0\n",
    "        p05 = float(vals.quantile(0.05)) if len(vals) > 1 else mn\n",
    "        p95 = float(vals.quantile(0.95)) if len(vals) > 1 else mx\n",
    "    else:\n",
    "        mn = mx = mean = std = p05 = p95 = float(\"nan\")\n",
    "    nonfinite = int(np.isfinite(vals).sum()) != len(vals)\n",
    "    nonfinite_any = nonfinite_any or nonfinite\n",
    "    nulls = len(pdf) - nn\n",
    "    feat_stats.append((f, nn, nulls, nulls/len(pdf) if len(pdf) else 0.0, mn, mx, mean, std, p05, p95, nonfinite))\n",
    "\n",
    "feat_stats.sort(key=lambda x: x[2], reverse=True)  # sort by null count desc (worst first)\n",
    "\n",
    "print(\"\\n=== FEATURES: WORST 10 BY NULL COUNT ===\")\n",
    "for f, nn, nulls, frac, mn, mx, mean, std, p05, p95, nf in feat_stats[:10]:\n",
    "    print(f\"{f:35s} nn={nn:5d} nulls={nulls:5d} null_frac={frac:5.3f} \"\n",
    "          f\"min={mn:.6g} p05={p05:.6g} mean={mean:.6g} p95={p95:.6g} max={mx:.6g} std={std:.6g} nonfinite={nf}\")\n",
    "\n",
    "print(\"\\n=== FEATURES: BEST 10 BY COMPLETENESS ===\")\n",
    "best = sorted(feat_stats, key=lambda x: x[2])[:10]\n",
    "for f, nn, nulls, frac, mn, mx, mean, std, p05, p95, nf in best:\n",
    "    print(f\"{f:35s} nn={nn:5d} nulls={nulls:5d} null_frac={frac:5.3f} \"\n",
    "          f\"min={mn:.6g} p05={p05:.6g} mean={mean:.6g} p95={p95:.6g} max={mx:.6g} std={std:.6g} nonfinite={nf}\")\n",
    "\n",
    "p\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a4cc7963-9b6b-467b-b89d-e99e843cf423",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== SPARK BACKTICK HINT (columns that need quoting) ===\n",
      "Count: 9\n",
      "Examples: ['Air_Fuel_Ratio_Commanded_:1', 'Air_Fuel_Ratio_Measured_:1', 'Turbo_Boost_&_Vacuum_Gauge_psi', 'ECU_7EA:_Engine_Coolant_Temperature', 'ECU_7EA:_Intake_Air_Temperature', 'ECU_7EB:_Ambient_air_temp', 'ECU_7EB:_Engine_Load_pct', 'ECU_7EB:_Engine_RPM_rpm', 'ECU_7EB:_Speed__OBD_km_h']\n",
      "\n",
      "=== HEAD(5) — minimal columns for visual sanity ===\n",
      "                                            row_hash               timestamp source_id kafka_key  offset                                        source_file  \\\n",
      "0  d3d180f31455aa45ad65af08914f7e9038861eb75c800a... 2025-09-16 07:00:15.797    sim001    sim001    1500  C:\\engine_module_pipeline\\delta\\engine_module_...   \n",
      "1  d0b0fa020237ffd78f6120c946eabd80d2763cedc815ad... 2025-09-16 07:00:15.797    sim001    sim001    1501  C:\\engine_module_pipeline\\delta\\engine_module_...   \n",
      "2  5e2f4fb2d4a5b8b56f1caa1ee20a9427a6f078364ba8be... 2025-09-16 07:00:15.797    sim001    sim001    1502  C:\\engine_module_pipeline\\delta\\engine_module_...   \n",
      "3  4da988185369189b08a4902cedd6dfa4fc9cdae9f7d7c6... 2025-09-16 07:00:15.797    sim001    sim001    1503  C:\\engine_module_pipeline\\delta\\engine_module_...   \n",
      "4  1e25ef1d3588f6a651a1307914cf91f9b529e4bf0afe9f... 2025-09-16 07:00:15.800    sim001    sim001    1504  C:\\engine_module_pipeline\\delta\\engine_module_...   \n",
      "\n",
      "         date  Air_Fuel_Ratio_Commanded_:1  Air_Fuel_Ratio_Measured_:1  Catalyst_Temperature__Bank_1_Sensor_1  \n",
      "0  2025-09-16                    14.088164                   13.410123                             616.200012  \n",
      "1  2025-09-16                    14.088164                   13.410123                             616.200012  \n",
      "2  2025-09-16                    14.088164                   13.410123                             616.200012  \n",
      "3  2025-09-16                    14.088164                   13.410123                             616.200012  \n",
      "4  2025-09-16                    14.088164                   17.359253                             611.000000  \n"
     ]
    }
   ],
   "source": [
    "print(\"\\n=== SPARK BACKTICK HINT (columns that need quoting) ===\")\n",
    "needs_backticks = [c for c in EXPECTED_COLS if not c.isidentifier() or any(ch in c for ch in [' ',':','&','-','/','(',')','%'])]\n",
    "print(\"Count:\", len(needs_backticks))\n",
    "print(\"Examples:\", needs_backticks[:10])\n",
    "\n",
    "print(\"\\n=== HEAD(5) — minimal columns for visual sanity ===\")\n",
    "cols_show = [\"row_hash\",\"timestamp\",\"source_id\",\"kafka_key\",\"offset\",\"source_file\",\"date\"] + features[:3]\n",
    "print(pdf[cols_show].head(5))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0c2b11fb-41eb-414d-8e69-b914303f05ca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== PREFLIGHT STRUCTURAL CHECK (df_out-shaped) ===\n",
      "Violations: [('model_versions', 0), ('model_versions', 1), ('model_versions', 2), ('model_versions', 3), ('model_versions', 4)]\n",
      "Sample columns present: ['row_hash', 'timestamp', 'source_id', 'kafka_key', 'offset', 'source_file', 'date', 'Air_Fuel_Ratio_Commanded_:1', 'Air_Fuel_Ratio_Measured_:1', 'Catalyst_Temperature__Bank_1_Sensor_1', 'Catalyst_Temperature__Bank_1_Sensor_2', 'Engine_kW__At_the_wheels_kW'] …\n"
     ]
    }
   ],
   "source": [
    "\n",
    "def _is_list_of_dicts(v):\n",
    "    if not isinstance(v, list): return False\n",
    "    for d in v:\n",
    "        if not isinstance(d, dict): return False\n",
    "        if \"feature\" not in d or \"contribution\" not in d: return False\n",
    "    return True\n",
    "\n",
    "def _is_map_float(d):\n",
    "    if not isinstance(d, dict): return False\n",
    "    for k, v in d.items():\n",
    "        try:\n",
    "            _ = float(v) if v is not None else 0.0\n",
    "        except Exception:\n",
    "            return False\n",
    "    return True\n",
    "\n",
    "sample = pdf.head(20).copy()\n",
    "df_out_shape = sample.copy()\n",
    "df_out_shape[\"recon_error_dense\"] = 0.0\n",
    "df_out_shape[\"dense_per_feature_error\"] = [{f: 0.0 for f in features} for _ in range(len(sample))]\n",
    "df_out_shape[\"recon_error_lstm\"] = None\n",
    "df_out_shape[\"lstm_window_id\"] = None\n",
    "df_out_shape[\"isolation_score\"] = 0.0\n",
    "df_out_shape[\"kde_logp\"] = 0.0\n",
    "df_out_shape[\"gmm_logp\"] = 0.0\n",
    "df_out_shape[\"combiner_score\"] = None\n",
    "df_out_shape[\"composite_score\"] = 0.5\n",
    "df_out_shape[\"anomaly_label\"] = \"suspicious\"\n",
    "df_out_shape[\"anomaly_severity\"] = 1\n",
    "df_out_shape[\"model_versions\"] = {\"dense\":\"ts\",\"lstm\":\"sd\",\"isof\":\"joblib\"}\n",
    "df_out_shape[\"inference_run_id\"] = \"run-preflight\"\n",
    "df_out_shape[\"inference_ts\"] = pd.Timestamp.utcnow()\n",
    "df_out_shape[\"processing_latency_ms\"] = None\n",
    "df_out_shape[\"explain_top_k\"] = [[{\"feature\": features[0], \"contribution\": 0.1}]] * len(sample)\n",
    "df_out_shape[\"raw_model_outputs\"] = {}\n",
    "df_out_shape[\"notes\"] = None\n",
    "df_out_shape[\"date\"] = pd.to_datetime(df_out_shape[\"timestamp\"]).dt.date.astype(str)\n",
    "\n",
    "violations = []\n",
    "for idx, r in df_out_shape.head(5).iterrows():\n",
    "    if not _is_list_of_dicts(r[\"explain_top_k\"]): violations.append((\"explain_top_k\", idx))\n",
    "    if not _is_map_float(r[\"dense_per_feature_error\"]): violations.append((\"dense_per_feature_error\", idx))\n",
    "    if not isinstance(r[\"model_versions\"], dict): violations.append((\"model_versions\", idx))\n",
    "    if not (isinstance(r[\"anomaly_severity\"], (int, np.integer))): violations.append((\"anomaly_severity\", idx))\n",
    "    if r[\"date\"] is None or not isinstance(r[\"date\"], str): violations.append((\"date\", idx))\n",
    "\n",
    "print(\"\\n=== PREFLIGHT STRUCTURAL CHECK (df_out-shaped) ===\")\n",
    "print(\"Violations:\", violations if violations else \"None\")\n",
    "print(\"Sample columns present:\", list(df_out_shape.columns)[:12], \"…\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "4e9dd489-6fc5-4f65-b507-23a9adc6ff2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== GO / NO-GO FOR INFERENCE ===\n",
      "GO ✅  — Inference notebook can read this table safely.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "issues = []\n",
    "\n",
    "if len(missing) > 0: issues.append(f\"Missing columns: {missing}\")\n",
    "if len(extras)  > 0: issues.append(f\"Unexpected columns: {extras}\")\n",
    "\n",
    "if \"DUP_ROW_HASH\" in locals().get(\"problems\", []): issues.append(\"Duplicate row_hash found\")\n",
    "if \"DATE_TS_MISMATCH\" in locals().get(\"problems\", []): issues.append(\"date != floor(timestamp) mismatch present\")\n",
    "if \"OFFSET_NON_INT\" in locals().get(\"problems\", []): issues.append(\"offset contains non-integers\")\n",
    "\n",
    "high_null_feats = [f for f, nn, nulls, frac, *_ in feat_stats if frac > 0.30]\n",
    "if high_null_feats: issues.append(f\"Features with >30% nulls: {high_null_feats[:5]}{'…' if len(high_null_feats)>5 else ''}\")\n",
    "\n",
    "print(\"\\n=== GO / NO-GO FOR INFERENCE ===\")\n",
    "if not issues:\n",
    "    print(\"GO ✅  — Inference notebook can read this table safely.\")\n",
    "else:\n",
    "    print(\"NO-GO  — Please fix before inference.\")\n",
    "    for it in issues:\n",
    "        print(\" -\", it)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4ae1995-1ed2-40f1-ba71-83d02f8dc93b",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark (Py3.11 + Spark 3.5)",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
