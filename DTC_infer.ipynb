{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ef22bc5c-2d0e-45aa-b350-06f87850fefe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 01:45:42,650 INFO Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: setup, imports, constants, utility helpers\n",
    "import os\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "import shutil\n",
    "import logging\n",
    "from pathlib import Path\n",
    "from typing import Dict, List, Tuple, Any, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as pq\n",
    "import joblib\n",
    "import torch\n",
    "\n",
    "# optional: deltalake for reading infer-ready table; fallback to pyarrow dataset if not present\n",
    "try:\n",
    "    from deltalake import DeltaTable\n",
    "    HAS_DELTALAKE = True\n",
    "except Exception:\n",
    "    HAS_DELTALAKE = False\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format=\"%(asctime)s %(levelname)s %(message)s\")\n",
    "log = logging.getLogger(\"dtc_infer\")\n",
    "\n",
    "# Root paths (Windows)\n",
    "ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "INFER_READY = ROOT / r\"delta\\engine_module_infer_ready\"\n",
    "ARTIFACTS_ROOT = ROOT / r\"DTC_stage\\artifacts\"\n",
    "OUTPUT_ROOT = ROOT / r\"DTC_stage\\data\\Output\"\n",
    "PER_DTC_OUT = OUTPUT_ROOT / \"per_dtc\"\n",
    "COMBINED_OUT = OUTPUT_ROOT / \"combined\"\n",
    "\n",
    "# Create output dirs\n",
    "PER_DTC_OUT.mkdir(parents=True, exist_ok=True)\n",
    "COMBINED_OUT.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Which DTCs to run inference for (will pick up artifacts)\n",
    "DTC_CODES = [\"P0234\", \"P0300\", \"P0420\", \"P0501\", \"P0562\"]\n",
    "\n",
    "# Max number of infer-ready rows to process in one run (as requested)\n",
    "MAX_ROWS = 2000\n",
    "\n",
    "# Device\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "log.info(f\"Using device: {DEVICE}\")\n",
    "\n",
    "# --- tiny helpers ---\n",
    "def atomic_write_df_to_parquet(df: pd.DataFrame, out_path: Path, partition_on_date: bool = True) -> None:\n",
    "    \"\"\"\n",
    "    Write df to out_path as parquet atomically.\n",
    "    If partition_on_date True, expects df['date'] present and writes into out_path/date=YYYY-MM-DD/\n",
    "    Otherwise writes single parquet file out_path.\n",
    "    \"\"\"\n",
    "    out_path = Path(out_path)\n",
    "    tmp_path = out_path.with_suffix(\".tmp.parquet\")\n",
    "    tmp_dir = tmp_path.parent\n",
    "    tmp_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    if partition_on_date and \"date\" in df.columns:\n",
    "        # write per-date files inside out_path, each as parquet\n",
    "        for date_val, grp in df.groupby(\"date\"):\n",
    "            date_str = pd.to_datetime(date_val).strftime(\"%Y-%m-%d\")\n",
    "            part_dir = out_path / f\"date={date_str}\"\n",
    "            part_dir.mkdir(parents=True, exist_ok=True)\n",
    "            file_name = f\"part_{int(time.time())}_{date_str}.parquet\"\n",
    "            file_path_tmp = part_dir / (file_name + \".tmp\")\n",
    "            file_path_final = part_dir / file_name\n",
    "            # write tmp then atomic replace\n",
    "            grp.to_parquet(file_path_tmp, engine=\"pyarrow\", index=False)\n",
    "            os.replace(file_path_tmp, file_path_final)\n",
    "    else:\n",
    "        # single-file write\n",
    "        df.to_parquet(tmp_path, engine=\"pyarrow\", index=False)\n",
    "        os.replace(tmp_path, out_path)\n",
    "\n",
    "def load_json(path: Path) -> Dict[str, Any]:\n",
    "    with open(path, \"r\", encoding=\"utf-8\") as f:\n",
    "        return json.load(f)\n",
    "\n",
    "def save_json(obj: Any, path: Path) -> None:\n",
    "    path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(obj, f, indent=2)\n",
    "\n",
    "def ensure_artifact_for_dtc(dtc_code: str) -> Path:\n",
    "    art = ARTIFACTS_ROOT / dtc_code\n",
    "    if not art.exists():\n",
    "        raise FileNotFoundError(f\"Artifacts for {dtc_code} not found under {art}\")\n",
    "    return art\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "50356ca1-07ae-4aef-9c53-5e56ad0a7ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: read up to `limit` rows from infer-ready table into a DataFrame\n",
    "def read_infer_ready_batch(limit: int = MAX_ROWS) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Attempt to read up to `limit` rows from the infer-ready Delta table.\n",
    "    Tries deltalake first; if not available, will scan parquet files under the delta directory.\n",
    "    Returns pandas.DataFrame with at least the canonical 25 features + row_hash + timestamp + date.\n",
    "    \"\"\"\n",
    "    if not INFER_READY.exists():\n",
    "        raise FileNotFoundError(f\"Infer-ready Delta path not found: {INFER_READY}\")\n",
    "\n",
    "    if HAS_DELTALAKE:\n",
    "        log.info(\"Reading infer-ready via deltalake (fast path)\")\n",
    "        dt = DeltaTable(INFER_READY.as_posix())\n",
    "        # fetch only necessary columns: row_hash, timestamp and canonical features (we don't know exact features file here)\n",
    "        # We'll ask the table for schema and then select first N rows\n",
    "        try:\n",
    "            tbl = dt.to_pyarrow_table()\n",
    "            df = tbl.to_pandas()\n",
    "            if len(df) > limit:\n",
    "                df = df.iloc[:limit].copy()\n",
    "            # Ensure timestamp is datetime (UTC naive)\n",
    "            if \"timestamp\" in df.columns:\n",
    "                df[\"timestamp\"] = pd.to_datetime(df[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "                df[\"date\"] = df[\"timestamp\"].dt.date\n",
    "            else:\n",
    "                raise RuntimeError(\"infer-ready table missing 'timestamp' column\")\n",
    "            return df\n",
    "        except Exception as e:\n",
    "            log.exception(\"deltalake read path failed; falling back to parquet scanning: %s\", e)\n",
    "\n",
    "    # Fallback: scan parquet files under the delta folder (fast, robust)\n",
    "    log.info(\"Reading infer-ready via parquet fallback (scanning files)\")\n",
    "    parquet_files = list((INFER_READY).rglob(\"*.parquet\"))\n",
    "    parquet_files = sorted(parquet_files, key=lambda p: p.stat().st_mtime, reverse=False)\n",
    "    if not parquet_files:\n",
    "        raise FileNotFoundError(f\"No parquet files found under {INFER_READY}\")\n",
    "    rows = []\n",
    "    read = 0\n",
    "    for pf in parquet_files:\n",
    "        try:\n",
    "            table = pq.read_table(pf.as_posix(), columns=None)\n",
    "            df_part = table.to_pandas()\n",
    "            # enforce timestamp parsing\n",
    "            if \"timestamp\" in df_part.columns:\n",
    "                df_part[\"timestamp\"] = pd.to_datetime(df_part[\"timestamp\"], utc=True).dt.tz_convert(None)\n",
    "                df_part[\"date\"] = df_part[\"timestamp\"].dt.date\n",
    "            else:\n",
    "                # skip files without timestamp column\n",
    "                continue\n",
    "            need = limit - read\n",
    "            if need <= 0:\n",
    "                break\n",
    "            df_chunk = df_part.iloc[:need]\n",
    "            rows.append(df_chunk)\n",
    "            read += len(df_chunk)\n",
    "            if read >= limit:\n",
    "                break\n",
    "        except Exception:\n",
    "            log.exception(\"Failed reading parquet file %s; skipping\", pf)\n",
    "            continue\n",
    "    if not rows:\n",
    "        raise RuntimeError(\"No rows read from infer-ready source\")\n",
    "    df = pd.concat(rows, ignore_index=True)\n",
    "    log.info(\"Read %d rows from infer-ready\", len(df))\n",
    "    return df\n",
    "\n",
    "# quick smoke read (not executed automatically)\n",
    "# df_infer = read_infer_ready_batch(100)\n",
    "# df_infer.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9ca16a98-06f9-4de4-9207-bd173740a18d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 3: windowing and thresholding utilities\n",
    "\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.isotonic import IsotonicRegression\n",
    "\n",
    "def sliding_windows_from_df(\n",
    "    df: pd.DataFrame,\n",
    "    features: List[str],\n",
    "    window: int,\n",
    "    stride: int = 1,\n",
    ") -> Tuple[np.ndarray, List[pd.Timestamp], List[str]]:\n",
    "    \"\"\"\n",
    "    Build sliding windows of shape [N_windows, window, n_features] from df[features].\n",
    "    Returns (X_windows, end_timestamps, row_hashes_for_end)\n",
    "    - X_windows: float32 numpy array (N_windows, window, C)\n",
    "    - end_timestamps: list of timestamps (window end row timestamp)\n",
    "    - row_hashes_for_end: list of row_hash strings mapped to the window end\n",
    "    Missing values remain as np.nan (scaler must handle or we impute later).\n",
    "    \"\"\"\n",
    "    n_rows = len(df)\n",
    "    if n_rows < window:\n",
    "        return np.zeros((0, window, len(features)), dtype=np.float32), [], []\n",
    "    idx_starts = list(range(0, n_rows - window + 1, stride))\n",
    "    X = np.zeros((len(idx_starts), window, len(features)), dtype=np.float32)\n",
    "    end_ts = []\n",
    "    end_row_hash = []\n",
    "    for i, s in enumerate(idx_starts):\n",
    "        wnd = df.iloc[s : s + window]\n",
    "        X[i, :, :] = wnd[features].to_numpy(dtype=np.float32)\n",
    "        end_ts.append(pd.to_datetime(wnd[\"timestamp\"].iloc[-1]))\n",
    "        end_row_hash.append(str(wnd[\"row_hash\"].iloc[-1]) if \"row_hash\" in wnd.columns else \"\")\n",
    "    return X, end_ts, end_row_hash\n",
    "\n",
    "def apply_thresholds_with_hysteresis(\n",
    "    probs: np.ndarray,\n",
    "    dPdt: np.ndarray,\n",
    "    thresholds: Dict[str, Any],\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    probs: 1D array of calibrated probabilities\n",
    "    dPdt: 1D array of derivative (same length)\n",
    "    thresholds: dict containing keys:\n",
    "        'T_on','T_off','rate_threshold','min_consec_on'\n",
    "    Returns boolean array of same length: True when predicted ON.\n",
    "    \"\"\"\n",
    "    T_on = float(thresholds.get(\"T_on\", 0.75))\n",
    "    T_off = float(thresholds.get(\"T_off\", max(0.0, T_on - 0.10)))\n",
    "    rate_thr = float(thresholds.get(\"rate_threshold\", 0.1))\n",
    "    min_consec = int(thresholds.get(\"min_consec_on\", 1))\n",
    "\n",
    "    out = np.zeros(len(probs), dtype=np.uint8)\n",
    "    on = False\n",
    "    consec = 0\n",
    "    for i in range(len(probs)):\n",
    "        p = float(probs[i]) if not (np.isnan(probs[i])) else 0.0\n",
    "        dp = float(dPdt[i]) if not (np.isnan(dPdt[i])) else 0.0\n",
    "        trigger = (p >= T_on) or (dp >= rate_thr and p >= max(0.4, T_on - 0.2))\n",
    "        if on:\n",
    "            if p <= T_off:\n",
    "                on = False\n",
    "                consec = 0\n",
    "        else:\n",
    "            if trigger:\n",
    "                consec += 1\n",
    "                if consec >= min_consec:\n",
    "                    on = True\n",
    "                    consec = 0\n",
    "            else:\n",
    "                consec = 0\n",
    "        out[i] = 1 if on else 0\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92b5a3da-f7ee-4402-9248-ad4d35dd56df",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 (FIXED): per-DTC inference (returns tidy DataFrame)\n",
    "from math import ceil\n",
    "from datetime import datetime\n",
    "\n",
    "def infer_for_one_dtc(\n",
    "    dtc_code: str,\n",
    "    df_batch: pd.DataFrame,\n",
    "    cadence_seconds: float = 1.0,\n",
    "    batch_windows_inference: int = 256,\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Run inference for one DTC over the df_batch (which is a slice of infer-ready rows).\n",
    "    Returns tidy DataFrame with columns described in the plan.\n",
    "    This version fixes datetime/diff handling and is more defensive about scalers.\n",
    "    \"\"\"\n",
    "    art = ensure_artifact_for_dtc(dtc_code)\n",
    "    feature_spec = load_json(art / \"feature_spec.json\")\n",
    "    features = feature_spec[\"features\"]\n",
    "    window_length = int(feature_spec.get(\"window_length\", 64))\n",
    "    stride = int(feature_spec.get(\"stride\", 1))\n",
    "    cadence_seconds = float(feature_spec.get(\"cadence_seconds\", cadence_seconds))\n",
    "\n",
    "    # Load artifacts (fail early with clear message)\n",
    "    scaler_path = art / f\"scaler_{dtc_code}.pkl\"\n",
    "    iso_prec_path = art / f\"calib_{dtc_code}_precursor.pkl\"\n",
    "    iso_fault_path = art / f\"calib_{dtc_code}_fault.pkl\"\n",
    "    thresholds_path = art / \"thresholds.json\"\n",
    "    model_ts_path = art / f\"model_{dtc_code}.ts\"\n",
    "\n",
    "    for p in (scaler_path, iso_prec_path, iso_fault_path, thresholds_path, model_ts_path):\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Missing artifact for {dtc_code}: {p}\")\n",
    "\n",
    "    scaler = joblib.load(scaler_path.as_posix())\n",
    "    iso_prec = joblib.load(iso_prec_path.as_posix())\n",
    "    iso_fault = joblib.load(iso_fault_path.as_posix())\n",
    "    thresholds = load_json(thresholds_path)\n",
    "\n",
    "    model = torch.jit.load(model_ts_path.as_posix(), map_location=\"cpu\").to(DEVICE)\n",
    "    model.eval()\n",
    "\n",
    "    # Ensure the df_batch contains all features; if missing, create columns with NaN and coerce to float\n",
    "    df_proc = df_batch.copy().reset_index(drop=True)\n",
    "    for f in features:\n",
    "        if f not in df_proc.columns:\n",
    "            df_proc[f] = np.nan\n",
    "    # coerce feature dtypes to float32 (NaNs remain NaN)\n",
    "    df_proc[features] = df_proc[features].astype(float)\n",
    "\n",
    "    # Build sliding windows\n",
    "    X_wnd, end_ts, end_row_hash = sliding_windows_from_df(df_proc, features, window_length, stride)\n",
    "    n_w = X_wnd.shape[0]\n",
    "    if n_w == 0:\n",
    "        log.info(\"No windows produced for %s (dataset too small for window %d)\", dtc_code, window_length)\n",
    "        return pd.DataFrame(\n",
    "            columns=[\n",
    "                \"row_hash\", \"timestamp\", \"date\", \"dtc_code\",\n",
    "                \"p_raw_precursor\", \"p_calib_precursor\", \"p_raw_fault\", \"p_calib_fault\",\n",
    "                \"dPdt_precursor\", \"dPdt_fault\", \"pred_precursor_on\", \"pred_fault_on\",\n",
    "                \"alert_level\", \"feature_snapshot\"\n",
    "            ]\n",
    "        )\n",
    "\n",
    "    # Apply scaler robustly:\n",
    "    # scaler.transform will error on unexpected shapes; we also need to handle NaNs.\n",
    "    # Strategy: flatten windows -> (n_w * window, C). If scaler fails due to NaNs, fall back to per-column median imputation.\n",
    "    X_reshaped = X_wnd.reshape(-1, len(features))  # (n_w * window, C)\n",
    "    try:\n",
    "        X_scaled_flat = scaler.transform(X_reshaped)\n",
    "    except Exception as e:\n",
    "        log.warning(\"scaler.transform failed for %s due to %s; performing median impute before transform\", dtc_code, e)\n",
    "        # median impute per-column based on df_proc stats\n",
    "        col_medians = np.nanmedian(X_reshaped, axis=0)\n",
    "        nan_mask = np.isnan(X_reshaped)\n",
    "        X_filled = X_reshaped.copy()\n",
    "        for ci in range(X_filled.shape[1]):\n",
    "            X_filled[nan_mask[:, ci], ci] = col_medians[ci] if not np.isnan(col_medians[ci]) else 0.0\n",
    "        X_scaled_flat = scaler.transform(X_filled)\n",
    "\n",
    "    X_scaled = X_scaled_flat.reshape(n_w, window_length, len(features)).astype(np.float32)\n",
    "\n",
    "    # Inference in mini-batches\n",
    "    p_prec_raw_list = []\n",
    "    p_fault_raw_list = []\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, n_w, batch_windows_inference):\n",
    "            xb = torch.from_numpy(X_scaled[i : i + batch_windows_inference]).to(DEVICE)  # [B, T, C]\n",
    "            res = model(xb)\n",
    "            # handle typical model outputs (dict with keys or tuple)\n",
    "            if isinstance(res, dict):\n",
    "                if \"p_precursor\" in res and \"p_fault\" in res:\n",
    "                    pp = res[\"p_precursor\"].cpu().numpy()\n",
    "                    pf = res[\"p_fault\"].cpu().numpy()\n",
    "                elif \"logits\" in res:\n",
    "                    # logits may be tuple/list with two tensors\n",
    "                    l = res[\"logits\"]\n",
    "                    if isinstance(l, (list, tuple)) and len(l) >= 2:\n",
    "                        pp = torch.sigmoid(l[0]).cpu().numpy()\n",
    "                        pf = torch.sigmoid(l[1]).cpu().numpy()\n",
    "                    else:\n",
    "                        raise RuntimeError(\"Unexpected 'logits' format from model for %s\" % dtc_code)\n",
    "                else:\n",
    "                    raise RuntimeError(\"Unexpected model output dict keys: %s\" % list(res.keys()))\n",
    "            elif isinstance(res, (tuple, list)) and len(res) >= 2:\n",
    "                # maybe model returned (p_precursor, p_fault)\n",
    "                pp = res[0].cpu().numpy()\n",
    "                pf = res[1].cpu().numpy()\n",
    "            else:\n",
    "                raise RuntimeError(\"Unexpected model output type: %s\" % type(res))\n",
    "\n",
    "            # Take last timestep of each window (index -1)\n",
    "            if pp.ndim == 2:\n",
    "                p_prec_raw_list.append(pp[:, -1].reshape(-1))\n",
    "            elif pp.ndim == 1:\n",
    "                # some models directly output a single prob per window\n",
    "                p_prec_raw_list.append(pp.reshape(-1))\n",
    "            else:\n",
    "                raise RuntimeError(\"Unexpected shape for pp output: %s\" % (pp.shape,))\n",
    "            if pf.ndim == 2:\n",
    "                p_fault_raw_list.append(pf[:, -1].reshape(-1))\n",
    "            elif pf.ndim == 1:\n",
    "                p_fault_raw_list.append(pf.reshape(-1))\n",
    "            else:\n",
    "                raise RuntimeError(\"Unexpected shape for pf output: %s\" % (pf.shape,))\n",
    "\n",
    "    p_prec_raw = np.concatenate(p_prec_raw_list, axis=0)[:n_w]\n",
    "    p_fault_raw = np.concatenate(p_fault_raw_list, axis=0)[:n_w]\n",
    "\n",
    "    # Calibration (isotonic on the scalar probs) with safe fallback\n",
    "    try:\n",
    "        p_prec_calib = iso_prec.transform(p_prec_raw)\n",
    "    except Exception:\n",
    "        log.warning(\"precursor calibrator failed for %s; clipping raw probabilities\", dtc_code)\n",
    "        p_prec_calib = np.clip(p_prec_raw, 0.0, 1.0)\n",
    "    try:\n",
    "        p_fault_calib = iso_fault.transform(p_fault_raw)\n",
    "    except Exception:\n",
    "        log.warning(\"fault calibrator failed for %s; clipping raw probabilities\", dtc_code)\n",
    "        p_fault_calib = np.clip(p_fault_raw, 0.0, 1.0)\n",
    "\n",
    "    # dP/dt (finite diff) using plain python datetime diffs (robust)\n",
    "    # ensure end_ts are datetime objects\n",
    "    end_ts_dt = []\n",
    "    for ts in end_ts:\n",
    "        if isinstance(ts, (pd.Timestamp, np.datetime64)):\n",
    "            end_ts_dt.append(pd.to_datetime(ts).to_pydatetime())\n",
    "        elif isinstance(ts, datetime):\n",
    "            end_ts_dt.append(ts)\n",
    "        else:\n",
    "            # fallback: try parse string\n",
    "            end_ts_dt.append(pd.to_datetime(ts).to_pydatetime())\n",
    "\n",
    "    dp_prec = np.zeros_like(p_prec_calib, dtype=np.float32)\n",
    "    dp_fault = np.zeros_like(p_fault_calib, dtype=np.float32)\n",
    "    for i in range(1, len(end_ts_dt)):\n",
    "        dt_sec = (end_ts_dt[i] - end_ts_dt[i - 1]).total_seconds()\n",
    "        if dt_sec <= 0:\n",
    "            dt_sec = cadence_seconds\n",
    "        dp_prec[i] = float((p_prec_calib[i] - p_prec_calib[i - 1]) / dt_sec)\n",
    "        dp_fault[i] = float((p_fault_calib[i] - p_fault_calib[i - 1]) / dt_sec)\n",
    "\n",
    "    # Thresholding/hysteresis -> boolean series\n",
    "    thr_prec = {\n",
    "        \"T_on\": thresholds.get(\"prec_on\", thresholds.get(\"prec_on\", 0.75)),\n",
    "        \"T_off\": thresholds.get(\"prec_off\", thresholds.get(\"prec_off\", max(0.0, thresholds.get(\"prec_on\", 0.75) - 0.1))),\n",
    "        \"rate_threshold\": thresholds.get(\"dPdt_prec\", 0.1),\n",
    "        \"min_consec_on\": thresholds.get(\"min_consec_on_prec\", 1),\n",
    "    }\n",
    "    thr_fault = {\n",
    "        \"T_on\": thresholds.get(\"fault_on\", thresholds.get(\"fault_on\", 0.75)),\n",
    "        \"T_off\": thresholds.get(\"fault_off\", thresholds.get(\"fault_off\", max(0.0, thresholds.get(\"fault_on\", 0.75) - 0.1))),\n",
    "        \"rate_threshold\": thresholds.get(\"dPdt_fault\", 0.1),\n",
    "        \"min_consec_on\": thresholds.get(\"min_consec_on_fault\", 1),\n",
    "    }\n",
    "    pred_prec = apply_thresholds_with_hysteresis(p_prec_calib, dp_prec, thr_prec)\n",
    "    pred_fault = apply_thresholds_with_hysteresis(p_fault_calib, dp_fault, thr_fault)\n",
    "\n",
    "    # alert_level: 0 none, 1 precursor-only, 2 fault, 3 both\n",
    "    alert_level = (pred_prec.astype(np.uint8) + pred_fault.astype(np.uint8) * 2)\n",
    "\n",
    "    # Build feature_snapshot as JSON strings (small) for hover/debug\n",
    "    df_reset = df_proc.reset_index(drop=True)\n",
    "    n_rows = len(df_reset)\n",
    "    starts = list(range(0, n_rows - window_length + 1, stride))\n",
    "    feature_snapshots = []\n",
    "    for s in starts:\n",
    "        row = df_reset.iloc[s + window_length - 1]\n",
    "        snap = {f: (None if (pd.isna(row.get(f))) else float(row.get(f))) for f in features}\n",
    "        feature_snapshots.append(json.dumps(snap))\n",
    "\n",
    "    # assemble output DataFrame\n",
    "    out_df = pd.DataFrame({\n",
    "        \"row_hash\": end_row_hash,\n",
    "        \"timestamp\": pd.to_datetime(end_ts_dt),\n",
    "        \"date\": [d.date() for d in end_ts_dt],\n",
    "        \"dtc_code\": [dtc_code] * n_w,\n",
    "        \"p_raw_precursor\": p_prec_raw.astype(np.float32),\n",
    "        \"p_calib_precursor\": p_prec_calib.astype(np.float32),\n",
    "        \"p_raw_fault\": p_fault_raw.astype(np.float32),\n",
    "        \"p_calib_fault\": p_fault_calib.astype(np.float32),\n",
    "        \"dPdt_precursor\": dp_prec.astype(np.float32),\n",
    "        \"dPdt_fault\": dp_fault.astype(np.float32),\n",
    "        \"pred_precursor_on\": pred_prec.astype(np.uint8),\n",
    "        \"pred_fault_on\": pred_fault.astype(np.uint8),\n",
    "        \"alert_level\": alert_level.astype(np.uint8),\n",
    "        \"feature_snapshot\": feature_snapshots,\n",
    "    })\n",
    "\n",
    "    # ensure proper dtypes\n",
    "    out_df[\"timestamp\"] = pd.to_datetime(out_df[\"timestamp\"])\n",
    "    return out_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e710039e-d63a-4362-9bd1-64a37b78d537",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-10-25 01:45:56,232 INFO Reading infer-ready via deltalake (fast path)\n",
      "2025-10-25 01:45:56,489 INFO Running inference for P0234\n",
      "2025-10-25 01:45:58,029 INFO Wrote 1953 rows for P0234\n",
      "2025-10-25 01:45:58,029 INFO Running inference for P0300\n",
      "2025-10-25 01:45:59,200 INFO Wrote 1937 rows for P0300\n",
      "2025-10-25 01:45:59,200 INFO Running inference for P0420\n",
      "2025-10-25 01:46:00,694 INFO Wrote 1873 rows for P0420\n",
      "2025-10-25 01:46:00,694 INFO Running inference for P0501\n",
      "2025-10-25 01:46:01,904 INFO Wrote 1937 rows for P0501\n",
      "2025-10-25 01:46:01,904 INFO Running inference for P0562\n",
      "2025-10-25 01:46:03,042 INFO Wrote 1937 rows for P0562\n",
      "2025-10-25 01:46:03,091 INFO Wrote combined table with 9637 rows to C:\\engine_module_pipeline\\DTC_stage\\data\\Output\\combined\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "  \"rows_read\": 2000,\n",
      "  \"dtc\": {\n",
      "    \"P0234\": {\n",
      "      \"rows\": 1953,\n",
      "      \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\per_dtc\\\\P0234\"\n",
      "    },\n",
      "    \"P0300\": {\n",
      "      \"rows\": 1937,\n",
      "      \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\per_dtc\\\\P0300\"\n",
      "    },\n",
      "    \"P0420\": {\n",
      "      \"rows\": 1873,\n",
      "      \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\per_dtc\\\\P0420\"\n",
      "    },\n",
      "    \"P0501\": {\n",
      "      \"rows\": 1937,\n",
      "      \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\per_dtc\\\\P0501\"\n",
      "    },\n",
      "    \"P0562\": {\n",
      "      \"rows\": 1937,\n",
      "      \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\per_dtc\\\\P0562\"\n",
      "    }\n",
      "  },\n",
      "  \"combined\": {\n",
      "    \"rows\": 9637,\n",
      "    \"written\": \"C:\\\\engine_module_pipeline\\\\DTC_stage\\\\data\\\\Output\\\\combined\"\n",
      "  },\n",
      "  \"time_seconds\": 6.858848571777344\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: orchestrator runner\n",
    "def run_inference_batch_and_write(limit: int = MAX_ROWS) -> Dict[str, Any]:\n",
    "    \"\"\"\n",
    "    Read up to `limit` infer-ready rows, run each DTC model, and write per-DTC and combined parquet outputs.\n",
    "    Returns a summary dict.\n",
    "    \"\"\"\n",
    "    start_ts = time.time()\n",
    "    df_infer = read_infer_ready_batch(limit)\n",
    "    if df_infer.empty:\n",
    "        raise RuntimeError(\"No infer-ready rows to process\")\n",
    "\n",
    "    # ensure canonical ordering columns exist (we'll not reorder here, each DTC picks columns it needs)\n",
    "    summary = {\"rows_read\": len(df_infer), \"dtc\": {}}\n",
    "    per_dtc_outputs = []\n",
    "    for dtc in DTC_CODES:\n",
    "        try:\n",
    "            log.info(\"Running inference for %s\", dtc)\n",
    "            out_df = infer_for_one_dtc(dtc, df_infer)\n",
    "            # if empty, still create empty file marker\n",
    "            art_dir = PER_DTC_OUT / dtc\n",
    "            art_dir.mkdir(parents=True, exist_ok=True)\n",
    "            timestamp_tag = pd.Timestamp.utcnow().strftime(\"%Y%m%d_%H%M%S\")\n",
    "            out_file = art_dir / f\"{dtc}_infer_{timestamp_tag}.parquet\"\n",
    "            # write partitioned by date under per_dtc/<dtc>/\n",
    "            atomic_write_df_to_parquet(out_df, art_dir, partition_on_date=True)\n",
    "            summary[\"dtc\"][dtc] = {\"rows\": len(out_df), \"written\": str(art_dir)}\n",
    "            per_dtc_outputs.append(out_df)\n",
    "            log.info(\"Wrote %d rows for %s\", len(out_df), dtc)\n",
    "        except Exception as e:\n",
    "            log.exception(\"Inference for %s failed: %s\", dtc, e)\n",
    "            summary[\"dtc\"][dtc] = {\"error\": str(e)}\n",
    "\n",
    "    # Combined tidy table: concat all per_dtc_outputs\n",
    "    combined_df = pd.concat(per_dtc_outputs, ignore_index=True) if per_dtc_outputs else pd.DataFrame()\n",
    "    if not combined_df.empty:\n",
    "        # dedupe on (dtc_code,timestamp,row_hash) keeping last (most recent)\n",
    "        combined_df.sort_values([\"dtc_code\", \"timestamp\"], inplace=True)\n",
    "        combined_df = combined_df.drop_duplicates(subset=[\"dtc_code\", \"timestamp\", \"row_hash\"], keep=\"last\")\n",
    "        # write combined output (partition by date)\n",
    "        atomic_write_df_to_parquet(combined_df, COMBINED_OUT, partition_on_date=True)\n",
    "        summary[\"combined\"] = {\"rows\": len(combined_df), \"written\": str(COMBINED_OUT)}\n",
    "        log.info(\"Wrote combined table with %d rows to %s\", len(combined_df), COMBINED_OUT)\n",
    "    else:\n",
    "        summary[\"combined\"] = {\"rows\": 0}\n",
    "\n",
    "    summary[\"time_seconds\"] = time.time() - start_ts\n",
    "    return summary\n",
    "\n",
    "# Run the orchestrator for up to MAX_ROWS rows\n",
    "summary = run_inference_batch_and_write(limit=MAX_ROWS)\n",
    "print(json.dumps(summary, indent=2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "98d1d815-9fc1-4bc5-a50e-7a97d8dfdbfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CSV export summary:\n",
      " - P0101: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0101_infer_output.csv\n",
      " - P0125: rows=1905, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0125_infer_output.csv\n",
      " - P0133: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0133_infer_output.csv\n",
      " - P0171: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0171_infer_output.csv\n",
      " - P0217: rows=1905, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0217_infer_output.csv\n",
      " - P0234: rows=1953, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0234_infer_output.csv\n",
      " - P0300: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0300_infer_output.csv\n",
      " - P0420: rows=1873, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0420_infer_output.csv\n",
      " - P0501: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0501_infer_output.csv\n",
      " - P0562: rows=1937, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\P0562_infer_output.csv\n",
      " - combined: rows=19258, csv=C:\\engine_module_pipeline\\DTC_stage\\data\\csv\\combined_infer_output.csv\n"
     ]
    }
   ],
   "source": [
    "# Cell: Export per-DTC and combined parquet outputs -> strict CSVs (atomic)\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "\n",
    "ROOT = Path(r\"C:\\engine_module_pipeline\")\n",
    "PER_DTC_OUT = ROOT / r\"DTC_stage\\data\\Output\\per_dtc\"\n",
    "COMBINED_OUT = ROOT / r\"DTC_stage\\data\\Output\\combined\"\n",
    "CSV_OUT_DIR = ROOT / r\"DTC_stage\\data\\csv\"\n",
    "CSV_OUT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Define the canonical output columns (must match the DataFrame built during inference)\n",
    "EXPECTED_COLUMNS = [\n",
    "    \"row_hash\",\n",
    "    \"timestamp\",\n",
    "    \"date\",\n",
    "    \"dtc_code\",\n",
    "    \"p_raw_precursor\",\n",
    "    \"p_calib_precursor\",\n",
    "    \"p_raw_fault\",\n",
    "    \"p_calib_fault\",\n",
    "    \"dPdt_precursor\",\n",
    "    \"dPdt_fault\",\n",
    "    \"pred_precursor_on\",\n",
    "    \"pred_fault_on\",\n",
    "    \"alert_level\",\n",
    "    \"feature_snapshot\",\n",
    "]\n",
    "\n",
    "def _read_all_parquets_under(path: Path) -> pd.DataFrame:\n",
    "    \"\"\"Recursively read all .parquet files under 'path' and concat into a single DataFrame.\n",
    "       If no parquet found, returns empty DataFrame().\n",
    "    \"\"\"\n",
    "    parquet_files = sorted([p for p in path.rglob(\"*.parquet\") if p.is_file()])\n",
    "    if not parquet_files:\n",
    "        return pd.DataFrame()\n",
    "    parts = []\n",
    "    for pf in parquet_files:\n",
    "        try:\n",
    "            # Use pyarrow for robust reading, then to_pandas\n",
    "            table = pq.read_table(pf.as_posix())\n",
    "            parts.append(table.to_pandas())\n",
    "        except Exception as e:\n",
    "            # fallback: pandas read_parquet\n",
    "            try:\n",
    "                parts.append(pd.read_parquet(pf.as_posix()))\n",
    "            except Exception as e2:\n",
    "                print(f\"Warning: failed to read {pf}: {e} / {e2} — skipping\")\n",
    "                continue\n",
    "    if not parts:\n",
    "        return pd.DataFrame()\n",
    "    df = pd.concat(parts, ignore_index=True, sort=False)\n",
    "    return df\n",
    "\n",
    "def _ensure_columns_and_fill(df: pd.DataFrame, expected_cols: list) -> pd.DataFrame:\n",
    "    \"\"\"Enforce expected columns order, add missing cols, drop unexpected unnamed columns,\n",
    "       and fill NaNs with empty string so every cell has a value.\n",
    "    \"\"\"\n",
    "    # Drop weird unnamed columns created by previous CSVs (like 'Unnamed: 0')\n",
    "    unnamed_cols = [c for c in df.columns if str(c).lower().startswith(\"unnamed\")]\n",
    "    if unnamed_cols:\n",
    "        df = df.drop(columns=unnamed_cols, errors=\"ignore\")\n",
    "\n",
    "    # Add any expected cols that are missing\n",
    "    for c in expected_cols:\n",
    "        if c not in df.columns:\n",
    "            df[c] = \"\"  # create with empty strings\n",
    "\n",
    "    # Keep any extra columns beyond expected (but move them after expected cols) — preserve info\n",
    "    extra_cols = [c for c in df.columns if c not in expected_cols]\n",
    "    ordered_cols = expected_cols + extra_cols\n",
    "\n",
    "    # Reorder and fill missing values with empty string\n",
    "    df = df.reindex(columns=ordered_cols)\n",
    "    df = df.fillna(\"\")  # strict: no NaNs, no None\n",
    "    return df\n",
    "\n",
    "def _atomic_write_csv(df: pd.DataFrame, out_path: Path, index: bool = False) -> None:\n",
    "    out_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "    tmp = out_path.with_suffix(out_path.suffix + \".tmp\")\n",
    "    df.to_csv(tmp.as_posix(), index=index)\n",
    "    # atomic replace\n",
    "    os.replace(tmp.as_posix(), out_path.as_posix())\n",
    "\n",
    "summary = {}\n",
    "\n",
    "# 1) Per-DTC folders\n",
    "if PER_DTC_OUT.exists():\n",
    "    for dtc_dir in sorted([d for d in PER_DTC_OUT.iterdir() if d.is_dir()]):\n",
    "        dtc_code = dtc_dir.name\n",
    "        df = _read_all_parquets_under(dtc_dir)\n",
    "        if df.empty:\n",
    "            print(f\"No parquet parts found for {dtc_code} under {dtc_dir}, skipping CSV creation.\")\n",
    "            summary[dtc_code] = {\"rows\": 0, \"csv\": None}\n",
    "            continue\n",
    "        # Ensure canonical columns and fill\n",
    "        df_strict = _ensure_columns_and_fill(df, EXPECTED_COLUMNS)\n",
    "        # Ensure timestamp and date columns are strings in CSV (ISO format)\n",
    "        if \"timestamp\" in df_strict.columns:\n",
    "            df_strict[\"timestamp\"] = pd.to_datetime(df_strict[\"timestamp\"], errors=\"coerce\").astype(str).fillna(\"\")\n",
    "        if \"date\" in df_strict.columns:\n",
    "            df_strict[\"date\"] = df_strict[\"date\"].astype(str).fillna(\"\")\n",
    "        out_csv = CSV_OUT_DIR / f\"{dtc_code}_infer_output.csv\"\n",
    "        _atomic_write_csv(df_strict, out_csv, index=False)\n",
    "        summary[dtc_code] = {\"rows\": len(df_strict), \"csv\": str(out_csv)}\n",
    "\n",
    "# 2) Combined table\n",
    "if COMBINED_OUT.exists():\n",
    "    df_comb = _read_all_parquets_under(COMBINED_OUT)\n",
    "    if df_comb.empty:\n",
    "        print(\"No combined parquet parts found, skipping combined CSV creation.\")\n",
    "        summary[\"combined\"] = {\"rows\": 0, \"csv\": None}\n",
    "    else:\n",
    "        df_comb_strict = _ensure_columns_and_fill(df_comb, EXPECTED_COLUMNS)\n",
    "        if \"timestamp\" in df_comb_strict.columns:\n",
    "            df_comb_strict[\"timestamp\"] = pd.to_datetime(df_comb_strict[\"timestamp\"], errors=\"coerce\").astype(str).fillna(\"\")\n",
    "        if \"date\" in df_comb_strict.columns:\n",
    "            df_comb_strict[\"date\"] = df_comb_strict[\"date\"].astype(str).fillna(\"\")\n",
    "        out_csv_comb = CSV_OUT_DIR / f\"combined_infer_output.csv\"\n",
    "        _atomic_write_csv(df_comb_strict, out_csv_comb, index=False)\n",
    "        summary[\"combined\"] = {\"rows\": len(df_comb_strict), \"csv\": str(out_csv_comb)}\n",
    "else:\n",
    "    print(f\"Combined output directory not found at {COMBINED_OUT}, skipping combined CSV creation.\")\n",
    "    summary[\"combined\"] = {\"rows\": 0, \"csv\": None}\n",
    "\n",
    "# Report\n",
    "print(\"CSV export summary:\")\n",
    "for k, v in summary.items():\n",
    "    print(f\" - {k}: rows={v['rows']}, csv={v['csv']}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8c85a66b-77df-4c62-b315-97d37b6b7349",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv-spark (Py3.11 + Spark 3.5)",
   "language": "python",
   "name": "venv-spark"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
